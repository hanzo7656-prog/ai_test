# self_learning/knowledge_base.py
import logging
import json
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict, deque
import hashlib

logger = logging.getLogger(__name__)

class KnowledgeBase:
    """Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ùˆ Ø­Ø§ÙØ¸Ù‡ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    
    def __init__(self):
        self.model_knowledge = defaultdict(dict)
        self.training_patterns = defaultdict(list)
        self.performance_insights = defaultdict(dict)
        self.experience_buffer = deque(maxlen=5000)
        
        # Ø¯Ø§Ù†Ø´ domain-specific
        self.market_patterns = {}
        self.trading_rules = {}
        self.risk_profiles = {}
        
        # Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ú©Ø´
        from debug_system.storage.cache_debugger import cache_debugger
        self.cache_manager = cache_debugger
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯
        self._load_existing_knowledge()
        
        logger.info("ğŸ“š Knowledge Base initialized")

    def _load_existing_knowledge(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø² Ú©Ø´"""
        try:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ù†Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§
            model_knowledge = self.cache_manager.get_data("utb", "model_knowledge")
            if model_knowledge:
                self.model_knowledge.update(model_knowledge)
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
            training_patterns = self.cache_manager.get_data("utb", "training_patterns") 
            if training_patterns:
                self.training_patterns.update(training_patterns)
                
            logger.info("âœ… Existing knowledge loaded from cache")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load existing knowledge: {e}")

    def save_model_experience(self, model_name: str, experience: Dict[str, Any]):
        """Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù…Ø¯Ù„"""
        try:
            experience_id = hashlib.md5(
                f"{model_name}_{datetime.now().timestamp()}".encode()
            ).hexdigest()
            
            experience_data = {
                'experience_id': experience_id,
                'model_name': model_name,
                'timestamp': datetime.now().isoformat(),
                'data': experience,
                'type': experience.get('type', 'training')
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¨Ø§ÙØ± ØªØ¬Ø±Ø¨ÛŒØ§Øª
            self.experience_buffer.append(experience_data)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯Ø§Ù†Ø´ Ù…Ø¯Ù„
            if model_name not in self.model_knowledge:
                self.model_knowledge[model_name] = {
                    'total_experiences': 0,
                    'last_updated': datetime.now().isoformat(),
                    'experiences': []
                }
            
            self.model_knowledge[model_name]['experiences'].append(experience_data)
            self.model_knowledge[model_name]['total_experiences'] += 1
            self.model_knowledge[model_name]['last_updated'] = datetime.now().isoformat()
            
            # Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ù„Ú¯ÙˆÙ‡Ø§
            self._analyze_training_patterns(model_name, experience)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
            self._save_knowledge_to_cache()
            
            logger.debug(f"ğŸ’¾ Saved experience for {model_name}: {experience_id}")
            
        except Exception as e:
            logger.error(f"âŒ Error saving model experience: {e}")

    def _analyze_training_patterns(self, model_name: str, experience: Dict[str, Any]):
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø§Ø² ØªØ¬Ø±Ø¨ÛŒØ§Øª"""
        try:
            pattern_key = f"{model_name}_{experience.get('type', 'general')}"
            
            pattern_data = {
                'timestamp': datetime.now().isoformat(),
                'performance_metrics': experience.get('performance_metrics', {}),
                'training_config': experience.get('training_config', {}),
                'data_characteristics': experience.get('data_characteristics', {})
            }
            
            self.training_patterns[pattern_key].append(pattern_data)
            
            # Ø­ÙØ¸ ÙÙ‚Ø· Û±Û°Û° Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù„Ú¯Ùˆ
            if len(self.training_patterns[pattern_key]) > 100:
                self.training_patterns[pattern_key] = self.training_patterns[pattern_key][-100:]
                
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ insightâ€ŒÙ‡Ø§
            self._extract_performance_insights(model_name, pattern_data)
            
        except Exception as e:
            logger.error(f"âŒ Error analyzing training patterns: {e}")

    def _extract_performance_insights(self, model_name: str, pattern_data: Dict[str, Any]):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ insightâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ø² Ø§Ù„Ú¯ÙˆÙ‡Ø§"""
        try:
            insights = self.performance_insights.get(model_name, {})
            
            metrics = pattern_data.get('performance_metrics', {})
            config = pattern_data.get('training_config', {})
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯
            if 'accuracy' in metrics and 'previous_accuracy' in metrics:
                improvement = metrics['accuracy'] - metrics['previous_accuracy']
                
                if improvement > insights.get('best_improvement', 0):
                    insights['best_improvement'] = improvement
                    insights['best_config'] = config
                    insights['best_timestamp'] = pattern_data['timestamp']
            
            # Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† ØªÙ†Ø¸ÛŒÙ…Ø§Øª
            if metrics.get('accuracy', 0) > insights.get('best_accuracy', 0):
                insights['best_accuracy'] = metrics['accuracy']
                insights['best_accuracy_config'] = config
            
            self.performance_insights[model_name] = insights
            
        except Exception as e:
            logger.error(f"âŒ Error extracting performance insights: {e}")

    def get_model_knowledge(self, model_name: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ù†Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø¯Ù„"""
        return self.model_knowledge.get(model_name, {
            'total_experiences': 0,
            'last_updated': None,
            'experiences': []
        })

    def find_similar_experiences(self, model_name: str, current_context: Dict[str, Any], 
                               max_results: int = 5) -> List[Dict[str, Any]]:
        """Ù¾ÛŒØ¯Ø§Ú©Ø±Ø¯Ù† ØªØ¬Ø±Ø¨ÛŒØ§Øª Ù…Ø´Ø§Ø¨Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ÛŒ"""
        try:
            similar_experiences = []
            model_experiences = self.model_knowledge.get(model_name, {}).get('experiences', [])
            
            for experience in model_experiences[-100:]:  # ÙÙ‚Ø· Û±Û°Û° ØªØ¬Ø±Ø¨Ù‡ Ø§Ø®ÛŒØ±
                similarity_score = self._calculate_context_similarity(
                    current_context, 
                    experience['data']
                )
                
                if similarity_score > 0.7:  threshold 
                    similar_experiences.append({
                        'experience': experience,
                        'similarity_score': similarity_score,
                        'relevance': self._calculate_relevance(experience, current_context)
                    })
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ similarity Ùˆ relevance
            similar_experiences.sort(key=lambda x: x['similarity_score'] * x['relevance'], reverse=True)
            
            return similar_experiences[:max_results]
            
        except Exception as e:
            logger.error(f"âŒ Error finding similar experiences: {e}")
            return []

    def _calculate_context_similarity(self, context1: Dict[str, Any], context2: Dict[str, Any]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ context"""
        try:
            similarity = 0.0
            compared_features = 0
            
            # Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
            numeric_features = ['data_size', 'feature_count', 'training_time', 'accuracy']
            
            for feature in numeric_features:
                if feature in context1 and feature in context2:
                    val1 = context1[feature] if context1[feature] else 0
                    val2 = context2[feature] if context2[feature] else 0
                    
                    if val1 + val2 > 0:
                        similarity += 1 - abs(val1 - val2) / max(val1, val2)
                        compared_features += 1
            
            # Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©ÛŒÙÛŒ
            qualitative_features = ['data_type', 'market_condition', 'training_strategy']
            
            for feature in qualitative_features:
                if feature in context1 and feature in context2:
                    if context1[feature] == context2[feature]:
                        similarity += 1.0
                    compared_features += 1
            
            return similarity / compared_features if compared_features > 0 else 0.0
            
        except Exception as e:
            logger.error(f"âŒ Error calculating context similarity: {e}")
            return 0.0

    def _calculate_relevance(self, experience: Dict[str, Any], current_context: Dict[str, Any]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ relevance ØªØ¬Ø±Ø¨Ù‡"""
        try:
            # relevance Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø²Ú¯ÛŒ Ùˆ performance
            experience_time = datetime.fromisoformat(experience['timestamp'])
            time_diff = (datetime.now() - experience_time).total_seconds()
            
            # ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø¬Ø¯ÛŒØ¯ØªØ± relevance Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯
            time_relevance = max(0, 1 - (time_diff / (30 * 24 * 3600)))  # 30 Ø±ÙˆØ²
            
            # ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø¨Ø§ performance Ø¨Ù‡ØªØ± relevance Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯
            performance = experience['data'].get('performance_metrics', {}).get('accuracy', 0.5)
            performance_relevance = performance
            
            return (time_relevance + performance_relevance) / 2
            
        except Exception as e:
            logger.error(f"âŒ Error calculating relevance: {e}")
            return 0.5

    def get_training_recommendations(self, model_name: str, current_performance: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯"""
        try:
            recommendations = []
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚ Ú¯Ø°Ø´ØªÙ‡
            successful_patterns = self._find_successful_patterns(model_name, current_performance)
            
            for pattern in successful_patterns:
                recommendation = {
                    'type': 'training_strategy',
                    'confidence': pattern['success_score'],
                    'suggested_config': pattern['config'],
                    'expected_improvement': pattern['improvement'],
                    'reasoning': pattern['reasoning'],
                    'based_on_experiences': pattern['experience_count']
                }
                recommendations.append(recommendation)
            
            # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ based on performance gaps
            performance_recommendations = self._generate_performance_recommendations(
                model_name, current_performance
            )
            recommendations.extend(performance_recommendations)
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ confidence
            recommendations.sort(key=lambda x: x['confidence'], reverse=True)
            
            return recommendations[:10]  # Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û° ØªÙˆØµÛŒÙ‡
            
        except Exception as e:
            logger.error(f"âŒ Error generating training recommendations: {e}")
            return []

    def _find_successful_patterns(self, model_name: str, current_performance: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Ù¾ÛŒØ¯Ø§Ú©Ø±Ø¯Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù…ÙˆÙÙ‚"""
        successful_patterns = []
        
        pattern_key = f"{model_name}_training"
        patterns = self.training_patterns.get(pattern_key, [])
        
        for pattern in patterns[-50:]:  # ÛµÛ° Ø§Ù„Ú¯ÙˆÛŒ Ø§Ø®ÛŒØ±
            metrics = pattern.get('performance_metrics', {})
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù…ÙˆÙÙ‚ÛŒØª
            success_score = self._calculate_success_score(metrics, current_performance)
            
            if success_score > 0.7:
                successful_patterns.append({
                    'config': pattern.get('training_config', {}),
                    'success_score': success_score,
                    'improvement': metrics.get('accuracy', 0) - current_performance.get('accuracy', 0),
                    'reasoning': f"Ø§ÛŒÙ† ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù‚Ø¨Ù„Ø§Ù‹ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ø¯Ù‚Øª {metrics.get('accuracy', 0):.3f} Ø´Ø¯Ù‡ Ø§Ø³Øª",
                    'experience_count': 1
                })
        
        return successful_patterns

    def _calculate_success_score(self, historical_metrics: Dict[str, Any], 
                               current_metrics: Dict[str, Any]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù„Ú¯Ùˆ"""
        score = 0.0
        factors = 0
        
        # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ù‚Øª
        if 'accuracy' in historical_metrics and 'accuracy' in current_metrics:
            accuracy_improvement = historical_metrics['accuracy'] - current_metrics['accuracy']
            if accuracy_improvement > 0:
                score += min(1.0, accuracy_improvement * 2)  # Ø¨Ù‡Ø¨ÙˆØ¯ ÛµÙª = Ø§Ù…ØªÛŒØ§Ø² Û±
            factors += 1
        
        # Ø³Ø§ÛŒØ± ÙØ§Ú©ØªÙˆØ±Ù‡Ø§
        if 'training_time' in historical_metrics:
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ù…ØªÛŒØ§Ø² Ø¨ÛŒØ´ØªØ±ÛŒ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯
            time_score = 1.0 / (1.0 + historical_metrics['training_time'] / 3600)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø¹Øª
            score += time_score
            factors += 1
        
        return score / factors if factors > 0 else 0.0

    def _generate_performance_recommendations(self, model_name: str, 
                                           current_performance: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ based on Ø´Ú©Ø§Ù Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        recommendations = []
        
        insights = self.performance_insights.get(model_name, {})
        
        # Ø§Ú¯Ø± Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ØªØ± Ø§Ø² Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙØ¹Ù„ÛŒ Ø§Ø³Øª
        best_accuracy = insights.get('best_accuracy', 0)
        current_accuracy = current_performance.get('accuracy', 0)
        
        if best_accuracy - current_accuracy > 0.1:  # Ø´Ú©Ø§Ù Û±Û°Ùª
            recommendations.append({
                'type': 'performance_gap',
                'confidence': 0.8,
                'suggested_action': 'use_best_known_config',
                'config': insights.get('best_accuracy_config', {}),
                'expected_improvement': best_accuracy - current_accuracy,
                'reasoning': f'Ø´Ú©Ø§Ù Ø¹Ù…Ù„Ú©Ø±Ø¯ {best_accuracy - current_accuracy:.3f} - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ú¯Ø°Ø´ØªÙ‡'
            })
        
        return recommendations

    def save_market_pattern(self, pattern_name: str, pattern_data: Dict[str, Any]):
        """Ø°Ø®ÛŒØ±Ù‡ Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¬Ø¯ÛŒØ¯"""
        try:
            pattern_id = hashlib.md5(pattern_name.encode()).hexdigest()
            
            market_pattern = {
                'pattern_id': pattern_id,
                'name': pattern_name,
                'data': pattern_data,
                'discovered_at': datetime.now().isoformat(),
                'confidence': pattern_data.get('confidence', 0.5),
                'occurrence_count': 1
            }
            
            # Ø§Ú¯Ø± Ø§Ù„Ú¯Ùˆ Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ occurrence Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø¯Ù‡
            if pattern_id in self.market_patterns:
                existing = self.market_patterns[pattern_id]
                existing['occurrence_count'] += 1
                existing['confidence'] = max(existing['confidence'], pattern_data.get('confidence', 0.5))
                existing['last_seen'] = datetime.now().isoformat()
            else:
                self.market_patterns[pattern_id] = market_pattern
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
            self.cache_manager.set_data("utb", f"market_pattern:{pattern_id}", market_pattern, expire=86400)
            
            logger.info(f"ğŸ” Saved market pattern: {pattern_name}")
            
        except Exception as e:
            logger.error(f"âŒ Error saving market pattern: {e}")

    def find_relevant_market_patterns(self, current_market_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Ù¾ÛŒØ¯Ø§Ú©Ø±Ø¯Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ù…Ø±ØªØ¨Ø·"""
        relevant_patterns = []
        
        for pattern_id, pattern in self.market_patterns.items():
            relevance = self._calculate_market_relevance(pattern, current_market_data)
            
            if relevance > 0.6:  # threshold
                relevant_patterns.append({
                    'pattern': pattern,
                    'relevance': relevance,
                    'predicted_outcome': pattern['data'].get('expected_outcome', 'unknown')
                })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ relevance
        relevant_patterns.sort(key=lambda x: x['relevance'], reverse=True)
        
        return relevant_patterns[:5]  # Ûµ Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø±ØªØ±

    def _calculate_market_relevance(self, pattern: Dict[str, Any], 
                                  current_data: Dict[str, Any]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ relevance Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ø§Ø±"""
        # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø³Øª
        # Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ Ø§Ø² similarity measures Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯
        return pattern.get('confidence', 0.5)  # placeholder

    def get_knowledge_summary(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯"""
        return {
            'timestamp': datetime.now().isoformat(),
            'total_models_with_knowledge': len(self.model_knowledge),
            'total_experiences': sum(
                model_data.get('total_experiences', 0) 
                for model_data in self.model_knowledge.values()
            ),
            'total_training_patterns': sum(
                len(patterns) for patterns in self.training_patterns.values()
            ),
            'total_market_patterns': len(self.market_patterns),
            'knowledge_quality_metrics': {
                'avg_experiences_per_model': self._calculate_avg_experiences(),
                'knowledge_freshness': self._calculate_knowledge_freshness(),
                'pattern_effectiveness': self._calculate_pattern_effectiveness()
            }
        }

    def _calculate_avg_experiences(self) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¬Ø±Ø¨ÛŒØ§Øª per model"""
        if not self.model_knowledge:
            return 0.0
        total = sum(model_data.get('total_experiences', 0) for model_data in self.model_knowledge.values())
        return total / len(self.model_knowledge)

    def _calculate_knowledge_freshness(self) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ§Ø²Ú¯ÛŒ Ø¯Ø§Ù†Ø´"""
        if not self.model_knowledge:
            return 0.0
        
        now = datetime.now()
        freshness_scores = []
        
        for model_data in self.model_knowledge.values():
            last_updated = datetime.fromisoformat(model_data.get('last_updated', now.isoformat()))
            days_old = (now - last_updated).total_seconds() / (24 * 3600)
            freshness = max(0, 1 - (days_old / 30))  # Ø¯Ø§Ù†Ø´ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ø§Ø² Û³Û° Ø±ÙˆØ² Ø§Ù…ØªÛŒØ§Ø² Ú©Ù…ØªØ±ÛŒ
            freshness_scores.append(freshness)
        
        return np.mean(freshness_scores) if freshness_scores else 0.0

    def _calculate_pattern_effectiveness(self) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø«Ø±Ø¨Ø®Ø´ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§"""
        # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø³Øª
        # Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§Ù‚Ø¹ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆØ¯
        return 0.75  # placeholder

    def _save_knowledge_to_cache(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ù†Ø´ Ø¯Ø± Ú©Ø´"""
        try:
            knowledge_data = {
                'model_knowledge': dict(self.model_knowledge),
                'training_patterns': dict(self.training_patterns),
                'performance_insights': dict(self.performance_insights),
                'market_patterns': self.market_patterns,
                'last_saved': datetime.now().isoformat()
            }
            
            self.cache_manager.set_data("utb", "knowledge_base", knowledge_data, expire=7200)  # 2 hours
            
        except Exception as e:
            logger.error(f"âŒ Error saving knowledge to cache: {e}")

    def export_knowledge(self, export_path: str = None) -> Dict[str, Any]:
        """Ø®Ø±ÙˆØ¬ÛŒ Ú¯Ø±ÙØªÙ† Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¨Ø±Ø§ÛŒ backup ÛŒØ§ transfer"""
        export_data = {
            'export_timestamp': datetime.now().isoformat(),
            'version': '1.0',
            'knowledge_base': {
                'model_knowledge': dict(self.model_knowledge),
                'training_patterns': dict(self.training_patterns),
                'performance_insights': dict(self.performance_insights),
                'market_patterns': self.market_patterns
            },
            'summary': self.get_knowledge_summary()
        }
        
        if export_path:
            try:
                with open(export_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ Knowledge exported to {export_path}")
            except Exception as e:
                logger.error(f"âŒ Error exporting knowledge to file: {e}")
        
        return export_data

# Ù†Ù…ÙˆÙ†Ù‡ global
knowledge_base = None

def initialize_knowledge_base():
    """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ knowledge base"""
    global knowledge_base
    knowledge_base = KnowledgeBase()
    return knowledge_base
