# self_learning/reinforcement_learner.py
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import random
import math

logger = logging.getLogger(__name__)

# Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨ÛŒØ§Øª
Experience = namedtuple('Experience', 
                       ['state', 'action', 'reward', 'next_state', 'done', 'timestamp'])

class ReinforcementNetwork(nn.Module):
    """Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ"""
    
    def __init__(self, state_size: int, action_size: int, hidden_layers: List[int] = [128, 64]):
        super().__init__()
        self.state_size = state_size
        self.action_size = action_size
        
        layers = []
        input_size = state_size
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†
        for hidden_size in hidden_layers:
            layers.extend([
                nn.Linear(input_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            input_size = hidden_size
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        layers.append(nn.Linear(input_size, action_size))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)

class ReinforcementLearner:
    """Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    
    def __init__(self, model_manager, state_size: int = 50, action_size: int = 10):
        self.model_manager = model_manager
        self.state_size = state_size
        self.action_size = action_size
        
        # Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ùˆ target
        self.q_network = ReinforcementNetwork(state_size, action_size)
        self.target_network = ReinforcementNetwork(state_size, action_size)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)
        
        # replay buffer Ø¨Ø±Ø§ÛŒ ØªØ¬Ø±Ø¨ÛŒØ§Øª
        self.memory = deque(maxlen=10000)
        self.batch_size = 32
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        self.gamma = 0.95  # discount factor
        self.epsilon = 1.0  # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.tau = 0.01  # Ø¨Ø±Ø§ÛŒ soft update
        
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        self.learning_history = []
        self.episode_rewards = []
        
        # Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ú©Ø´
        from debug_system.storage.cache_debugger import cache_debugger
        self.cache_manager = cache_debugger
        
        logger.info("ğŸ¤– Reinforcement Learner initialized")

    def add_experience(self, state: np.ndarray, action: int, reward: float, 
                      next_state: np.ndarray, done: bool):
        """Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± replay buffer"""
        experience = Experience(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            timestamp=datetime.now().isoformat()
        )
        self.memory.append(experience)
        
        # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø§Ú¯Ø± Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø±ÛŒÙ…
        if len(self.memory) > self.batch_size:
            self._learn_from_experiences()

    def _learn_from_experiences(self):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ replay buffer"""
        try:
            # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² replay buffer
            batch = random.sample(self.memory, self.batch_size)
            
            states = torch.FloatTensor([exp.state for exp in batch])
            actions = torch.LongTensor([exp.action for exp in batch])
            rewards = torch.FloatTensor([exp.reward for exp in batch])
            next_states = torch.FloatTensor([exp.next_state for exp in batch])
            dones = torch.BoolTensor([exp.done for exp in batch])
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Q-values ÙØ¹Ù„ÛŒ
            current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ target Q-values
            with torch.no_grad():
                next_q_values = self.target_network(next_states).max(1)[0]
                target_q_values = rewards + (self.gamma * next_q_values * ~dones)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss
            loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
            
            # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # Ú©Ø§Ù‡Ø´ exploration rate
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            
            # soft update target network
            self._soft_update_target_network()
            
            # Ø°Ø®ÛŒØ±Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
            learning_step = {
                'timestamp': datetime.now().isoformat(),
                'loss': loss.item(),
                'epsilon': self.epsilon,
                'memory_size': len(self.memory),
                'average_reward': np.mean([exp.reward for exp in batch])
            }
            self.learning_history.append(learning_step)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ Ù‡Ø± Û±Û°Û° step
            if len(self.learning_history) % 100 == 0:
                self._save_learning_progress()
                
        except Exception as e:
            logger.error(f"âŒ Error in reinforcement learning: {e}")

    def _soft_update_target_network(self):
        """Soft update Ø¨Ø±Ø§ÛŒ target network"""
        for target_param, local_param in zip(self.target_network.parameters(), 
                                           self.q_network.parameters()):
            target_param.data.copy_(self.tau * local_param.data + 
                                  (1.0 - self.tau) * target_param.data)

    def get_action(self, state: np.ndarray) -> int:
        """Ø¯Ø±ÛŒØ§ÙØª action Ø¨Ø± Ø§Ø³Ø§Ø³ state ÙØ¹Ù„ÛŒ"""
        if np.random.random() < self.epsilon:
            # Exploration: action ØªØµØ§Ø¯ÙÛŒ
            return random.randint(0, self.action_size - 1)
        else:
            # Exploitation: Ø¨Ù‡ØªØ±ÛŒÙ† action Ø¨Ø± Ø§Ø³Ø§Ø³ Q-values
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()

    def optimize_model_parameters(self, model_name: str, state: np.ndarray) -> Dict[str, Any]:
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ"""
        try:
            if model_name not in self.model_manager.active_models:
                raise ValueError(f"Model {model_name} not found")
            
            # Ø¯Ø±ÛŒØ§ÙØª action Ø¨Ù‡ÛŒÙ†Ù‡
            action = self.get_action(state)
            
            # Ø§Ø¹Ù…Ø§Ù„ action Ø¨Ù‡ Ù…Ø¯Ù„ (Ù…Ø«Ù„Ø§Ù‹ ØªÙ†Ø¸ÛŒÙ… learning rateØŒ ØªØºÛŒÛŒØ± architecture)
            reward = self._apply_action_to_model(model_name, action)
            
            # Ù…Ø´Ø§Ù‡Ø¯Ù‡ state Ø¨Ø¹Ø¯ÛŒ
            next_state = self._get_next_state(model_name, state, action)
            
            # Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡
            self.add_experience(state, action, reward, next_state, done=False)
            
            optimization_result = {
                'model': model_name,
                'action_taken': action,
                'reward_earned': reward,
                'epsilon': self.epsilon,
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"ğŸ¯ RL optimization for {model_name}: action={action}, reward={reward:.3f}")
            
            return optimization_result
            
        except Exception as e:
            logger.error(f"âŒ RL optimization failed for {model_name}: {e}")
            return {'error': str(e)}

    def _apply_action_to_model(self, model_name: str, action: int) -> float:
        """Ø§Ø¹Ù…Ø§Ù„ action Ø¨Ù‡ Ù…Ø¯Ù„ Ùˆ Ø¯Ø±ÛŒØ§ÙØª reward"""
        try:
            model_info = self.model_manager.active_models[model_name]
            model = model_info['model']
            
            # Ø¨Ø± Ø§Ø³Ø§Ø³ actionØŒ ØªØºÛŒÛŒØ±Ø§Øª Ù…Ø®ØªÙ„Ù Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            if action == 0:
                # Ø§ÙØ²Ø§ÛŒØ´ learning rate
                reward = self._adjust_learning_rate(model, 1.1)
            elif action == 1:
                # Ú©Ø§Ù‡Ø´ learning rate
                reward = self._adjust_learning_rate(model, 0.9)
            elif action == 2:
                # Ø§ÙØ²Ø§ÛŒØ´ dropout
                reward = self._adjust_dropout(model, 1.1)
            elif action == 3:
                # Ú©Ø§Ù‡Ø´ dropout
                reward = self._adjust_dropout(model, 0.9)
            else:
                # Ø³Ø§ÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª
                reward = self._other_adjustments(model, action)
            
            return reward
            
        except Exception as e:
            logger.error(f"âŒ Error applying action to model: {e}")
            return -1.0  # reward Ù…Ù†ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ø·Ø§

    def _adjust_learning_rate(self, model, factor: float) -> float:
        """ØªÙ†Ø¸ÛŒÙ… learning rate Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ reward"""
        # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø³Øª
        # Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ performance Ù…Ø¯Ù„ reward Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆØ¯
        return 0.1  # placeholder

    def _adjust_dropout(self, model, factor: float) -> float:
        """ØªÙ†Ø¸ÛŒÙ… dropout rate"""
        return 0.1  # placeholder

    def _other_adjustments(self, model, action: int) -> float:
        """Ø³Ø§ÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„"""
        return 0.05  # placeholder

    def _get_next_state(self, model_name: str, current_state: np.ndarray, action: int) -> np.ndarray:
        """Ø¯Ø±ÛŒØ§ÙØª state Ø¨Ø¹Ø¯ÛŒ Ù¾Ø³ Ø§Ø² Ø§Ø¹Ù…Ø§Ù„ action"""
        # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ state Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ù¾Ø³ Ø§Ø² Ø§Ø¹Ù…Ø§Ù„ action Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
        # Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø´Ø§Ù…Ù„ performance Ø¬Ø¯ÛŒØ¯ Ù…Ø¯Ù„ Ø¨Ø§Ø´Ø¯
        return current_state  # placeholder

    def train_trading_agent(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ø¢Ù…ÙˆØ²Ø´ agent Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ù‡â€ŒÚ¯Ø±ÛŒ"""
        try:
            # ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø¨Ù‡ state
            state = self._market_data_to_state(market_data)
            
            # Ø¯Ø±ÛŒØ§ÙØª action
            action = self.get_action(state)
            
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹Ø§Ù…Ù„Ù‡ Ùˆ Ø¯Ø±ÛŒØ§ÙØª reward
            reward, done = self._simulate_trade(action, market_data)
            
            # state Ø¨Ø¹Ø¯ÛŒ
            next_state = self._get_next_trading_state(state, action, market_data)
            
            # Ø°Ø®ÛŒØ±Ù‡ ØªØ¬Ø±Ø¨Ù‡
            self.add_experience(state, action, reward, next_state, done)
            
            training_result = {
                'episode': len(self.episode_rewards) + 1,
                'action': self._action_to_trade_type(action),
                'reward': reward,
                'epsilon': self.epsilon,
                'timestamp': datetime.now().isoformat()
            }
            
            if done:
                self.episode_rewards.append(reward)
                training_result['episode_complete'] = True
                training_result['total_episode_reward'] = reward
            
            return training_result
            
        except Exception as e:
            logger.error(f"âŒ Trading agent training failed: {e}")
            return {'error': str(e)}

    def _market_data_to_state(self, market_data: Dict[str, Any]) -> np.ndarray:
        """ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø¨Ù‡ state Ø¨Ø±Ø§ÛŒ RL"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø²Ø§Ø±
        features = []
        
        # Ù‚ÛŒÙ…Øª Ùˆ Ø­Ø¬Ù…
        if 'raw_coins' in market_data.get('sources', {}):
            coin_data = market_data['sources']['raw_coins'].get('data', [])
            if coin_data and isinstance(coin_data, list):
                prices = [item.get('price', 0) for item in coin_data[:10] if isinstance(item, dict)]
                if prices:
                    features.extend([
                        np.mean(prices),
                        np.std(prices),
                        (prices[-1] - prices[0]) / prices[0] if prices[0] > 0 else 0
                    ])
        
        # Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø±
        if 'raw_news' in market_data.get('sources', {}):
            news_data = market_data['sources']['raw_news'].get('data', [])
            if news_data and isinstance(news_data, list):
                # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª
                sentiment = self._calculate_news_sentiment(news_data)
                features.append(sentiment)
        
        # Ø§Ú¯Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ú©Ù… Ù‡Ø³ØªÙ†Ø¯ØŒ padding Ú©Ù†
        while len(features) < self.state_size:
            features.append(0.0)
        
        return np.array(features[:self.state_size], dtype=np.float32)

    def _calculate_news_sentiment(self, news_data: List[Dict]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ"""
        if not news_data:
            return 0.5
        
        positive_words = ['ØµØ¹ÙˆØ¯', 'Ø±Ø´Ø¯', 'Ø³ÙˆØ¯', 'Ù…Ø«Ø¨Øª', 'Ù‚ÙˆÛŒ']
        negative_words = ['Ù†Ø²ÙˆÙ„', 'Ø³Ù‚ÙˆØ·', 'Ø¶Ø±Ø±', 'Ù…Ù†ÙÛŒ', 'Ø¶Ø¹ÛŒÙ']
        
        total_sentiment = 0
        for news_item in news_data[:5]:  # ÙÙ‚Ø· Ûµ Ø®Ø¨Ø± Ø§ÙˆÙ„
            text = f"{news_item.get('title', '')} {news_item.get('description', '')}".lower()
            positive_count = sum(1 for word in positive_words if word in text)
            negative_count = sum(1 for word in negative_words if word in text)
            
            total = positive_count + negative_count
            if total > 0:
                total_sentiment += positive_count / total
        
        return total_sentiment / min(5, len(news_data)) if news_data else 0.5

    def _action_to_trade_type(self, action: int) -> str:
        """ØªØ¨Ø¯ÛŒÙ„ action Ø¨Ù‡ Ù†ÙˆØ¹ Ù…Ø¹Ø§Ù…Ù„Ù‡"""
        actions = {
            0: "BUY",
            1: "SELL", 
            2: "HOLD",
            3: "BUY_AGGRESSIVE",
            4: "SELL_AGGRESSIVE"
        }
        return actions.get(action, "HOLD")

    def _simulate_trade(self, action: int, market_data: Dict[str, Any]) -> Tuple[float, bool]:
        """Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹Ø§Ù…Ù„Ù‡ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ reward"""
        # Ø§ÛŒÙ† ÛŒÚ© Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø³Øª
        # Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§Ø²Ø§Ø± Ú©Ø§Ø± Ú©Ù†Ø¯
        
        reward = random.uniform(-1.0, 1.0)  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡
        done = random.random() < 0.1  # 10% chance episode ØªÙ…Ø§Ù… Ø´ÙˆØ¯
        
        return reward, done

    def _get_next_trading_state(self, current_state: np.ndarray, action: int, 
                              market_data: Dict[str, Any]) -> np.ndarray:
        """Ø¯Ø±ÛŒØ§ÙØª state Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ù‡"""
        # Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ state Ø¬Ø¯ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ action Ùˆ ØªØºÛŒÛŒØ±Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ø¨Ø§Ø´Ø¯
        return current_state  # placeholder

    def _save_learning_progress(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ´Ø±ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ú©Ø´"""
        try:
            progress_data = {
                'learning_history': self.learning_history[-100:],  # Û±Û°Û° Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ø®Ø±
                'episode_rewards': self.episode_rewards,
                'epsilon': self.epsilon,
                'memory_size': len(self.memory),
                'timestamp': datetime.now().isoformat()
            }
            
            self.cache_manager.set_data("utb", "rl_learning_progress", progress_data, expire=3600)
            
        except Exception as e:
            logger.error(f"âŒ Error saving RL progress: {e}")

    def get_learning_stats(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        return {
            'timestamp': datetime.now().isoformat(),
            'memory_size': len(self.memory),
            'epsilon': self.epsilon,
            'total_episodes': len(self.episode_rewards),
            'average_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'learning_steps': len(self.learning_history),
            'recent_loss': self.learning_history[-1]['loss'] if self.learning_history else 0
        }

# Ù†Ù…ÙˆÙ†Ù‡ global
reinforcement_learner = None

def initialize_reinforcement_learner(model_manager):
    """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ reinforcement learner"""
    global reinforcement_learner
    reinforcement_learner = ReinforcementLearner(model_manager)
    return reinforcement_learner
