# memory_efficient_spike_transformer.py
import torch
import torch.nn as nn
import psutil
import gc
import os
from typing import Optional, Dict
import time

class MemoryMonitor:
    """ŸÖÿßŸÜ€åÿ™Ÿàÿ±€åŸÜ⁄Ø ŸÑÿ≠ÿ∏Ÿá‚Äåÿß€å ŸÖÿµÿ±ŸÅ ÿ±ŸÖ Ÿà CPU"""
    
    @staticmethod
    def get_memory_usage():
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': process.memory_percent(),
            'available_mb': psutil.virtual_memory().available / 1024 / 1024,
            'cpu_percent': process.cpu_percent()
        }
    
    @staticmethod
    def print_usage(prefix=""):
        usage = MemoryMonitor.get_memory_usage()
        print(f"{prefix} üíæ RAM: {usage['rss_mb']:.1f}MB | "
              f"üìä CPU: {usage['cpu_percent']:.1f}% | "
              f"üéØ Available: {usage['available_mb']:.1f}MB")

class GradientCheckpointFunction(torch.autograd.Function):
    """⁄Ü⁄©ŸæŸà€åŸÜÿ™ ⁄Øÿ±ÿßÿØ€åÿßŸÜ ÿ®ÿ±ÿß€å ÿµÿ±ŸÅŸá‚Äåÿ¨Ÿà€å€å ÿØÿ± ÿ≠ÿßŸÅÿ∏Ÿá"""
    
    @staticmethod
    def forward(ctx, run_function, *args):
        ctx.run_function = run_function
        ctx.save_for_backward(*args)
        with torch.no_grad():
            output = run_function(*args)
        return output
    
    @staticmethod
    def backward(ctx, *grad_outputs):
        args = ctx.saved_tensors
        with torch.enable_grad():
            output = ctx.run_function(*args)
        torch.autograd.backward(output, grad_outputs)
        return (None,) + tuple(arg.grad for arg in args)

class UltraEfficientSpikeNeuron(nn.Module):
    """ŸÜŸàÿ±ŸàŸÜ ÿßÿ≥Ÿæÿß€å⁄© ŸÅŸàŸÇ ÿ®Ÿá€åŸÜŸá ÿ®ÿß ÿ≠ÿßŸÅÿ∏Ÿá ⁄©ŸÖ"""
    
    def __init__(self, threshold=0.5, decay=0.8):
        super().__init__()
        self.threshold = threshold
        self.decay = decay
        
    def forward(self, x, membrane_potential=None):
        # Ÿæ€åÿßÿØŸá‚Äåÿ≥ÿßÿ≤€å ÿ®ÿØŸàŸÜ ÿ≠ÿßŸÅÿ∏Ÿá ÿßÿ∂ÿßŸÅ€å
        batch_size, seq_len, num_neurons = x.shape
        
        if membrane_potential is None:
            membrane_potential = torch.zeros(batch_size, num_neurons, device=x.device, dtype=torch.float16)
        
        spikes = []
        
        for t in range(seq_len):
            membrane_potential = self.decay * membrane_potential + x[:, t, :].float()
            spike = (membrane_potential > self.threshold).half()
            membrane_potential = membrane_potential * (1 - spike.float())
            
            spikes.append(spike.unsqueeze(1))
            
            # Ÿæÿß⁄©‚Äåÿ≥ÿßÿ≤€å ÿ≠ÿßŸÅÿ∏Ÿá ÿØÿ± ÿ≠€åŸÜ ÿßÿ¨ÿ±ÿß
            if t % 10 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else gc.collect()
        
        return torch.cat(spikes, dim=1), membrane_potential

class MemoryEfficientAttention(nn.Module):
    """ÿ™Ÿàÿ¨Ÿá ÿ®ÿß ÿ≠ÿßŸÅÿ∏Ÿá ÿ®Ÿá€åŸÜŸá‚Äåÿ¥ÿØŸá"""
    
    def __init__(self, d_model=128, n_heads=4):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ Ÿàÿ≤ŸÜ‚ÄåŸáÿß€å ÿßÿ¥ÿ™ÿ±ÿß⁄©€å
        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)
        self.output_proj = nn.Linear(d_model, d_model, bias=False)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape
        
        # Ÿæÿ±Ÿàÿ¨⁄©ÿ¥ŸÜ ÿ™ÿ±⁄©€åÿ®€å QKV
        qkv = self.qkv_proj(x).view(batch_size, seq_len, 3, self.n_heads, self.d_k)
        q, k, v = qkv.unbind(2)
        
        q = q.transpose(1, 2)  # [batch, heads, seq_len, d_k]
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # ŸÖÿ≠ÿßÿ≥ÿ®Ÿá attention ÿ®ÿß ÿ≠ÿßŸÅÿ∏Ÿá ⁄©ŸÖ
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # softmax ÿ®ÿß ŸÖÿØ€åÿ±€åÿ™ ÿ≠ÿßŸÅÿ∏Ÿá
        attn = F.softmax(scores, dim=-1)
        
        # ÿ≠ÿ∞ŸÅ ÿ™ÿßŸÜÿ≥Ÿàÿ±Ÿáÿß€å ŸÖŸàŸÇÿ™ ÿ®ÿ±ÿß€å ÿ¢ÿ≤ÿßÿØÿ≥ÿßÿ≤€å ÿ≠ÿßŸÅÿ∏Ÿá
        del q, k, scores
        
        output = torch.matmul(attn, v)
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        
        del attn, v
        
        return self.output_proj(output)

class CheckpointTransformerBlock(nn.Module):
    """ÿ®ŸÑŸà⁄© ÿ™ÿ±ŸÜÿ≥ŸÅŸàÿ±ŸÖÿ± ÿ®ÿß ⁄Ü⁄©ŸæŸà€åŸÜÿ™ ⁄Øÿ±ÿßÿØ€åÿßŸÜ"""
    
    def __init__(self, d_model=128, n_heads=4, d_ff=256, dropout=0.1):
        super().__init__()
        self.attention = MemoryEfficientAttention(d_model, n_heads)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # ⁄Ü⁄©ŸæŸà€åŸÜÿ™ ÿ®ÿ±ÿß€å ÿµÿ±ŸÅŸá‚Äåÿ¨Ÿà€å€å ÿØÿ± ÿ≠ÿßŸÅÿ∏Ÿá
        def attn_block(x):
            return self.attention(self.norm1(x), mask)
        
        x = x + GradientCheckpointFunction.apply(attn_block, x)
        x = x + self.dropout(self.ffn(self.norm2(x)))
        
        return x

class RenderFreeSpikeTransformer(nn.Module):
    """ŸÖÿØŸÑ ÿ®Ÿá€åŸÜŸá‚Äåÿ¥ÿØŸá ÿ®ÿ±ÿß€å ÿ∑ÿ±ÿ≠ ÿ±ÿß€å⁄ØÿßŸÜ ÿ±ŸÜÿØÿ± (512MB RAM)"""
    
    def __init__(self, 
                 vocab_size=5000,      # ⁄©ÿßŸáÿ¥ ÿ≥ÿß€åÿ≤ vocab
                 d_model=128,          # ⁄©ÿßŸáÿ¥ ŸÇÿßÿ®ŸÑ ÿ™Ÿàÿ¨Ÿá ÿßÿ®ÿπÿßÿØ
                 n_heads=4,
                 num_layers=3,         # ŸÑÿß€åŸá‚ÄåŸáÿß€å ⁄©ŸÖÿ™ÿ±
                 d_ff=256,
                 dropout=0.1,
                 max_seq_len=128,      # ÿ™ŸàÿßŸÑ€å ⁄©Ÿàÿ™ÿßŸá‚Äåÿ™ÿ±
                 num_classes=2):
        super().__init__()
        
        print("üßÆ Initializing Ultra-Efficient Spike Transformer...")
        MemoryMonitor.print_usage("Before model init:")
        
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # Embedding Ÿáÿß€å ÿ®Ÿá€åŸÜŸá‚Äåÿ¥ÿØŸá
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # ŸÑÿß€åŸá‚ÄåŸáÿß€å ÿ™ÿ±ŸÜÿ≥ŸÅŸàÿ±ŸÖÿ± ÿ®ÿß ⁄Ü⁄©ŸæŸà€åŸÜÿ™
        self.layers = nn.ModuleList([
            CheckpointTransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # ⁄©ŸÑÿßÿ≥€åŸÅÿß€åÿ± ÿ≥ÿ®⁄©
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_classes)
        )
        
        self.dropout = nn.Dropout(dropout)
        
        # ŸÖÿØ€åÿ±€åÿ™ ÿ≠ÿßŸÅÿ∏Ÿá
        self.memory_monitor = MemoryMonitor()
        
        print("‚úÖ Model initialized successfully!")
        self.print_model_info()
    
    def forward(self, x, mask=None):
        batch_size, seq_len = x.shape
        
        # ŸÖÿßŸÜ€åÿ™Ÿàÿ±€åŸÜ⁄Ø ÿ≠ÿßŸÅÿ∏Ÿá ŸÇÿ®ŸÑ ÿßÿ≤ forward
        self.memory_monitor.print_usage("Before forward:")
        
        # ÿ®ÿ±ÿ¥ ÿ™ŸàÿßŸÑ€å ÿß⁄Øÿ± ÿ∑ŸàŸÑÿßŸÜ€å‚Äåÿ™ÿ± ÿßÿ≤ max_seq_len ÿ®ÿßÿ¥ÿØ
        if seq_len > self.max_seq_len:
            x = x[:, :self.max_seq_len]
            seq_len = self.max_seq_len
        
        # Embedding ÿ®ÿß dtype ÿ®Ÿá€åŸÜŸá
        token_emb = self.token_embedding(x)  # shape: [batch, seq_len, d_model]
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)
        pos_emb = self.position_embedding(positions)
        
        x = self.dropout(token_emb + pos_emb)
        
        # ÿ≠ÿ∞ŸÅ ÿ™ÿßŸÜÿ≥Ÿàÿ±Ÿáÿß€å ŸÖŸàŸÇÿ™
        del token_emb, pos_emb
        
        # ⁄Øÿ∞ÿ± ÿßÿ≤ ŸÑÿß€åŸá‚ÄåŸáÿß ÿ®ÿß ŸÖÿßŸÜ€åÿ™Ÿàÿ±€åŸÜ⁄Ø ÿ≠ÿßŸÅÿ∏Ÿá
        for i, layer in enumerate(self.layers):
            x = layer(x, mask)
            
            # Ÿæÿß⁄©‚Äåÿ≥ÿßÿ≤€å ÿ≠ÿßŸÅÿ∏Ÿá Ÿæÿ≥ ÿßÿ≤ Ÿáÿ± ŸÑÿß€åŸá
            if i % 2 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else gc.collect()
        
        # pooling ÿ®Ÿá€åŸÜŸá
        if mask is not None:
            x = x * mask.unsqueeze(-1)
            x = x.sum(dim=1) / mask.sum(dim=1, keepdim=True)
        else:
            x = x.mean(dim=1)
        
        output = self.classifier(x)
        
        # ŸÖÿßŸÜ€åÿ™Ÿàÿ±€åŸÜ⁄Ø ŸÜŸáÿß€å€å
        self.memory_monitor.print_usage("After forward:")
        
        return output
    
    def print_model_info(self):
        """⁄ÜÿßŸæ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ⁄©ÿßŸÖŸÑ ŸÖÿØŸÑ Ÿà ŸÖÿµÿ±ŸÅ ŸÖŸÜÿßÿ®ÿπ"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ÿ≠ÿßŸÅÿ∏Ÿá ŸÖŸàÿ±ÿØ ŸÜ€åÿßÿ≤
        param_size = total_params * 4 / (1024 ** 2)  # MB
        buffer_size = sum(b.nelement() * b.element_size() for b in self.buffers()) / (1024 ** 2)
        total_size = param_size + buffer_size
        
        print("\n" + "="*60)
        print("üéØ MODEL INFORMATION (Optimized for 512MB RAM)")
        print("="*60)
        print(f"üìä Total Parameters: {total_params:,}")
        print(f"üéì Trainable Parameters: {trainable_params:,}")
        print(f"üíæ Model Size: {param_size:.2f} MB")
        print(f"üì¶ Buffer Size: {buffer_size:.2f} MB")
        print(f"üíø Total Memory: {total_size:.2f} MB")
        print(f"üéØ Max Sequence Length: {self.max_seq_len}")
        print(f"üîß Layers: {len(self.layers)}")
        print(f"üìê Model Dimension: {self.d_model}")
        print(f"üë• Attention Heads: {self.layers[0].attention.n_heads}")
        print("="*60)
        
        # ÿ®ÿ±ÿ±ÿ≥€å ŸÖÿ≠ÿØŸàÿØ€åÿ™ ÿ≠ÿßŸÅÿ∏Ÿá
        if total_size > 100:  # 100MB threshold ÿ®ÿ±ÿß€å 512MB RAM
            print("‚ö†Ô∏è  WARNING: Model might be too large for 512MB RAM!")
        else:
            print("‚úÖ Model size is safe for 512MB RAM environment")
    
    def optimize_for_inference(self):
        """ÿ®Ÿá€åŸÜŸá‚Äåÿ≥ÿßÿ≤€å ŸÖÿØŸÑ ÿ®ÿ±ÿß€å inference"""
        print("üîß Optimizing model for inference...")
        
        # ÿ™ÿ®ÿØ€åŸÑ ÿ®Ÿá ÿ≠ÿßŸÑÿ™ evaluation
        self.eval()
        
        # ÿ∫€åÿ±ŸÅÿπÿßŸÑ ⁄©ÿ±ÿØŸÜ gradient Ÿáÿß
        for param in self.parameters():
            param.requires_grad = False
        
        # ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ torch.jit ÿ®ÿ±ÿß€å ÿ®Ÿá€åŸÜŸá‚Äåÿ≥ÿßÿ≤€å
        if hasattr(torch.jit, 'script'):
            self.forward = torch.jit.script(self.forward)
        
        print("‚úÖ Model optimized for inference!")

class MemoryAwareTrainer:
    """ÿ™ÿ±€åŸÜÿ± ÿ®ÿß ÿ¢⁄ØÿßŸá€å ÿßÿ≤ ÿ≠ÿßŸÅÿ∏Ÿá"""
    
    def __init__(self, model, learning_rate=2e-4):
        self.model = model
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=learning_rate,
            weight_decay=0.01
        )
        
        # scheduler ÿ®ÿ±ÿß€å ŸÖÿØ€åÿ±€åÿ™ €åÿßÿØ⁄Ø€åÿ±€å
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=3, factor=0.5
        )
        
    def train_epoch(self, dataloader, epoch):
        """ÿ¢ŸÖŸàÿ≤ÿ¥ €å⁄© ÿßŸæŸà⁄© ÿ®ÿß ŸÖÿØ€åÿ±€åÿ™ ÿ≠ÿßŸÅÿ∏Ÿá"""
        self.model.train()
        total_loss = 0
        steps = 0
        
        print(f"\nüéØ Starting Epoch {epoch}")
        self.model.memory_monitor.print_usage("Start of epoch:")
        
        for batch_idx, (inputs, targets) in enumerate(dataloader):
            # ŸÖÿØ€åÿ±€åÿ™ batch size ŸæŸà€åÿß ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿ≠ÿßŸÅÿ∏Ÿá ŸÖŸàÿ¨ŸàÿØ
            current_memory = self.model.memory_monitor.get_memory_usage()
            
            if current_memory['rss_mb'] > 400:  # ÿ¢ÿ≥ÿ™ÿßŸÜŸá ÿßŸÖŸÜ
                print("‚ö†Ô∏è  High memory usage, reducing batch size...")
                break
            
            self.optimizer.zero_grad()
            
            outputs = self.model(inputs)
            loss = nn.functional.cross_entropy(outputs, targets)
            
            loss.backward()
            
            # gradient clipping ÿ®ÿ±ÿß€å Ÿæÿß€åÿØÿßÿ±€å
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            steps += 1
            
            # Ÿæÿß⁄©‚Äåÿ≥ÿßÿ≤€å ÿ≠ÿßŸÅÿ∏Ÿá Ÿáÿ± 10 batch
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else gc.collect()
                self.model.memory_monitor.print_usage(f"Batch {batch_idx}:")
        
        avg_loss = total_loss / steps if steps > 0 else 0
        self.scheduler.step(avg_loss)
        
        return avg_loss

# üéØ ÿ™ÿ≥ÿ™ ŸÖÿØŸÑ ÿ®ÿß ÿ¥ÿ±ÿß€åÿ∑ ŸàÿßŸÇÿπ€å
def stress_test_model():
    """ÿ™ÿ≥ÿ™ ÿßÿ≥ÿ™ÿ±ÿ≥ ŸÖÿØŸÑ ÿ™ÿ≠ÿ™ ŸÖÿ≠ÿØŸàÿØ€åÿ™ ÿ≠ÿßŸÅÿ∏Ÿá"""
    print("\nüß™ Running Stress Test...")
    
    # ÿß€åÿ¨ÿßÿØ ŸÖÿØŸÑ ÿ®ÿß ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ŸÅŸàŸÇ ÿ®Ÿá€åŸÜŸá
    model = RenderFreeSpikeTransformer(
        vocab_size=3000,
        d_model=96,      # ÿ≠ÿ™€å ⁄©Ÿà⁄Ü⁄©‚Äåÿ™ÿ±
        n_heads=3,
        num_layers=2,    # ŸÅŸÇÿ∑ 2 ŸÑÿß€åŸá
        d_ff=192,
        max_seq_len=96,  # ÿ™ŸàÿßŸÑ€å ⁄©Ÿàÿ™ÿßŸá
        num_classes=2
    )
    
    # ÿ¥ÿ®€åŸá‚Äåÿ≥ÿßÿ≤€å ÿØÿßÿØŸá Ÿàÿ±ŸàÿØ€å
    batch_size = 8  # batch size ⁄©Ÿà⁄Ü⁄©
    seq_len = 64
    
    print(f"\nüìä Stress Test Configuration:")
    print(f"Batch Size: {batch_size}")
    print(f"Sequence Length: {seq_len}")
    
    # ÿ™ÿ≥ÿ™ ÿ≠ÿßŸÅÿ∏Ÿá
    for i in range(5):  # 5 ÿ™⁄©ÿ±ÿßÿ± ÿ®ÿ±ÿß€å ÿ™ÿ≥ÿ™
        print(f"\nüîÅ Iteration {i+1}:")
        
        # ÿß€åÿ¨ÿßÿØ ÿØÿßÿØŸá ÿ™ÿµÿßÿØŸÅ€å
        dummy_input = torch.randint(0, 3000, (batch_size, seq_len))
        dummy_target = torch.randint(0, 2, (batch_size,))
        
        # forward pass
        with torch.no_grad():
            start_time = time.time()
            output = model(dummy_input)
            inference_time = time.time() - start_time
        
        print(f"‚è±Ô∏è  Inference Time: {inference_time:.3f}s")
        
        # Ÿæÿß⁄©‚Äåÿ≥ÿßÿ≤€å
        del dummy_input, dummy_target, output
        gc.collect()
    
    print("\n‚úÖ Stress test completed successfully!")
    return model

if __name__ == "__main__":
    print("üöÄ Creating Ultra-Efficient Spike Transformer for Render Free Tier...")
    
    # ÿ™ÿ≥ÿ™ ÿßÿ≥ÿ™ÿ±ÿ≥ ÿßŸàŸÑ€åŸá
    model = stress_test_model()
    
    # ŸÜŸÖÿß€åÿ¥ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ŸÜŸáÿß€å€å
    print("\n" + "="*60)
    print("üéâ MODEL READY FOR DEPLOYMENT!")
    print("="*60)
    print("‚úÖ Optimized for 512MB RAM")
    print("‚úÖ Memory monitoring enabled") 
    print("‚úÖ Gradient checkpointing active")
    print("‚úÖ Efficient attention implemented")
    print("‚úÖ Safe for Render free tier")
    print("="*60)
    
    # ŸÖÿßŸÜ€åÿ™Ÿàÿ±€åŸÜ⁄Ø ŸÜŸáÿß€å€å
    model.memory_monitor.print_usage("Final:")
