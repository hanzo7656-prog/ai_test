# ml_core/model_manager.py
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Any
import logging
from datetime import datetime
import asyncio

logger = logging.getLogger(__name__)

class MLModelManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù…ØªÙ…Ø±Ú©Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    
    def __init__(self):
        self.active_models = {}
        self.model_versions = {}
        self.performance_metrics = {}
        
        # Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ù…ÙˆØ¬ÙˆØ¯
        from debug_system.storage.cache_debugger import cache_debugger
        self.cache_manager = cache_debugger
        
        logger.info("ğŸ§  ML Model Manager initialized")

    def register_model(self, model_name: str, model: nn.Module, version: str = "1.0.0"):
        """Ø«Ø¨Øª Ù…Ø¯Ù„ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ø³ÛŒØ³ØªÙ…"""
        self.active_models[model_name] = {
            'model': model,
            'version': version,
            'created_at': datetime.now(),
            'last_used': datetime.now(),
            'performance': {}
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ metadata Ø¯Ø± Ú©Ø´ UTA
        model_metadata = {
            'name': model_name,
            'version': version,
            'parameters': sum(p.numel() for p in model.parameters()),
            'architecture': str(model.__class__.__name__),
            'registered_at': datetime.now().isoformat()
        }
        
        self.cache_manager.set_data("uta", f"model_meta:{model_name}", model_metadata, expire=86400)
        logger.info(f"âœ… Model registered: {model_name} v{version}")

    async def predict(self, model_name: str, input_data: torch.Tensor) -> Dict[str, Any]:
        """Ø§Ù†Ø¬Ø§Ù… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ù…Ø´Ø®Øµ"""
        if model_name not in self.active_models:
            raise ValueError(f"Model {model_name} not found")
        
        model_info = self.active_models[model_name]
        model = model_info['model']
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ù†ØªÛŒØ¬Ù‡ Ù…Ø´Ø§Ø¨Ù‡
        cache_key = f"prediction:{model_name}:{self._tensor_hash(input_data)}"
        cached_result = self.cache_manager.get_data("uta", cache_key)
        
        if cached_result is not None:
            logger.info(f"âœ… Prediction cache HIT for {model_name}")
            return cached_result
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        model.eval()
        with torch.no_grad():
            start_time = datetime.now()
            output = model(input_data)
            inference_time = (datetime.now() - start_time).total_seconds()
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø±ÙˆØ¬ÛŒ
        result = self._process_model_output(model_name, output)
        result['inference_time'] = inference_time
        result['model_version'] = model_info['version']
        result['timestamp'] = datetime.now().isoformat()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
        self.cache_manager.set_data("uta", cache_key, result, expire=300)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
        self._update_performance_metrics(model_name, inference_time, True)
        
        return result

    def _tensor_hash(self, tensor: torch.Tensor) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ø´ Ø§Ø² ØªØ§Ù†Ø³ÙˆØ± Ø¨Ø±Ø§ÛŒ Ú©Ù„ÛŒØ¯ Ú©Ø´"""
        import hashlib
        tensor_str = str(tensor.shape) + str(tensor.sum().item())
        return hashlib.md5(tensor_str.encode()).hexdigest()

    def _process_model_output(self, model_name: str, output: Any) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹"""
        if isinstance(output, dict):
            return output
        elif isinstance(output, torch.Tensor):
            return {
                'predictions': output.cpu().numpy().tolist(),
                'confidence': torch.max(torch.softmax(output, dim=-1)).item()
            }
        else:
            return {'raw_output': str(output)}

    def _update_performance_metrics(self, model_name: str, inference_time: float, success: bool):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        if model_name not in self.performance_metrics:
            self.performance_metrics[model_name] = {
                'total_predictions': 0,
                'successful_predictions': 0,
                'total_inference_time': 0,
                'average_inference_time': 0
            }
        
        metrics = self.performance_metrics[model_name]
        metrics['total_predictions'] += 1
        metrics['total_inference_time'] += inference_time
        metrics['average_inference_time'] = metrics['total_inference_time'] / metrics['total_predictions']
        
        if success:
            metrics['successful_predictions'] += 1
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯
        self.cache_manager.set_data("uta", f"metrics:{model_name}", metrics, expire=3600)

    def get_model_health(self, model_name: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ù…Ø¯Ù„"""
        if model_name not in self.active_models:
            return {'status': 'not_found', 'health': 'unknown'}
        
        model_info = self.active_models[model_name]
        metrics = self.performance_metrics.get(model_name, {})
        
        return {
            'status': 'active',
            'health': 'healthy' if metrics.get('success_rate', 1) > 0.95 else 'degraded',
            'version': model_info['version'],
            'uptime': (datetime.now() - model_info['created_at']).total_seconds(),
            'performance_metrics': metrics,
            'last_used': model_info['last_used'].isoformat()
        }

    async def batch_predict(self, model_name: str, batch_data: List[torch.Tensor]) -> List[Dict[str, Any]]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ"""
        results = []
        for data in batch_data:
            result = await self.predict(model_name, data)
            results.append(result)
        return results

# Ù†Ù…ÙˆÙ†Ù‡ global Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³Ø±Ø§Ø³Ø± Ø³ÛŒØ³ØªÙ…
ml_model_manager = MLModelManager()
