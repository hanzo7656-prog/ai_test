# ml_core/data_integration.py
import logging
import aiohttp
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
import json

logger = logging.getLogger(__name__)

class DataIntegration:
    """ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø² routes Ù…Ø®ØªÙ„Ù"""
    
    def __init__(self):
        self.base_url = os.getenv("SERVICE_URL")  # Ø¢Ø¯Ø±Ø³ Ø³Ø±ÙˆØ± Ø§ØµÙ„ÛŒ
        self.timeout = aiohttp.ClientTimeout(total=30)
        
        # Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ù…ÙˆØ¬ÙˆØ¯
        from debug_system.storage.cache_debugger import cache_debugger
        self.cache_manager = cache_debugger
        
        logger.info("ðŸ”— Data Integration initialized")

    async def collect_raw_data(self) -> Dict[str, Any]:
        """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Û´ Ø±ÙˆØª Ø®Ø§Ù…"""
        raw_data = {
            'timestamp': datetime.now().isoformat(),
            'sources': {},
            'metadata': {
                'total_sources': 4,
                'successful_sources': 0,
                'failed_sources': 0
            }
        }
        
        # ØªØ¹Ø±ÛŒÙ Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…
        data_sources = {
            'raw_coins': '/api/raw_data/coins',
            'raw_exchanges': '/api/raw_data/exchanges',
            'raw_news': '/api/raw_data/news', 
            'raw_insights': '/api/raw_data/insights'
        }
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        tasks = []
        for source_name, endpoint in data_sources.items():
            task = self._fetch_from_source(source_name, endpoint)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬
        for i, (source_name, endpoint) in enumerate(data_sources.items()):
            result = results[i]
            
            if isinstance(result, Exception):
                raw_data['sources'][source_name] = {
                    'status': 'error',
                    'error': str(result),
                    'endpoint': endpoint
                }
                raw_data['metadata']['failed_sources'] += 1
                logger.error(f"âŒ Failed to fetch {source_name}: {result}")
            else:
                raw_data['sources'][source_name] = {
                    'status': 'success',
                    'data': result,
                    'endpoint': endpoint,
                    'data_size': len(str(result))
                }
                raw_data['metadata']['successful_sources'] += 1
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± Ú©Ø´ UTC
        if raw_data['metadata']['successful_sources'] > 0:
            cache_key = f"raw_data_batch:{datetime.now().strftime('%Y%m%d_%H%M')}"
            self.cache_manager.set_data("utc", cache_key, raw_data, expire=1800)  # 30 Ø¯Ù‚ÛŒÙ‚Ù‡
            
            logger.info(f"âœ… Collected data from {raw_data['metadata']['successful_sources']}/4 sources")
        else:
            logger.warning("âš ï¸ No data collected from any source")
        
        return raw_data

    async def _fetch_from_source(self, source_name: str, endpoint: str) -> Any:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ù…Ù†Ø¨Ø¹ Ø®Ø§Øµ"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´ Ø§ÙˆÙ„
            cache_key = f"source_cache:{source_name}"
            cached_data = self.cache_manager.get_data("utc", cache_key)
            
            if cached_data is not None:
                logger.info(f"âœ… Cache HIT for {source_name}")
                return cached_data
            
            # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø§Ø² API
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(f"{self.base_url}{endpoint}") as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
                        self.cache_manager.set_data("utc", cache_key, data, expire=300)  # 5 Ø¯Ù‚ÛŒÙ‚Ù‡
                        
                        logger.info(f"âœ… Fetched fresh data for {source_name}")
                        return data
                    else:
                        raise Exception(f"HTTP {response.status}: {await response.text()}")
                        
        except asyncio.TimeoutError:
            raise Exception(f"Timeout while fetching {source_name}")
        except Exception as e:
            raise Exception(f"Error fetching {source_name}: {str(e)}")

    async def get_structured_training_data(self) -> Dict[str, Any]:
        """ØªÙ‡ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„"""
        try:
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…
            raw_data = await self.collect_raw_data()
            
            # Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
            structured_data = {
                'timestamp': datetime.now().isoformat(),
                'training_ready': False,
                'datasets': {},
                'statistics': {
                    'total_samples': 0,
                    'feature_count': 0,
                    'data_quality': 'unknown'
                }
            }
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡
            for source_name, source_data in raw_data['sources'].items():
                if source_data['status'] == 'success':
                    processed = self._process_data_source(source_name, source_data['data'])
                    structured_data['datasets'][source_name] = processed
                    structured_data['statistics']['total_samples'] += processed.get('sample_count', 0)
            
            # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡
            if structured_data['statistics']['total_samples'] > 0:
                structured_data['training_ready'] = True
                structured_data['statistics']['data_quality'] = self._assess_data_quality(structured_data)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ UTB Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
            if structured_data['training_ready']:
                cache_key = "training_data:latest"
                self.cache_manager.set_data("utb", cache_key, structured_data, expire=3600)  # 1 Ø³Ø§Ø¹Øª
                
                logger.info(f"âœ… Prepared training data with {structured_data['statistics']['total_samples']} samples")
            
            return structured_data
            
        except Exception as e:
            logger.error(f"âŒ Error preparing training data: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'training_ready': False,
                'error': str(e),
                'datasets': {},
                'statistics': {'total_samples': 0, 'data_quality': 'poor'}
            }

    def _process_data_source(self, source_name: str, raw_data: Any) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ù…Ù†Ø¨Ø¹ Ø®Ø§Øµ"""
        try:
            if source_name == 'raw_coins':
                return self._process_coins_data(raw_data)
            elif source_name == 'raw_exchanges':
                return self._process_exchanges_data(raw_data)
            elif source_name == 'raw_news':
                return self._process_news_data(raw_data)
            elif source_name == 'raw_insights':
                return self._process_insights_data(raw_data)
            else:
                return {'sample_count': 0, 'features': [], 'error': 'Unknown source'}
                
        except Exception as e:
            logger.error(f"âŒ Error processing {source_name}: {e}")
            return {'sample_count': 0, 'features': [], 'error': str(e)}

    def _process_coins_data(self, data: Any) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÛŒÙ†â€ŒÙ‡Ø§"""
        # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§
        return {
            'sample_count': len(data) if isinstance(data, list) else 1,
            'features': ['price', 'volume', 'market_cap', 'change_24h'],
            'data_type': 'numeric',
            'processing_time': datetime.now().isoformat()
        }

    def _process_exchanges_data(self, data: Any) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØµØ±Ø§ÙÛŒâ€ŒÙ‡Ø§"""
        return {
            'sample_count': len(data) if isinstance(data, list) else 1,
            'features': ['volume', 'pairs', 'liquidity', 'fees'],
            'data_type': 'numeric',
            'processing_time': datetime.now().isoformat()
        }

    def _process_news_data(self, data: Any) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ"""
        return {
            'sample_count': len(data) if isinstance(data, list) else 1,
            'features': ['sentiment', 'topics', 'urgency', 'relevance'],
            'data_type': 'textual',
            'processing_time': datetime.now().isoformat()
        }

    def _process_insights_data(self, data: Any) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ"""
        return {
            'sample_count': len(data) if isinstance(data, list) else 1,
            'features': ['analysis_depth', 'confidence', 'trends', 'patterns'],
            'data_type': 'analytical',
            'processing_time': datetime.now().isoformat()
        }

    def _assess_data_quality(self, structured_data: Dict[str, Any]) -> str:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡"""
        total_samples = structured_data['statistics']['total_samples']
        source_count = len([d for d in structured_data['datasets'].values() if d.get('sample_count', 0) > 0])
        
        if total_samples > 1000 and source_count >= 3:
            return 'excellent'
        elif total_samples > 500 and source_count >= 2:
            return 'good'
        elif total_samples > 100:
            return 'fair'
        else:
            return 'poor'

    async def validate_data_sources(self) -> Dict[str, Any]:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡"""
        validation_report = {
            'timestamp': datetime.now().isoformat(),
            'sources': {},
            'overall_status': 'healthy'
        }
        
        data_sources = {
            'raw_coins': '/api/raw_data/coins',
            'raw_exchanges': '/api/raw_data/exchanges',
            'raw_news': '/api/raw_data/news',
            'raw_insights': '/api/raw_data/insights'
        }
        
        for source_name, endpoint in data_sources.items():
            try:
                async with aiohttp.ClientSession(timeout=self.timeout) as session:
                    async with session.get(f"{self.base_url}{endpoint}") as response:
                        status = 'available' if response.status == 200 else 'unavailable'
                        validation_report['sources'][source_name] = {
                            'status': status,
                            'response_time': 'N/A',  # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø® Ø±Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ú¯ÛŒØ±ÛŒØ¯
                            'endpoint': endpoint
                        }
                        
                        if status == 'unavailable':
                            validation_report['overall_status'] = 'degraded'
                            
            except Exception as e:
                validation_report['sources'][source_name] = {
                    'status': 'error',
                    'error': str(e),
                    'endpoint': endpoint
                }
                validation_report['overall_status'] = 'degraded'
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        self.cache_manager.set_data("mother_a", "data_validation_report", validation_report, expire=600)
        
        return validation_report

# Ù†Ù…ÙˆÙ†Ù‡ global
data_integrator = DataIntegration()
