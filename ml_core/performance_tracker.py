# ml_core/performance_tracker.py
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict, deque
import numpy as np

logger = logging.getLogger(__name__)

class PerformanceTracker:
    """Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.performance_history = defaultdict(lambda: deque(maxlen=1000))  # Ø°Ø®ÛŒØ±Ù‡ 1000 Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø®ÛŒØ±
        self.alert_thresholds = {
            'inference_time': 5.0,  # Ø«Ø§Ù†ÛŒÙ‡
            'memory_usage': 0.9,    # 90%
            'error_rate': 0.05,     # 5%
            'confidence_drop': 0.2   # 20% Ú©Ø§Ù‡Ø´
        }
        self.alerts = []
        
        # Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ù…ÙˆØ¬ÙˆØ¯
        from debug_system.storage.cache_debugger import cache_debugger
        self.cache_manager = cache_debugger
        
        logger.info("ðŸ“Š Performance Tracker initialized")

    def track_inference(self, model_name: str, inference_time: float, 
                       confidence: float, success: bool, input_size: tuple):
        """Ø±Ø¯ÛŒØ§Ø¨ÛŒ ÛŒÚ© inference Ø¬Ø¯ÛŒØ¯"""
        timestamp = datetime.now()
        
        metrics = {
            'timestamp': timestamp.isoformat(),
            'inference_time': inference_time,
            'confidence': confidence,
            'success': success,
            'input_size': input_size,
            'throughput': 1 / inference_time if inference_time > 0 else 0
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.performance_history[model_name].append(metrics)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
        self._check_alerts(model_name, metrics)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ
        self._update_aggregate_metrics(model_name)
        
        logger.debug(f"ðŸ“ˆ Tracked inference for {model_name}: {inference_time:.3f}s")

    def _check_alerts(self, model_name: str, metrics: Dict[str, Any]):
        """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ø´Ø¯Ø§Ø± Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²"""
        alerts_triggered = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø²Ù…Ø§Ù† inference
        if metrics['inference_time'] > self.alert_thresholds['inference_time']:
            alerts_triggered.append({
                'type': 'slow_inference',
                'model': model_name,
                'value': metrics['inference_time'],
                'threshold': self.alert_thresholds['inference_time'],
                'timestamp': metrics['timestamp']
            })
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ù‡Ø´ confidence
        recent_confidence = self._get_recent_confidence(model_name)
        if recent_confidence and metrics['confidence'] < recent_confidence - self.alert_thresholds['confidence_drop']:
            alerts_triggered.append({
                'type': 'confidence_drop',
                'model': model_name,
                'current': metrics['confidence'],
                'previous_avg': recent_confidence,
                'drop_amount': recent_confidence - metrics['confidence'],
                'timestamp': metrics['timestamp']
            })
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
        for alert in alerts_triggered:
            self.alerts.append(alert)
            logger.warning(f"ðŸš¨ Performance alert: {alert['type']} for {model_name}")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù‡Ø´Ø¯Ø§Ø±
            self.cache_manager.set_data(
                "uta", 
                f"alert:{model_name}:{datetime.now().timestamp()}", 
                alert, 
                expire=3600
            )

    def _get_recent_confidence(self, model_name: str, window: int = 50) -> Optional[float]:
        """Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† confidence Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±"""
        history = list(self.performance_history[model_name])
        if len(history) < window:
            return None
            
        recent = history[-window:]
        confidences = [m['confidence'] for m in recent if m['success']]
        return np.mean(confidences) if confidences else None

    def _update_aggregate_metrics(self, model_name: str):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ"""
        history = list(self.performance_history[model_name])
        if not history:
            return
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ
        successful_inferences = [m for m in history if m['success']]
        failed_inferences = [m for m in history if not m['success']]
        
        aggregate_metrics = {
            'total_inferences': len(history),
            'successful_inferences': len(successful_inferences),
            'failed_inferences': len(failed_inferences),
            'success_rate': len(successful_inferences) / len(history) if history else 0,
            'avg_inference_time': np.mean([m['inference_time'] for m in successful_inferences]) if successful_inferences else 0,
            'avg_confidence': np.mean([m['confidence'] for m in successful_inferences]) if successful_inferences else 0,
            'throughput_1min': self._calculate_throughput(model_name, window=60),
            'throughput_5min': self._calculate_throughput(model_name, window=300),
            'last_updated': datetime.now().isoformat()
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
        self.cache_manager.set_data(
            "uta", 
            f"aggregate_metrics:{model_name}", 
            aggregate_metrics, 
            expire=600
        )

    def _calculate_throughput(self, model_name: str, window: int) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ throughput Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ (Ø«Ø§Ù†ÛŒÙ‡)"""
        cutoff_time = datetime.now() - timedelta(seconds=window)
        recent_inferences = [
            m for m in self.performance_history[model_name]
            if datetime.fromisoformat(m['timestamp']) > cutoff_time
        ]
        return len(recent_inferences) / window if recent_inferences else 0

    def get_model_performance(self, model_name: str, time_window: str = "1h") -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ"""
        try:
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡
            window_seconds = {
                "1h": 3600,
                "6h": 21600,
                "24h": 86400,
                "7d": 604800
            }.get(time_window, 3600)
            
            cutoff_time = datetime.now() - timedelta(seconds=window_seconds)
            
            # ÙÛŒÙ„ØªØ± ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
            history = [
                m for m in self.performance_history[model_name]
                if datetime.fromisoformat(m['timestamp']) > cutoff_time
            ]
            
            if not history:
                return {
                    'model': model_name,
                    'time_window': time_window,
                    'total_inferences': 0,
                    'message': 'No data available for the specified time window'
                }
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚
            successful = [m for m in history if m['success']]
            failed = [m for m in history if not m['success']]
            
            inference_times = [m['inference_time'] for m in successful]
            confidences = [m['confidence'] for m in successful]
            
            performance_report = {
                'model': model_name,
                'time_window': time_window,
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_inferences': len(history),
                    'successful_inferences': len(successful),
                    'failed_inferences': len(failed),
                    'success_rate': len(successful) / len(history),
                    'error_rate': len(failed) / len(history)
                },
                'timing_metrics': {
                    'avg_inference_time': np.mean(inference_times) if inference_times else 0,
                    'std_inference_time': np.std(inference_times) if inference_times else 0,
                    'p95_inference_time': np.percentile(inference_times, 95) if inference_times else 0,
                    'min_inference_time': min(inference_times) if inference_times else 0,
                    'max_inference_time': max(inference_times) if inference_times else 0
                },
                'quality_metrics': {
                    'avg_confidence': np.mean(confidences) if confidences else 0,
                    'std_confidence': np.std(confidences) if confidences else 0,
                    'min_confidence': min(confidences) if confidences else 0,
                    'max_confidence': max(confidences) if confidences else 0
                },
                'throughput_metrics': {
                    'current_throughput': self._calculate_throughput(model_name, window=60),
                    'avg_throughput': len(history) / window_seconds,
                    'peak_throughput': self._find_peak_throughput(model_name, window_seconds)
                }
            }
            
            return performance_report
            
        except Exception as e:
            logger.error(f"âŒ Error generating performance report for {model_name}: {e}")
            return {
                'model': model_name,
                'time_window': time_window,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def _find_peak_throughput(self, model_name: str, window_seconds: int) -> float:
        """Ù¾ÛŒØ¯Ø§Ú©Ø±Ø¯Ù† peak throughput Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ"""
        # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ - Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø´ÙˆØ¯
        history = list(self.performance_history[model_name])
        if not history:
            return 0
        
        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ù‚ÛŒÙ‚Ù‡
        minute_groups = defaultdict(int)
        for metric in history:
            dt = datetime.fromisoformat(metric['timestamp'])
            minute_key = dt.strftime("%Y%m%d%H%M")
            minute_groups[minute_key] += 1
        
        return max(minute_groups.values()) / 60 if minute_groups else 0  # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø± Ø«Ø§Ù†ÛŒÙ‡

    def get_comparative_analysis(self) -> Dict[str, Any]:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        comparative_report = {
            'timestamp': datetime.now().isoformat(),
            'models': {},
            'rankings': {}
        }
        
        for model_name in self.model_manager.active_models.keys():
            try:
                performance = self.get_model_performance(model_name, "24h")
                comparative_report['models'][model_name] = performance
            except Exception as e:
                logger.error(f"âŒ Error analyzing {model_name}: {e}")
                comparative_report['models'][model_name] = {'error': str(e)}
        
        # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        if comparative_report['models']:
            # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ success rate
            success_rates = {
                name: data.get('summary', {}).get('success_rate', 0)
                for name, data in comparative_report['models'].items()
                if 'error' not in data
            }
            comparative_report['rankings']['by_success_rate'] = dict(
                sorted(success_rates.items(), key=lambda x: x[1], reverse=True)
            )
            
            # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø±Ø¹Øª
            speeds = {
                name: data.get('timing_metrics', {}).get('avg_inference_time', float('inf'))
                for name, data in comparative_report['models'].items()
                if 'error' not in data
            }
            comparative_report['rankings']['by_speed'] = dict(
                sorted(speeds.items(), key=lambda x: x[1])
            )
        
        return comparative_report

    def get_active_alerts(self) -> List[Dict[str, Any]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„"""
        # ÙÛŒÙ„ØªØ± Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø§Ø®ÛŒØ± (Û± Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡)
        cutoff_time = datetime.now() - timedelta(hours=1)
        recent_alerts = [
            alert for alert in self.alerts
            if datetime.fromisoformat(alert['timestamp']) > cutoff_time
        ]
        return recent_alerts

    def clear_old_data(self, days_old: int = 7):
        """Ù¾Ø§Ú©â€ŒÚ©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        cutoff_time = datetime.now() - timedelta(days=days_old)
        
        for model_name in self.performance_history.keys():
            self.performance_history[model_name] = deque([
                m for m in self.performance_history[model_name]
                if datetime.fromisoformat(m['timestamp']) > cutoff_time
            ], maxlen=1000)
        
        logger.info(f"ðŸ§¹ Cleared performance data older than {days_old} days")

# Ù†Ù…ÙˆÙ†Ù‡ global
performance_tracker = None

def initialize_performance_tracker(model_manager):
    """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ performance tracker"""
    global performance_tracker
    performance_tracker = PerformanceTracker(model_manager)
    return performance_tracker
