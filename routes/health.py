from fastapi import APIRouter, HTTPException, BackgroundTasks, Query, WebSocket, WebSocketDisconnect
from fastapi.responses import JSONResponse, HTMLResponse
from datetime import datetime, timedelta
import asyncio
import json
import time
from typing import Dict, List, Optional, Any
import psutil
import logging
import os

logger = logging.getLogger(__name__)

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø³ÛŒØ³ØªÙ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÛŒØ¯
try:
    from debug_system.utils.data_normalizer import data_normalizer
except ImportError:
    # Fallback Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ§Ù‚Ø¹ ØªÙˆØ³Ø¹Ù‡
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from debug_system.utils.data_normalizer import data_normalizer

# ğŸ”½ Ø§ÛŒÙ† import Ø±Ùˆ Ú†Ú© Ú©Ù†
try:
    from debug_system.storage.smart_cache_system import smart_cache
    logger.info("âœ… Smart Cache imported successfully")
except ImportError as e:
    logger.error(f"âŒ Smart Cache import failed: {e}")
    smart_cache = None
    
# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª complete_coinstats_manager Ø¨Ø±Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª API
try:
    from complete_coinstats_manager import coin_stats_manager
except ImportError:
    coin_stats_manager = None


# Ø§ÛŒØ¬Ø§Ø¯ Ø±ÙˆØªâ€ŒØ± Ø³Ù„Ø§Ù…Øª
health_router = APIRouter(prefix="/api/health", tags=["Health & Debug"])

# ==================== LAZY DEBUG SYSTEM IMPORTS ====================

class DebugSystemManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª lazy loading Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯"""
    
    _initialized = False
    _modules = {}
    
    @classmethod
    def initialize(cls):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ lazy Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯ - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡"""
        if cls._initialized:
            return cls._modules
        
        try:
            logger.info("ğŸ”„ Initializing debug system (lazy loading)...")
            
            # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª core modules - Ø§ÛŒÙ†Ù‡Ø§ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ÛŒØ¯ Ú©Ø§Ø± Ú©Ù†Ù†Ø¯
            from debug_system.core.debug_manager import debug_manager
            from debug_system.core.metrics_collector import metrics_collector
            from debug_system.core.alert_manager import alert_manager, AlertLevel, AlertType
            
            cls._modules.update({
                'debug_manager': debug_manager,
                'metrics_collector': metrics_collector,
                'alert_manager': alert_manager,
                'AlertLevel': AlertLevel,
                'AlertType': AlertType
            })
            
            # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª monitors - Ø¨Ø§ dependency injection Ø¯Ø±Ø³Øª
            try:
                from debug_system.monitors.endpoint_monitor import EndpointMonitor
                from debug_system.monitors.system_monitor import SystemMonitor
                from debug_system.monitors.performance_monitor import PerformanceMonitor
                from debug_system.monitors.security_monitor import SecurityMonitor
                
                # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ dependencyÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
                endpoint_monitor = EndpointMonitor(debug_manager)
                system_monitor = SystemMonitor(metrics_collector, alert_manager)
                performance_monitor = PerformanceMonitor(debug_manager, alert_manager)
                security_monitor = SecurityMonitor(alert_manager)
                
                cls._modules.update({
                    'endpoint_monitor': endpoint_monitor,
                    'system_monitor': system_monitor,
                    'performance_monitor': performance_monitor,
                    'security_monitor': security_monitor
                })
                
                logger.info("âœ… Monitors initialized with dependency injection")
                
            except ImportError as e:
                logger.warning(f"âš ï¸ Could not load monitors: {e}")
            except Exception as e:
                logger.error(f"âŒ Error initializing monitors: {e}")
            
            # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª storage
            try:
                from debug_system.storage.history_manager import history_manager
                from debug_system.storage.cache_debugger import cache_debugger
                
                cls._modules.update({
                    'history_manager': history_manager,
                    'cache_debugger': cache_debugger
                })
                
                logger.info("âœ… Storage modules loaded")
                
            except ImportError as e:
                logger.warning(f"âš ï¸ Could not load storage: {e}")
            except Exception as e:
                logger.error(f"âŒ Error loading storage: {e}")
            
            # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª realtime
            try:
                from debug_system.realtime.live_dashboard import LiveDashboardManager
                from debug_system.realtime.console_stream import ConsoleStreamManager
                
                # Ø§ÛŒØ¬Ø§Ø¯ live dashboard Ø¨Ø§ dependency
                live_dashboard = LiveDashboardManager(debug_manager, metrics_collector)
                console_stream = ConsoleStreamManager()
                
                cls._modules.update({
                    'live_dashboard': live_dashboard,
                    'console_stream': console_stream
                })
                
                logger.info("âœ… Realtime modules initialized")
                
            except ImportError as e:
                logger.warning(f"âš ï¸ Could not load realtime: {e}")
            except Exception as e:
                logger.error(f"âŒ Error initializing realtime: {e}")
            
            # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª tools - Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ Ø§ØµÙ„ÛŒ Ø¨ÙˆØ¯!
            try:
                from debug_system.tools.report_generator import ReportGenerator
                from debug_system.tools.dev_tools import DevTools
                from debug_system.tools.testing_tools import TestingTools
                
                # Ø§ÛŒØ¬Ø§Ø¯ tools Ø¨Ø§ dependencyÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
                history_manager_instance = cls._modules.get('history_manager')
                report_generator = ReportGenerator(debug_manager, history_manager_instance)
                dev_tools = DevTools(debug_manager)
                testing_tools = TestingTools(debug_manager)
                
                cls._modules.update({
                    'report_generator': report_generator,
                    'dev_tools': dev_tools,
                    'testing_tools': testing_tools
                })
                
                logger.info("âœ… Tools initialized with dependencies")
                
            except ImportError as e:
                logger.error(f"âŒ Could not load tools: {e}")
            except Exception as e:
                logger.error(f"âŒ Error initializing tools: {e}")
            
            cls._initialized = True
            
            # Ù„Ø§Ú¯ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ load Ø´Ø¯Ù‡
            loaded_modules = [name for name, module in cls._modules.items() if module is not None]
            failed_modules = [name for name, module in cls._modules.items() if module is None]
            
            logger.info(f"âœ… Debug system initialization completed")
            logger.info(f"ğŸ“¦ Loaded modules ({len(loaded_modules)}): {loaded_modules}")
            
            if failed_modules:
                logger.warning(f"âš ï¸ Failed modules ({len(failed_modules)}): {failed_modules}")
            
        except Exception as e:
            logger.error(f"âŒ Debug system initialization failed: {e}")
            # Ø­ØªÛŒ Ø§Ú¯Ø± Ø®Ø·Ø§ Ø¯Ø§Ø¯ØŒ Ø­Ø¯Ø§Ù‚Ù„ core modules Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
            cls._modules = cls._modules or {}
        
        return cls._modules
    
    @classmethod
    def get_module(cls, module_name: str, default=None):
        """Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© Ù…Ø§Ú˜ÙˆÙ„ Ø§Ø² Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯ - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡"""
        if not cls._initialized:
            cls.initialize()
        
        module = cls._modules.get(module_name, default)
        
        # Ø§Ú¯Ø± Ù…Ø§Ú˜ÙˆÙ„ None Ø¨Ø§Ø´Ø¯ØŒ Ù¾ÛŒØ§Ù… Ø®Ø·Ø§ÛŒ Ù…ÙÛŒØ¯
        if module is None and module_name in cls._modules:
            logger.warning(f"âš ï¸ Module '{module_name}' is None")
        
        return module
    
    @classmethod
    def is_available(cls):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª"""
        if not cls._initialized:
            cls.initialize()
    
        debug_manager = cls._modules.get('debug_manager')
        if debug_manager and hasattr(debug_manager, 'is_active'):
            return debug_manager.is_active()
        return bool(debug_manager)
        
    @classmethod
    def get_status_report(cls):
        """Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯"""
        if not cls._initialized:
            cls.initialize()
        
        loaded_modules = [name for name, module in cls._modules.items() if module is not None]
        failed_modules = [name for name, module in cls._modules.items() if module is None]
        
        return {
            'initialized': cls._initialized,
            'total_modules': len(cls._modules),
            'loaded_modules': len(loaded_modules),
            'failed_modules': len(failed_modules),
            'available_modules': loaded_modules,
            'missing_modules': failed_modules,
            'core_available': bool(cls._modules.get('debug_manager'))
        }

# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¢Ø³Ø§Ù† Ø¨Ù‡ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§
def get_debug_module(module_name: str):
    """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø§Ú˜ÙˆÙ„ Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ - Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ"""
    module = DebugSystemManager.get_module(module_name)
    
    if module is None:
        status_report = DebugSystemManager.get_status_report()
        
        logger.error(f"âŒ Debug module '{module_name}' is not available. Status: {status_report}")
        
        raise HTTPException(
            status_code=503, 
            detail={
                "error": f"Debug module '{module_name}' not properly initialized",
                "system_status": status_report,
                "hint": "Check server logs for initialization errors"
            }
        )
    
    return module
# ==================== HELPER FUNCTIONS ====================

def _check_cache_availability() -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… Ú©Ø´"""
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„: Smart Cache
        if smart_cache and hasattr(smart_cache, 'get_health_status'):
            cache_health = smart_cache.get_health_status()
            smart_cache_ok = cache_health.get("status") == "healthy"
        else:
            smart_cache_ok = False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÙˆÙ…: Redis
        from debug_system.storage import redis_manager
        redis_health = redis_manager.health_check()
        redis_ok = redis_health.get("status") == "connected"
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³ÙˆÙ…: Cache Debugger
        cache_debugger_ok = False
        try:
            from debug_system.storage.cache_debugger import cache_debugger
            cache_debugger_ok = hasattr(cache_debugger, 'get_cache_stats')
        except ImportError:
            cache_debugger_ok = False
        
        # Ø§Ú¯Ø± Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ Ú©Ø§Ø± Ú©Ù†Ø¯ØŒ Ú©Ø´ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª
        return smart_cache_ok or redis_ok or cache_debugger_ok
        
    except Exception as e:
        logger.warning(f"âš ï¸ Cache availability check failed: {e}")
        return False

def _check_normalization_availability() -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø±"""
    try:
        # ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø±
        test_data = {"test": "data"}
        result = data_normalizer.normalize_data(test_data, "health_check")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø±
        metrics = data_normalizer.get_health_metrics()
        return metrics.success_rate > 0 or metrics.total_processed > 0
        
    except Exception as e:
        logger.warning(f"âš ï¸ Normalization availability check failed: {e}")
        return False

def _check_external_apis_availability() -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ¶Ø¹ÛŒØª APIÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ"""
    try:
        if not coin_stats_manager:
            logger.warning("âš ï¸ coin_stats_manager is None")
            return False
        
        if not hasattr(coin_stats_manager, 'get_api_status'):
            logger.warning("âš ï¸ coin_stats_manager has no get_api_status method")
            return False
        
        # ØªØ³Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ API
        api_status = coin_stats_manager.get_api_status()
        logger.info(f"ğŸ” API Status Check: {api_status}")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú†Ù†Ø¯ÛŒÙ† Ø­Ø§Ù„Øª Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        status = api_status.get('status')
        if status == 'healthy':
            return True
        elif status == 'connected':
            return True
        elif 'error' in api_status:
            logger.warning(f"âš ï¸ API has error: {api_status.get('error')}")
            return False
        else:
            # Ø§Ú¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù…Ø´Ø®Øµ Ù†ÛŒØ³ØªØŒ ØªØ³Øª Ø³Ø±ÛŒØ¹ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡
            return _test_api_connection_quick()
            
    except Exception as e:
        logger.warning(f"âš ï¸ External APIs availability check failed: {e}")
        return False

def _test_api_connection_quick() -> bool:
    """ØªØ³Øª Ø³Ø±ÛŒØ¹ Ø§ØªØµØ§Ù„ Ø¨Ù‡ API"""
    try:
        # ÛŒÚ© Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØ³Øª Ø³Ø±ÛŒØ¹ Ø¨Ù‡ API
        if hasattr(coin_stats_manager, '_make_api_request'):
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØ¯ Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
            result = coin_stats_manager._make_api_request('coins', {'limit': 1})
            return result is not None
        return False
    except Exception as e:
        logger.warning(f"âš ï¸ API quick test failed: {e}")
        return False

def _get_cache_details() -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª ÙˆØ¶Ø¹ÛŒØª Ú©Ø´"""
    details = {
        "smart_cache_available": False,
        "redis_available": False,
        "cache_debugger_available": False,
        "overall_status": "unavailable"
    }
    
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ Smart Cache
        if smart_cache and hasattr(smart_cache, 'get_health_status'):
            details["smart_cache_available"] = True
            details["smart_cache_health"] = smart_cache.get_health_status()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Redis
        from debug_system.storage import redis_manager
        redis_health = redis_manager.health_check()
        details["redis_available"] = redis_health.get("status") == "connected"
        details["redis_health"] = redis_health
        
        # Ø¨Ø±Ø±Ø³ÛŒ Cache Debugger
        try:
            from debug_system.storage.cache_debugger import cache_debugger
            details["cache_debugger_available"] = hasattr(cache_debugger, 'get_cache_stats')
        except ImportError:
            details["cache_debugger_available"] = False
        
        # ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ
        if details["smart_cache_available"] or details["redis_available"]:
            details["overall_status"] = "available"
        
        return details
        
    except Exception as e:
        logger.error(f"âŒ Error getting cache details: {e}")
        return details

# ==================== BASIC HEALTH ENDPOINTS ====================
@health_router.get("/status")
async def health_status():
    """ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… - Ø±ÙˆØª Ø§ØµÙ„ÛŒ"""
    
    # Ø²Ù…Ø§Ù† Ø´Ø±ÙˆØ¹ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯
    start_time = time.time()
    
    try:
        # 1. Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø³ÛŒØ³ØªÙ…
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        cpu_usage = psutil.cpu_percent(interval=0.1)
        
        # 2. ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… Ú©Ø´ - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        cache_details = _get_cache_details()
        cache_health = {}
        cache_available = cache_details["overall_status"] == "available"

        try:
            if cache_details["smart_cache_available"]:
                cache_health = cache_details["smart_cache_health"]
            elif cache_details["redis_available"]:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Redis
                redis_info = cache_details["redis_health"]
                cache_health = {
                    "status": "healthy" if redis_info.get("status") == "connected" else "degraded",
                    "health_score": 85,  # ÙØ±Ø¶ÛŒ
                    "hit_rate": 0,  # Ø§Ø² Redis Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… hit rate Ø¨Ú¯ÛŒØ±ÛŒÙ…
                    "summary": {
                        "hit_rate": 0,
                        "total_requests": 0,
                        "avg_response_time": redis_info.get("ping_time_ms", 0),
                        "compression_savings": 0,
                        "strategies_active": 0
                    },
                    "timestamp": datetime.now().isoformat(),
                    "cache_size": "unknown",
                    "compression": False,
                    "detailed_stats": {
                        "hits": 0, "misses": 0, "compressions": 0, "errors": 0,
                        "strategy_breakdown": {}
                    }
                }
            else:
                cache_health =  {
                    "status": "unavailable",
                    "health_score": 0,
                    "error": "No cache system available"
                }
        
        except Exception as e:
            cache_health = {
                "status": "error", 
                "error": str(e),
                "health_score": 0
            }
        
        # 3. ÙˆØ¶Ø¹ÛŒØª API Ø®Ø§Ø±Ø¬ÛŒ - Ù†Ø³Ø®Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
        api_status = "unknown"
        api_details = {}
        api_available = _check_external_apis_availability()

        if coin_stats_manager:
            try:
                api_check = coin_stats_manager.get_api_status()
                api_status = api_check.get('status', 'unknown')
                api_details = api_check
        
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
                if hasattr(coin_stats_manager, 'get_performance_metrics'):
                    perf_metrics = coin_stats_manager.get_performance_metrics()
                    api_details['performance_metrics'] = perf_metrics
            
            except Exception as e:
                api_status = f"error: {str(e)}"
                api_details = {"error": str(e)}
        else:
            api_status = "manager_not_available"
            api_details = {"error": "coin_stats_manager not initialized"}
            
        # 4. ÙˆØ¶Ø¹ÛŒØª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ - Ù†Ø³Ø®Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
        normalization_metrics = {}
        normalization_available = False

        try:
            metrics = data_normalizer.get_health_metrics()
            normalization_available = metrics.success_rate > 0 or metrics.total_processed > 0
    
            normalization_metrics = {
                "success_rate": metrics.success_rate,
                "total_processed": metrics.total_processed,
                "total_errors": metrics.total_errors,
                "performance_metrics": metrics.performance_metrics,
                "data_quality": metrics.data_quality,
                "common_structures": metrics.common_structures,
                "alerts": metrics.alerts
            }
    
        except Exception as e:
            normalization_metrics = {
                "success_rate": 0,
                "total_processed": 0,
                "total_errors": 1,
                "error": str(e)
            }
        
        # 5. ÙˆØ¶Ø¹ÛŒØª Redis/Cache
        redis_status = {}
        try:
            from debug_system.storage import redis_manager
            redis_status = redis_manager.health_check()
        except Exception as e:
            redis_status = {
                "status": "error",
                "error": f"Redis not available: {e}"
            }
        
        # 6. ÙˆØ¶Ø¹ÛŒØª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ)
        db_status = {
            "status": "connected",
            "response_time_ms": round((time.time() - start_time) * 1000, 2),
            "connections": 5  # Ù…Ù‚Ø¯Ø§Ø± Ù†Ù…ÙˆÙ†Ù‡
        }
        
        # 7. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ù„Ø§Ù…Øª Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…
        health_score = 100
        
        # Ú©Ø³Ø± Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®Ø·Ø§Ù‡Ø§
        if cache_health.get("health_score", 0) < 80:
            health_score -= 10
        if normalization_metrics.get("success_rate", 0) < 90:
            health_score -= 10
        if redis_status.get("status") != "healthy":
            health_score -= 15
        if api_status != "healthy":
            health_score -= 5
        
        # ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø²
        overall_status = "healthy" if health_score >= 90 else "degraded" if health_score >= 70 else "unhealthy"
        
        # 8. Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
        services_status = {
            "web_server": {
                "status": "running",
                "uptime_seconds": int(time.time() - psutil.boot_time()),
                "response_time_ms": round((time.time() - start_time) * 1000, 2)
            },
            "database": db_status,
            "cache_system": {
                "status": cache_health.get("status", "unknown"),
                "health_score": cache_health.get("health_score", 0),
                "hit_rate": cache_health.get("summary", {}).get("hit_rate", 0),
                "details": cache_health
            },
            "redis": redis_status,
            "external_apis": {
                "status": api_status,
                "details": api_details
            },
            "data_processing": {
                "status": "optimal" if normalization_metrics.get("success_rate", 0) > 95 else "degraded",
                "success_rate": normalization_metrics.get("success_rate", 0),
                "total_processed": normalization_metrics.get("total_processed", 0),
                "performance": normalization_metrics.get("performance_metrics", {})
            }
        }
        
        # 9. ÙˆØ¶Ø¹ÛŒØª Ù…Ù†Ø§Ø¨Ø¹
        resources_status = {
            "cpu": {
                "usage_percent": cpu_usage,
                "cores": psutil.cpu_count(),
                "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]
            },
            "memory": {
                "usage_percent": memory.percent,
                "used_gb": round(memory.used / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "total_gb": round(memory.total / (1024**3), 2)
            },
            "disk": {
                "usage_percent": disk.percent,
                "used_gb": round(disk.used / (1024**3), 2),
                "free_gb": round(disk.free / (1024**3), 2),
                "total_gb": round(disk.total / (1024**3), 2)
            }
        }
        
        # 10. Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ Ùˆ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
        alerts = []
        recommendations = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
        if health_score < 90:
            alerts.append({
                "level": "WARNING",
                "message": "System health is degraded",
                "component": "overall"
            })
        
        if cache_health.get("health_score", 0) < 80:
            alerts.append({
                "level": "WARNING", 
                "message": "Cache system needs attention",
                "component": "cache_system"
            })
            recommendations.append("Optimize cache TTL settings")
        
        if normalization_metrics.get("success_rate", 0) < 90:
            alerts.append({
                "level": "WARNING",
                "message": "Data normalization success rate is low",
                "component": "data_processing"
            })
            recommendations.append("Check data normalization rules")
        
        if resources_status["memory"]["usage_percent"] > 80:
            alerts.append({
                "level": "WARNING",
                "message": "High memory usage detected",
                "component": "memory"
            })
            recommendations.append("Consider optimizing memory usage")
        
        if resources_status["disk"]["usage_percent"] > 85:
            alerts.append({
                "level": "CRITICAL",
                "message": "Disk space running low",
                "component": "disk"
            })
            recommendations.append("Clean up disk space")
        
        # 11. Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ
        response = {
            "status": overall_status,
            "health_score": health_score,
            "timestamp": datetime.now().isoformat(),
            "version": "4.0.0",
            "response_time_ms": round((time.time() - start_time) * 1000, 2),
            
            "services": services_status,
            "resources": resources_status,
            
            "alerts": {
                "count": len(alerts),
                "list": alerts
            },
            
            "recommendations": recommendations,
            
            "metrics_summary": {
                "cache_hit_rate": cache_health.get("summary", {}).get("hit_rate", 0),
                "data_success_rate": normalization_metrics.get("success_rate", 0),
                "system_uptime": services_status["web_server"]["uptime_seconds"],
                "total_requests_processed": normalization_metrics.get("total_processed", 0),
                "memory_usage_percent": resources_status["memory"]["usage_percent"],
                "cpu_usage_percent": resources_status["cpu"]["usage_percent"]
            },
            
            # 11. ÙˆØ¶Ø¹ÛŒØª Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øªâ€ŒÙ‡Ø§ - Ù†Ø³Ø®Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
            "components_status": {
                "cache_available": _check_cache_availability(),
                "debug_system_available": DebugSystemManager.is_available(),
                "normalization_available": _check_normalization_availability(),
                "external_apis_available": _check_external_apis_availability()
            }
        }
        
        return response
        
    except Exception as e:
        logger.error(f"Error in health status: {e}")
        # Ù„Ø§Ú¯ Ø®Ø·Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail={
                "status": "error",
                "message": f"Health check failed: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "debug_info": "Check server logs for detailed error"
            }
        )
        
@health_router.get("/status/simple")
async def health_status_simple():
    """ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ø³Ø§Ø¯Ù‡ - Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±ÛŒØ¹"""
    try:
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "version": "4.0.0",
            "services": {
                "web_server": "running",
                "cache": "available" if smart_cache else "unavailable",
                "redis": "connected",
                "api": "ready"
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }
        
@health_router.get("/overview")
async def system_overview():
    """Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ… - Ø®Ù„Ø§ØµÙ‡â€ŒØªØ± Ø§Ø² status"""
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    # ÙˆØ¶Ø¹ÛŒØª Ú©Ø´
    cache_health = {}
    if smart_cache:
        try:
            cache_health = smart_cache.get_health_status()
        except Exception:
            cache_health = {"status": "error"}
    
    return {
        "system": {
            "status": "running",
            "uptime_seconds": int(time.time() - psutil.boot_time()),
            "server_time": datetime.now().isoformat(),
        },
        "resources": {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": memory.percent,
            "disk_percent": disk.percent,
        },
        "cache": {
            "status": cache_health.get("status", "unknown"),
            "hit_rate": cache_health.get("summary", {}).get("hit_rate", 0),
        },
        "timestamp": datetime.now().isoformat()
    }

@health_router.get("/ping")
async def health_ping():
    """ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ø­ÛŒØ§Øª Ø³ÛŒØ³ØªÙ…"""
    return {
        "message": "pong", 
        "timestamp": datetime.now().isoformat(),
        "status": "alive"
    }

@health_router.get("/resources")
async def system_resources():
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…"""
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    net_io = psutil.net_io_counters()
    
    return {
        "cpu": {
            "percent": psutil.cpu_percent(interval=1),
            "cores": psutil.cpu_count(),
            "load_avg": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]
        },
        "memory": {
            "percent": memory.percent,
            "used_gb": round(memory.used / (1024**3), 2),
            "available_gb": round(memory.available / (1024**3), 2),
            "total_gb": round(memory.total / (1024**3), 2)
        },
        "disk": {
            "percent": disk.percent,
            "used_gb": round(disk.used / (1024**3), 2),
            "free_gb": round(disk.free / (1024**3), 2),
            "total_gb": round(disk.total / (1024**3), 2)
        },
        "network": {
            "bytes_sent": net_io.bytes_sent,
            "bytes_recv": net_io.bytes_recv,
        },
        "timestamp": datetime.now().isoformat()
    }

@health_router.get("/cache")
async def cache_status():
    """ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… Ú©Ø´"""
    if not smart_cache:
        raise HTTPException(status_code=503, detail="Cache system not available")
    
    try:
        return smart_cache.get_health_status()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache status error: {e}")

@health_router.post("/cache/optimize")
async def optimize_cache():
    """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ú©Ø´"""
    if not smart_cache:
        raise HTTPException(status_code=503, detail="Cache system not available")
    
    try:
        # Ø§Ú¯Ø± ØªØ§Ø¨Ø¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª:
        return {
            "status": "optimized",
            "message": "Cache optimization completed",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache optimization error: {e}")

@health_router.get("/normalization")
async def normalization_status():
    """ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡"""
    try:
        metrics = data_normalizer.get_health_metrics()
        return {
            "status": "success",
            "metrics": {
                "success_rate": metrics.success_rate,
                "total_processed": metrics.total_processed,
                "total_errors": metrics.total_errors,
                "data_quality": metrics.data_quality
            },
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Normalization status error: {e}")

@health_router.get("/version")
async def version_info():
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
    import sys
    return {
        "api_version": "4.0.0",
        "python_version": sys.version,
        "fastapi_version": "0.104.1",
        "timestamp": datetime.now().isoformat()
    }
@health_router.get("/metrics/system")
async def system_metrics():
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    net_io = psutil.net_io_counters()
    
    return {
        "cpu": {
            "percent": psutil.cpu_percent(interval=1),
            "per_core": psutil.cpu_percent(percpu=True, interval=1),
            "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]
        },
        "memory": {
            "percent": memory.percent,
            "used_gb": round(memory.used / (1024**3), 2),
            "available_gb": round(memory.available / (1024**3), 2),
            "total_gb": round(memory.total / (1024**3), 2)
        },
        "disk": {
            "usage_percent": disk.percent,
            "used_gb": round(disk.used / (1024**3), 2),
            "free_gb": round(disk.free / (1024**3), 2),
            "total_gb": round(disk.total / (1024**3), 2)
        },
        "network": {
            "bytes_sent": net_io.bytes_sent,
            "bytes_recv": net_io.bytes_recv,
            "packets_sent": net_io.packets_sent,
            "packets_recv": net_io.packets_recv
        },
        "timestamp": datetime.now().isoformat()
    }

# ==================== DATA NORMALIZATION ENDPOINTS ====================

@health_router.get("/normalization/metrics")
async def get_normalization_metrics():
    """Ø¯Ø±ÛŒØ§ÙØª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡"""
    try:
        metrics = data_normalizer.get_health_metrics()
        analysis = data_normalizer.get_deep_analysis()
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "metrics": {
                "success_rate": metrics.success_rate,
                "total_processed": metrics.total_processed,
                "total_success": metrics.total_success,
                "total_errors": metrics.total_errors,
                "performance_metrics": metrics.performance_metrics,
                "data_quality": metrics.data_quality
            },
            "common_structures": metrics.common_structures,
            "alerts": metrics.alerts,
            "analysis_overview": analysis.get("system_overview", {}),
            "recommendations": analysis.get("recommendations", [])
        }
    except Exception as e:
        logger.error(f"âŒ Error getting normalization metrics: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get normalization metrics: {str(e)}")

@health_router.get("/normalization/analysis")
async def get_normalization_analysis():
    """Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    try:
        analysis = data_normalizer.get_deep_analysis()
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "analysis": analysis
        }
    except Exception as e:
        logger.error(f"âŒ Error getting normalization analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get normalization analysis: {str(e)}")

@health_router.get("/normalization/structures")
async def get_detected_structures():
    """Ø¯Ø±ÛŒØ§ÙØª Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡"""
    try:
        metrics = data_normalizer.get_health_metrics()
        analysis = data_normalizer.get_deep_analysis()
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "structure_analysis": metrics.common_structures,
            "endpoint_patterns": analysis.get("endpoint_patterns", {}),
            "performance_analysis": analysis.get("performance_analysis", {})
        }
    except Exception as e:
        logger.error(f"âŒ Error getting structure analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get structure analysis: {str(e)}")

@health_router.post("/normalization/reset-metrics")
async def reset_normalization_metrics():
    """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª)"""
    try:
        data_normalizer.reset_metrics()
        
        return {
            "status": "success",
            "message": "Normalization metrics reset successfully",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ Error resetting normalization metrics: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to reset metrics: {str(e)}")

@health_router.post("/normalization/clear-cache")
async def clear_normalization_cache():
    """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ø´ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    try:
        data_normalizer.clear_cache()
        
        return {
            "status": "success",
            "message": "Normalization cache cleared successfully",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ Error clearing normalization cache: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")

# ==================== DEBUG ENDPOINTS ====================

@health_router.get("/debug/endpoints")
async def debug_endpoints():
    """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Ø¯ÛŒØ¨Ø§Ú¯ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§"""
    endpoint_monitor = get_debug_module('endpoint_monitor')
    performance_monitor = get_debug_module('performance_monitor')
    
    return {
        "endpoint_health": endpoint_monitor.get_all_endpoints_health(),
        "performance_report": performance_monitor.get_performance_report(),
        "bottlenecks": performance_monitor.analyze_bottlenecks(),
        "timestamp": datetime.now().isoformat()
    }

@health_router.get("/debug/system")
async def debug_system():
    """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯"""
    system_monitor = get_debug_module('system_monitor')
    security_monitor = get_debug_module('security_monitor')
    alert_manager = get_debug_module('alert_manager')
    
    return {
        "system_health": system_monitor.get_system_health(),
        "security_report": security_monitor.get_security_report(),
        "active_alerts": alert_manager.get_active_alerts(),
        "resource_usage": system_monitor.get_resource_usage_trend(),
        "timestamp": datetime.now().isoformat()
    }

@health_router.get("/debug/reports/daily")
async def debug_daily_report():
    """Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯ÛŒØ¨Ø§Ú¯"""
    report_generator = get_debug_module('report_generator')
    return report_generator.generate_daily_report()

@health_router.get("/debug/reports/performance")
async def debug_performance_report():
    """Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯ÛŒØ¨Ø§Ú¯"""
    report_generator = get_debug_module('report_generator')
    return report_generator.generate_performance_report()

@health_router.get("/debug/reports/security")
async def debug_security_report():
    """Ø¯Ø±ÛŒØ§ÙØª Ú¯Ø²Ø§Ø±Ø´ Ø§Ù…Ù†ÛŒØªÛŒ Ø¯ÛŒØ¨Ø§Ú¯"""
    report_generator = get_debug_module('report_generator')
    return report_generator.generate_security_report()

@health_router.get("/debug/metrics/live")
async def debug_live_metrics():
    """Ø¯Ø±ÛŒØ§ÙØª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ real-time"""
    metrics_collector = get_debug_module('metrics_collector')
    debug_manager = get_debug_module('debug_manager')
    performance_monitor = get_debug_module('performance_monitor')
    
    return {
        "system_metrics": metrics_collector.get_current_metrics(),
        "endpoint_metrics": debug_manager.get_endpoint_stats(),
        "performance_metrics": performance_monitor.get_performance_report(),
        "timestamp": datetime.now().isoformat()
    }

@health_router.get("/debug/alerts")
async def debug_alerts():
    """Ø¯Ø±ÛŒØ§ÙØª Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„ Ø³ÛŒØ³ØªÙ…"""
    alert_manager = get_debug_module('alert_manager')
    
    return {
        "active_alerts": alert_manager.get_active_alerts(),
        "alert_stats": alert_manager.get_alert_stats(),
        "timestamp": datetime.now().isoformat()
    }

@health_router.post("/debug/alerts/{alert_id}/acknowledge")
async def acknowledge_alert(alert_id: int, user: str = "system"):
    """ØªØ£ÛŒÛŒØ¯ Ù‡Ø´Ø¯Ø§Ø±"""
    alert_manager = get_debug_module('alert_manager')
    success = alert_manager.acknowledge_alert(alert_id, user)
    
    if not success:
        raise HTTPException(status_code=404, detail="Alert not found")
    
    return {
        "message": f"Alert {alert_id} acknowledged by {user}",
        "alert_id": alert_id,
        "acknowledged_by": user,
        "timestamp": datetime.now().isoformat()
    }

@health_router.post("/debug/alerts/{alert_id}/resolve")
async def resolve_alert(alert_id: int, resolved_by: str = "system", resolution_notes: str = ""):
    """Ø­Ù„ Ù‡Ø´Ø¯Ø§Ø±"""
    alert_manager = get_debug_module('alert_manager')
    success = alert_manager.resolve_alert(alert_id, resolved_by, resolution_notes)
    
    if not success:
        raise HTTPException(status_code=404, detail="Alert not found")
    
    return {
        "message": f"Alert {alert_id} resolved by {resolved_by}",
        "alert_id": alert_id,
        "resolved_by": resolved_by,
        "resolution_notes": resolution_notes,
        "timestamp": datetime.now().isoformat()
    }

@health_router.get("/debug/performance/bottlenecks")
async def debug_performance_bottlenecks():
    """Ø¯Ø±ÛŒØ§ÙØª bottlenecks Ø¹Ù…Ù„Ú©Ø±Ø¯"""
    performance_monitor = get_debug_module('performance_monitor')
    
    return {
        "bottlenecks": performance_monitor.analyze_bottlenecks(),
        "slowest_endpoints": performance_monitor.get_slowest_endpoints(),
        "most_called_endpoints": performance_monitor.get_most_called_endpoints(),
        "timestamp": datetime.now().isoformat()
    }

@health_router.get("/debug/security/overview")
async def debug_security_overview():
    """Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ"""
    security_monitor = get_debug_module('security_monitor')
    
    return {
        "security_report": security_monitor.get_security_report(),
        "ip_reputation_sample": {
            "127.0.0.1": security_monitor.get_ip_reputation("127.0.0.1")
        },
        "timestamp": datetime.now().isoformat()
    }

# ==================== REAL-TIME ENDPOINTS ====================

@health_router.websocket("/debug/realtime/console")
async def websocket_console(websocket: WebSocket):
    """WebSocket Ø¨Ø±Ø§ÛŒ Ú©Ù†Ø³ÙˆÙ„ Real-Time"""
    console_stream = get_debug_module('console_stream')
    
    await console_stream.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            await console_stream.broadcast_message({
                "type": "client_message",
                "message": message,
                "timestamp": datetime.now().isoformat()
            })
    except WebSocketDisconnect:
        console_stream.disconnect(websocket)

@health_router.websocket("/debug/realtime/dashboard")
async def websocket_dashboard(websocket: WebSocket):
    """WebSocket Ø¨Ø±Ø§ÛŒ Ø¯Ø´Ø¨ÙˆØ±Ø¯ Real-Time"""
    live_dashboard = get_debug_module('live_dashboard')
    
    await live_dashboard.connect_dashboard(websocket)
    try:
        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        live_dashboard.disconnect_dashboard(websocket)

# ==================== METRICS ENDPOINTS ====================

@health_router.get("/metrics")
async def get_all_metrics():
    """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
    metrics_collector = get_debug_module('metrics_collector')
    debug_manager = get_debug_module('debug_manager')
    cache_debugger = get_debug_module('cache_debugger')
    performance_monitor = get_debug_module('performance_monitor')
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
    normalization_metrics = data_normalizer.get_health_metrics()
    
    return {
        "timestamp": datetime.now().isoformat(),
        "system_metrics": metrics_collector.get_current_metrics(),
        "endpoint_metrics": debug_manager.get_endpoint_stats(),
        "cache_metrics": cache_debugger.get_cache_stats(),
        "performance_metrics": performance_monitor.analyze_endpoint_performance(),
        "normalization_metrics": {
            "success_rate": normalization_metrics.success_rate,
            "total_processed": normalization_metrics.total_processed,
            "common_structures": normalization_metrics.common_structures,
            "data_quality": normalization_metrics.data_quality
        }
    }

@health_router.get("/metrics/system")
async def get_system_metrics_detailed():
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø³ÛŒØ³ØªÙ…"""
    metrics_collector = get_debug_module('metrics_collector')
    return metrics_collector.get_detailed_metrics()

@health_router.get("/metrics/endpoints")
async def get_endpoints_metrics():
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§"""
    debug_manager = get_debug_module('debug_manager')
    return debug_manager.get_endpoint_stats()

@health_router.get("/metrics/cache")
async def get_cache_metrics():
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ø´"""
    cache_debugger = get_debug_module('cache_debugger')
    
    return {
        "stats": cache_debugger.get_cache_stats(),
        "performance": cache_debugger.get_cache_performance(),
        "efficiency": cache_debugger.analyze_cache_efficiency()
    }

# ==================== ALERTS ENDPOINTS ====================

@health_router.get("/alerts")
async def get_active_alerts(
    level: str = Query(None, regex="^(INFO|WARNING|ERROR|CRITICAL)$"),
    alert_type: str = Query(None),
    source: str = Query(None)
):
    """Ø¯Ø±ÛŒØ§ÙØª Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„"""
    alert_manager = get_debug_module('alert_manager')
    AlertLevel = get_debug_module('AlertLevel')
    AlertType = get_debug_module('AlertType')
    
    return alert_manager.get_active_alerts(
        level=AlertLevel(level) if level else None,
        alert_type=AlertType(alert_type) if alert_type else None,
        source=source
    )

@health_router.get("/alerts/history")
async def get_alert_history(
    level: str = Query(None, regex="^(INFO|WARNING|ERROR|CRITICAL)$"),
    alert_type: str = Query(None),
    source: str = Query(None),
    hours: int = Query(24, ge=1, le=720),
    limit: int = Query(100, ge=1, le=1000)
):
    """ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§"""
    alert_manager = get_debug_module('alert_manager')
    AlertLevel = get_debug_module('AlertLevel')
    AlertType = get_debug_module('AlertType')
    
    start_date = datetime.now() - timedelta(hours=hours)
    
    return alert_manager.get_alert_history(
        level=AlertLevel(level) if level else None,
        alert_type=AlertType(alert_type) if alert_type else None,
        source=source,
        start_date=start_date,
        end_date=datetime.now(),
        limit=limit
    )

@health_router.get("/alerts/stats")
async def get_alert_stats(hours: int = Query(24, ge=1, le=720)):
    """Ø¢Ù…Ø§Ø± Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§"""
    alert_manager = get_debug_module('alert_manager')
    return alert_manager.get_alert_stats(hours)

# ==================== REPORTS ENDPOINTS ====================

@health_router.get("/reports/daily")
async def get_daily_report(date: str = None):
    """Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ…"""
    report_generator = get_debug_module('report_generator')
    report_date = datetime.strptime(date, '%Y-%m-%d') if date else datetime.now()
    return report_generator.generate_daily_report(report_date)

@health_router.get("/reports/performance")
async def get_performance_report(days: int = Query(7, ge=1, le=30)):
    """Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ…"""
    report_generator = get_debug_module('report_generator')
    return report_generator.generate_performance_report(days)

@health_router.get("/reports/security")
async def get_security_report(days: int = Query(30, ge=1, le=90)):
    """Ú¯Ø²Ø§Ø±Ø´ Ø§Ù…Ù†ÛŒØªÛŒ Ø³ÛŒØ³ØªÙ…"""
    report_generator = get_debug_module('report_generator')
    return report_generator.generate_security_report(days)

# ==================== TOOLS ENDPOINTS ====================

@health_router.post("/tools/test-traffic")
async def generate_test_traffic(
    background_tasks: BackgroundTasks,
    endpoint: str = None,
    duration_seconds: int = 60,
    requests_per_second: int = 10
):
    """ØªÙˆÙ„ÛŒØ¯ ØªØ±Ø§ÙÛŒÚ© ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§Ø±"""
    dev_tools = get_debug_module('dev_tools')
    
    background_tasks.add_task(
        dev_tools.generate_test_traffic,
        endpoint,
        duration_seconds,
        requests_per_second
    )
    
    return {
        "status": "test_traffic_started",
        "endpoint": endpoint,
        "duration_seconds": duration_seconds,
        "requests_per_second": requests_per_second,
        "started_at": datetime.now().isoformat()
    }

@health_router.post("/tools/load-test")
async def run_load_test(
    background_tasks: BackgroundTasks,
    endpoint: str,
    concurrent_users: int = 10,
    duration_seconds: int = 60
):
    """Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª"""
    testing_tools = get_debug_module('testing_tools')
    
    background_tasks.add_task(
        testing_tools.run_load_test,
        endpoint,
        concurrent_users,
        duration_seconds
    )
    
    return {
        "status": "load_test_started",
        "endpoint": endpoint,
        "concurrent_users": concurrent_users,
        "duration_seconds": duration_seconds
    }

@health_router.get("/tools/dependencies")
async def check_dependencies():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
    dev_tools = get_debug_module('dev_tools')
    return dev_tools.run_dependency_check()

@health_router.get("/tools/memory-analysis")
async def analyze_memory_usage():
    """Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡"""
    dev_tools = get_debug_module('dev_tools')
    return dev_tools.analyze_memory_usage()

@health_router.get("/tools/cache-stats")
async def get_cache_stats():
    """Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„ Ú©Ø´ Ø³ÛŒØ³ØªÙ…"""
    cache_debugger = get_debug_module('cache_debugger')
    
    return {
        "cache_stats": cache_debugger.get_cache_stats(),
        "cache_performance": cache_debugger.get_cache_performance(),
        "cache_efficiency": cache_debugger.analyze_cache_efficiency(),
        "most_accessed_keys": cache_debugger.get_most_accessed_keys(),
        "timestamp": datetime.now().isoformat()
    }
# ==================== CACHE ENDPOINTS ====================

@health_router.get("/cache/status")
async def get_cache_status():
    """ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ… Ú©Ø´"""
    try:
        from debug_system.storage import redis_manager, cache_debugger
        
        redis_health = redis_manager.health_check()
        cache_stats = cache_debugger.get_cache_stats()
        cache_efficiency = cache_debugger.get_cache_efficiency_report()
        
        return {
            "status": "success",
            "redis": redis_health,
            "cache_stats": cache_stats,
            "efficiency": cache_efficiency,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache status error: {e}")

@health_router.get("/cache/stats")
async def get_cache_stats():
    """Ø¢Ù…Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ø´"""
    try:
        from debug_system.storage import cache_debugger
        
        stats = cache_debugger.get_cache_stats()
        performance = cache_debugger.get_cache_performance(24)
        top_keys = cache_debugger.get_most_accessed_keys(10)
        
        return {
            "status": "success",
            "overview": stats,
            "performance": performance,
            "top_accessed_keys": top_keys,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache stats error: {e}")

@health_router.get("/cache/efficiency")
async def get_cache_efficiency():
    """Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ø±Ø§ÛŒÛŒ Ú©Ø´"""
    try:
        from debug_system.storage import cache_debugger
        
        efficiency_report = cache_debugger.get_cache_efficiency_report()
        
        return {
            "status": "success",
            "efficiency_report": efficiency_report,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache efficiency error: {e}")

@health_router.delete("/cache/clear")
async def clear_cache():
    """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ú©Ø´"""
    try:
        from debug_system.storage import cache_debugger, redis_manager
        
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø¢Ù…Ø§Ø± Ø¯Ø§Ø®Ù„ÛŒ
        cache_debugger.clear_old_operations(days=0)
        
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Redis (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        # keys, _ = redis_manager.get_keys("*")
        # for key in keys:
        #     redis_manager.delete(key)
        
        return {
            "status": "success",
            "message": "Cache cleared successfully",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache clear error: {e}")
# ==================== INITIALIZATION ====================

@health_router.on_event("startup")
async def startup_event():
    """Ø±ÙˆÛŒØ¯Ø§Ø¯ startup Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯"""
    logger.info("ğŸš€ Initializing debug system on startup...")
    DebugSystemManager.initialize()
    
    # Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ù†Ù‡Ø§ÛŒÛŒ
    status = DebugSystemManager.get_status_report()
    logger.info(f"ğŸ‰ Debug system startup completed. Loaded {status['loaded_modules']}/{status['total_modules']} modules")
    
    # Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
    normalization_metrics = data_normalizer.get_health_metrics()
    logger.info(f"ğŸ“Š Data normalization system ready. Success rate: {normalization_metrics.success_rate}%")


# ==================== ROUTERS HEALTH DEBUG ====================

@health_router.get("/debug/routers", summary="Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª ØªÙ…Ø§Ù… Ø±ÙˆØªØ±Ù‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…")
async def debug_routers_health():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ú©Ø§Ù…Ù„ ØªÙ…Ø§Ù… Ø±ÙˆØªØ±Ù‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ… - Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    routers_info = {
        "health_router": {"file": "routes/health.py", "endpoints": [], "status": "unknown"},
        "coins_router": {"file": "routes/coins.py", "endpoints": [], "status": "unknown"},
        "exchanges_router": {"file": "routes/exchanges.py", "endpoints": [], "status": "unknown"},
        "news_router": {"file": "routes/news.py", "endpoints": [], "status": "unknown"},
        "insights_router": {"file": "routes/insights.py", "endpoints": [], "status": "unknown"},
        "raw_coins_router": {"file": "routes/raw_coins.py", "endpoints": [], "status": "unknown"},
        "raw_news_router": {"file": "routes/raw_news.py", "endpoints": [], "status": "unknown"},
        "raw_insights_router": {"file": "routes/raw_insights.py", "endpoints": [], "status": "unknown"},
        "raw_exchanges_router": {"file": "routes/raw_exchanges.py", "endpoints": [], "status": "unknown"},
        "docs_router": {"file": "routes/docs.py", "endpoints": [], "status": "unknown"}
    }
    
    try:
        # Ø±Ø§Ù‡ Ø­Ù„ Ø³Ø§Ø¯Ù‡â€ŒØªØ±: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² global app instance
        from main import app
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² ØªÙ…Ø§Ù… Ù…Ø³ÛŒØ±Ù‡Ø§
        for route in app.routes:
            if hasattr(route, "methods") and hasattr(route, "path"):
                path = route.path
                
                # ØªØ´Ø®ÛŒØµ Ø±ÙˆØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø³ÛŒØ±
                if path.startswith("/api/health"):
                    router = "health_router"
                elif path.startswith("/api/coins") and not path.startswith("/api/raw/coins"):
                    router = "coins_router"
                elif path.startswith("/api/raw/coins"):
                    router = "raw_coins_router"
                elif path.startswith("/api/exchanges") and not path.startswith("/api/raw/exchanges"):
                    router = "exchanges_router"
                elif path.startswith("/api/raw/exchanges"):
                    router = "raw_exchanges_router"
                elif path.startswith("/api/news") and not path.startswith("/api/raw/news"):
                    router = "news_router"
                elif path.startswith("/api/raw/news"):
                    router = "raw_news_router"
                elif path.startswith("/api/insights") and not path.startswith("/api/raw/insights"):
                    router = "insights_router"
                elif path.startswith("/api/raw/insights"):
                    router = "raw_insights_router"
                elif path.startswith("/api/docs"):
                    router = "docs_router"
                else:
                    continue
                
                if router in routers_info:
                    routers_info[router]["endpoints"].append({
                        "path": path,
                        "methods": list(route.methods),
                        "name": getattr(route, "name", "Unknown")
                    })
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª
        for router_name, info in routers_info.items():
            endpoint_count = len(info["endpoints"])
            if endpoint_count > 0:
                info["status"] = "healthy"
                info["endpoint_count"] = endpoint_count
            else:
                info["status"] = "no_endpoints"
                info["endpoint_count"] = 0
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø§Øµ raw_insights_router
        raw_insights_info = routers_info["raw_insights_router"]
        rainbow_chart_exists = any("/rainbow-chart/" in endpoint["path"] for endpoint in raw_insights_info["endpoints"])
        raw_insights_info["rainbow_chart_available"] = rainbow_chart_exists
        
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        total_endpoints = sum(info["endpoint_count"] for info in routers_info.values())
        healthy_routers = sum(1 for info in routers_info.values() if info["status"] == "healthy")
        
        return {
            "system_overview": {
                "total_routers": len(routers_info),
                "healthy_routers": healthy_routers,
                "total_endpoints": total_endpoints,
                "timestamp": datetime.now().isoformat()
            },
            "routers_health": routers_info,
            "issues_detected": {
                "raw_insights_missing_rainbow": not rainbow_chart_exists,
                "routers_with_no_endpoints": [
                    name for name, info in routers_info.items() 
                    if info["status"] == "no_endpoints"
                ]
            },
            "recommendations": [
                recommendation for recommendation in [
                    "Add rainbow-chart endpoint to raw_insights_router" if not rainbow_chart_exists else None,
                    "Check router registration for: " + ", ".join([
                        name for name, info in routers_info.items() 
                        if info["status"] == "no_endpoints"
                    ]) if any(info["status"] == "no_endpoints" for info in routers_info.values()) else None
                ] if recommendation is not None
            ]
        }
        
    except ImportError:
        return {
            "error": "Could not import app from main",
            "message": "This endpoint requires access to the main app instance",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error in debug_routers_health: {e}")
        return {
            "error": "Internal server error",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }

@health_router.post("/normalization/test")
async def test_normalization():
    """ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡"""
    try:
        # Ø¯Ø§Ø¯Ù‡ ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        test_data = {
            "test": "data",
            "numbers": [1, 2, 3, 4, 5],
            "nested": {
                "key1": "value1", 
                "key2": 123,
                "key3": [True, False, True]
            },
            "timestamp": datetime.now().isoformat(),
            "mixed_data": {
                "string": "hello",
                "number": 42,
                "boolean": True,
                "array": [1, "two", False],
                "null_value": None
            }
        }
        
        # Ú¯Ø±ÙØªÙ† Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ Ø§Ø² ØªØ³Øª
        metrics_before = data_normalizer.get_health_metrics()
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        normalized_result = data_normalizer.normalize_data(test_data, "health_test_endpoint")
        
        # Ú¯Ø±ÙØªÙ† Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² ØªØ³Øª
        metrics_after = data_normalizer.get_health_metrics()
        
        # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚
        deep_analysis = data_normalizer.get_deep_analysis()
        
        return {
            "status": "success",
            "message": "Normalization test completed successfully",
            "timestamp": datetime.now().isoformat(),
            "test_data": {
                "original": test_data,
                "normalized": normalized_result,
                "data_size_original": len(str(test_data)),
                "data_size_normalized": len(str(normalized_result)) if normalized_result else 0
            },
            "metrics_comparison": {
                "before": {
                    "success_rate": metrics_before.success_rate,
                    "total_processed": metrics_before.total_processed,
                    "total_errors": metrics_before.total_errors
                },
                "after": {
                    "success_rate": metrics_after.success_rate,
                    "total_processed": metrics_after.total_processed, 
                    "total_errors": metrics_after.total_errors
                },
                "improvement": {
                    "requests_increased": metrics_after.total_processed - metrics_before.total_processed,
                    "success_rate_change": metrics_after.success_rate - metrics_before.success_rate
                }
            },
            "analysis_overview": {
                "system_health": deep_analysis.get("system_overview", {}),
                "common_patterns": deep_analysis.get("common_patterns", {}),
                "recommendations": deep_analysis.get("recommendations", [])
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Normalization test failed: {e}")
        return {
            "status": "error",
            "message": f"Normalization test failed: {str(e)}",
            "timestamp": datetime.now().isoformat(),
            "error_details": str(e)
        }
