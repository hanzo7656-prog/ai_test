from fastapi import APIRouter, HTTPException, BackgroundTasks, Query, WebSocket, WebSocketDisconnect, Request
from datetime import datetime, timedelta
import asyncio
import json
import time
from typing import Dict, List, Optional, Any
import psutil
import logging
import os
import glob
import shutil
import threading

logger = logging.getLogger(__name__)

# ==================== IMPORTS ====================

# Ø³ÛŒØ³ØªÙ… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
try:
    from debug_system.utils.data_normalizer import data_normalizer
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from debug_system.utils.data_normalizer import data_normalizer

# Cache Optimization Engine
try:
    from debug_system.storage.smart_cache_system import cache_optimizer
    logger.info("âœ… Cache Optimization Engine imported")
except ImportError as e:
    logger.warning(f"âš ï¸ Cache Optimization Engine: {e}")
    cache_optimizer = None

# complete_coinstats_manager
try:
    from complete_coinstats_manager import coin_stats_manager
except ImportError:
    coin_stats_manager = None
    logger.warning("âš ï¸ coin_stats_manager not available")

# Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ø¬Ø¯ÛŒØ¯
try:
    from debug_system.storage.cache_decorators import (
        cache_coins_with_archive, cache_news_with_archive, cache_insights_with_archive, cache_exchanges_with_archive,
        cache_raw_coins_with_archive, cache_raw_news_with_archive, cache_raw_insights_with_archive, cache_raw_exchanges_with_archive,
        get_historical_data, get_archive_stats, cleanup_old_archives
    )
    NEW_CACHE_SYSTEM_AVAILABLE = True
    logger.info("âœ… New Cache System imported")
except ImportError as e:
    logger.warning(f"âš ï¸ New Cache System: {e}")
    NEW_CACHE_SYSTEM_AVAILABLE = False

try:
    from ai_brain.vortex_brain import vortex_brain, get_ai_health
    AI_SYSTEM_AVAILABLE = True
    logger.info("âœ… AI Brain system imported successfully")
except ImportError as e:
    logger.warning(f"âš ï¸ AI Brain system not available: {e}")
    AI_SYSTEM_AVAILABLE = False
    
# Ø§ÛŒØ¬Ø§Ø¯ Ø±ÙˆØªâ€ŒØ± Ø³Ù„Ø§Ù…Øª
health_router = APIRouter(prefix="/api/health", tags=["Health & Monitoring"])

# ==================== OPTIMIZED DEBUG SYSTEM MANAGER ====================

class DebugSystemManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ø§ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡ Ùˆ Ù…ØµØ±Ù CPU Ú©Ù†ØªØ±Ù„â€ŒØ´Ø¯Ù‡"""
    
    _initialized = False
    _modules = {}
    _load_stages = {
        'core': False,
        'monitors': False, 
        'storage': False,
        'realtime': False,
        'tools': False
    }
    
    @classmethod
    def initialize(cls):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯"""
        if cls._initialized:
            logger.info("ğŸ”§ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯ Ø§Ø² Ù‚Ø¨Ù„ ÙØ¹Ø§Ù„")
            return cls._modules
        
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯...")
        start_time = time.time()
        
        try:
            # Ù…Ø±Ø­Ù„Ù‡ 1: Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
            logger.info("ğŸ“¦ Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù‡Ø³ØªÙ‡ Ø§ØµÙ„ÛŒ")
            cls._load_core_modules()
            time.sleep(0.02)
            
            # Ù…Ø±Ø­Ù„Ù‡ 2: Ù…Ø§Ù†ÛŒØªÙˆØ±Ù‡Ø§
            logger.info("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 2: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±Ù‡Ø§")
            cls._load_monitors()
            time.sleep(0.02)
            
            # Ù…Ø±Ø­Ù„Ù‡ 3: Ø³ÛŒØ³ØªÙ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
            logger.info("ğŸ’¾ Ù…Ø±Ø­Ù„Ù‡ 3: ØªÙ†Ø¸ÛŒÙ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ")
            cls._load_storage()
            time.sleep(0.02)
            
            # Ù…Ø±Ø­Ù„Ù‡ 4: Ø³ÛŒØ³ØªÙ… real-time
            logger.info("âš¡ Ù…Ø±Ø­Ù„Ù‡ 4: ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ real-time")
            cls._load_realtime()
            time.sleep(0.02)
            
            # Ù…Ø±Ø­Ù„Ù‡ 5: Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§
            logger.info("ğŸ”§ Ù…Ø±Ø­Ù„Ù‡ 5: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§")
            cls._load_tools()
            
            cls._initialized = True
            total_time = time.time() - start_time
            
            logger.info(f"âœ… Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ - Ø²Ù…Ø§Ù†: {total_time:.2f}Ø«Ø§Ù†ÛŒÙ‡ - Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§: {len([name for name, module in cls._modules.items() if module is not None])}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ: {e}")
        
        return cls._modules
    
    @classmethod
    def _load_core_modules(cls):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ"""
        try:
            from debug_system.core.debug_manager import debug_manager
            from debug_system.core.metrics_collector import metrics_collector
            from debug_system.core.alert_manager import alert_manager, AlertLevel, AlertType
            
            cls._modules.update({
                'debug_manager': debug_manager,
                'metrics_collector': metrics_collector,
                'alert_manager': alert_manager,
                'AlertLevel': AlertLevel,
                'AlertType': AlertType
            })
            cls._load_stages['core'] = True
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ù‡Ø³ØªÙ‡: {e}")
    
    @classmethod
    def _load_monitors(cls):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±Ù‡Ø§"""
        try:
            from debug_system.monitors.endpoint_monitor import EndpointMonitor
            from debug_system.monitors.system_monitor import SystemMonitor
            from debug_system.monitors.performance_monitor import PerformanceMonitor
            from debug_system.monitors.security_monitor import SecurityMonitor
            
            debug_manager = cls._modules.get('debug_manager')
            metrics_collector = cls._modules.get('metrics_collector')
            alert_manager = cls._modules.get('alert_manager')
            
            if all([debug_manager, metrics_collector, alert_manager]):
                cls._modules.update({
                    'endpoint_monitor': EndpointMonitor(debug_manager),
                    'system_monitor': SystemMonitor(metrics_collector, alert_manager),
                    'performance_monitor': PerformanceMonitor(debug_manager, alert_manager),
                    'security_monitor': SecurityMonitor(alert_manager)
                })
                cls._load_stages['monitors'] = True
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±Ù‡Ø§: {e}")
    
    @classmethod
    def _load_storage(cls):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        try:
            from debug_system.storage.history_manager import history_manager
            from debug_system.storage.cache_debugger import cache_debugger
            
            cls._modules.update({
                'history_manager': history_manager,
                'cache_debugger': cache_debugger
            })
            cls._load_stages['storage'] = True
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
    
    @classmethod
    def _load_realtime(cls):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ… real-time"""
        try:
            from debug_system.realtime.live_dashboard import LiveDashboardManager
            from debug_system.realtime.console_stream import ConsoleStreamManager
            
            debug_manager = cls._modules.get('debug_manager')
            metrics_collector = cls._modules.get('metrics_collector')
            
            if debug_manager and metrics_collector:
                cls._modules.update({
                    'live_dashboard': LiveDashboardManager(debug_manager, metrics_collector),
                    'console_stream': ConsoleStreamManager()
                })
                cls._load_stages['realtime'] = True
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ real-time: {e}")
    
    @classmethod
    def _load_tools(cls):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§"""
        try:
            from debug_system.tools import initialize_tools_system
            
            debug_manager = cls._modules.get('debug_manager')
            history_manager = cls._modules.get('history_manager')
            
            if debug_manager and history_manager:
                tools_result = initialize_tools_system(
                    debug_manager_instance=debug_manager,
                    history_manager_instance=history_manager
                )
                
                cls._modules.update({
                    'report_generator': tools_result.get('report_generator'),
                    'dev_tools': tools_result.get('dev_tools'),
                    'testing_tools': tools_result.get('testing_tools')
                })
                cls._load_stages['tools'] = True
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§: {e}")
    
    @classmethod
    def get_module(cls, module_name: str, default=None):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø§Ú˜ÙˆÙ„ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
        if not cls._initialized:
            cls.initialize()
        
        module = cls._modules.get(module_name, default)
        
        if module is None and module_name in cls._modules:
            logger.debug(f"âš ï¸ Ù…Ø§Ú˜ÙˆÙ„ '{module_name}' Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª")
        
        return module
    
    @classmethod
    def is_available(cls):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯"""
        if not cls._initialized:
            cls.initialize()
        
        debug_manager = cls._modules.get('debug_manager')
        if debug_manager and hasattr(debug_manager, 'is_active'):
            return debug_manager.is_active()
        return bool(debug_manager)
    
    @classmethod
    def get_status_report(cls):
        """Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…"""
        if not cls._initialized:
            cls.initialize()
        
        loaded_modules = [name for name, module in cls._modules.items() if module is not None]
        failed_modules = [name for name, module in cls._modules.items() if module is None]
        
        return {
            'initialized': cls._initialized,
            'stages_completed': cls._load_stages,
            'loaded_modules': len(loaded_modules),
            'total_modules': len(cls._modules),
            'available_modules': loaded_modules,
            'failed_modules': failed_modules
        }

# ==================== HELPER FUNCTIONS ====================

def _check_cache_availability() -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… Ú©Ø´"""
    try:
        from debug_system.storage import redis_manager
        redis_health = redis_manager.health_check()
        
        connected_dbs = 0
        for db_name, status in redis_health.items():
            if isinstance(status, dict) and status.get('status') == 'connected':
                connected_dbs += 1
        
        return connected_dbs > 0
        
    except Exception as e:
        logger.error(f"âŒ Cache availability check failed: {e}")
        return False

def _check_normalization_availability() -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø±"""
    try:
        test_data = {"test": "data"}
        result = data_normalizer.normalize_data(test_data, "health_check")
        
        metrics = data_normalizer.get_health_metrics()
        return metrics.success_rate > 0 or metrics.total_processed > 0
        
    except Exception as e:
        logger.warning(f"âš ï¸ Normalization availability check failed: {e}")
        return False

def _check_external_apis_availability() -> Dict[str, Any]:
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ¶Ø¹ÛŒØª APIÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ"""
    try:
        if not coin_stats_manager:
            return {
                "available": False,
                "status": "manager_not_initialized",
                "details": {"error": "coin_stats_manager is None"}
            }
        
        api_status = coin_stats_manager.get_api_status()
        connection_test = coin_stats_manager.test_api_connection_quick()
        
        return {
            "available": connection_test and api_status.get('status') == 'healthy',
            "status": api_status.get('status', 'unknown'),
            "connection_test": connection_test,
            "details": api_status,
            "timestamp": datetime.now().isoformat()
        }
            
    except Exception as e:
        logger.error(f"âŒ API availability check failed: {e}")
        return {
            "available": False,
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

def _get_cache_details() -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª ÙˆØ¶Ø¹ÛŒØª Ú©Ø´"""
    details = {
        "smart_cache_available": False,
        "cache_optimizer_available": False,
        "new_cache_system_available": NEW_CACHE_SYSTEM_AVAILABLE,
        "redis_available": False,
        "cache_debugger_available": False,
        "connected_databases": 0,
        "database_details": {},
        "overall_status": "unavailable",
        "real_metrics": {}
    }
    
    try:
        from debug_system.storage import redis_manager
        redis_health = redis_manager.health_check()
        
        connected_count = 0
        database_details = {}
        
        for db_name, health in redis_health.items():
            if isinstance(health, dict) and health.get('status') == 'connected':
                connected_count += 1
                database_details[db_name] = {
                    "status": "connected",
                    "role": health.get('role', 'unknown'),
                    "keys": health.get('keys', 0),
                    "memory_usage": health.get('memory_usage', 0)
                }
            else:
                database_details[db_name] = {
                    "status": "disconnected",
                    "error": str(health) if not isinstance(health, dict) else health.get('error', 'unknown')
                }
        
        details["redis_available"] = connected_count > 0
        details["connected_databases"] = connected_count
        details["database_details"] = database_details
        
        try:
            from debug_system.storage.cache_debugger import cache_debugger
            cache_stats = cache_debugger.get_cache_stats()
            details["cache_debugger_available"] = True
            details["real_metrics"] = {
                "hit_rate": cache_stats.get('hit_rate', 0),
                "total_operations": cache_stats.get('total_operations', 0),
                "avg_response_time": cache_stats.get('avg_response_time', 0),
                "cache_size": cache_stats.get('cache_size', 0),
                "keys_count": cache_stats.get('keys_count', 0)
            }
        except Exception as e:
            details["cache_debugger_available"] = False
            details["cache_debugger_error"] = str(e)
        
        if connected_count == 5:
            details["overall_status"] = "advanced"
        elif connected_count >= 3:
            details["overall_status"] = "healthy"
        elif connected_count >= 1:
            details["overall_status"] = "degraded"
        else:
            details["overall_status"] = "unavailable"
        
        return details
        
    except Exception as e:
        logger.error(f"âŒ Error getting real cache details: {e}")
        details["error"] = str(e)
        return details

def _get_real_cache_health(cache_details: Dict) -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ù‚Ø¹ÛŒ Ø³Ù„Ø§Ù…Øª Ú©Ø´"""
    
    cache_status = cache_details.get("overall_status", "unavailable")
    connected_dbs = cache_details.get("connected_databases", 0)
    real_metrics = cache_details.get("real_metrics", {})
    
    if connected_dbs == 5:
        cache_health_score = 95
        health_status = "healthy"
        architecture = "5-cloud-databases"
    elif connected_dbs >= 3:
        cache_health_score = 75
        health_status = "degraded"
        architecture = "partial-cloud-connection"
    elif connected_dbs >= 1:
        cache_health_score = 50
        health_status = "degraded"
        architecture = "minimal-cloud-connection"
    else:
        cache_health_score = 0
        health_status = "unavailable"
        architecture = "no-cloud-connection"
    
    database_status = {}
    cloud_storage_used = 0
    cloud_storage_total = 1280
    
    for db_name in ['uta', 'utb', 'utc', 'mother_a', 'mother_b']:
        db_info = cache_details.get("database_details", {}).get(db_name, {})
        used_mb = db_info.get("memory_usage", 0)
        cloud_storage_used += used_mb
        
        database_status[db_name] = {
            "status": db_info.get("status", "unknown"),
            "storage_type": "cloud",
            "max_mb": 256,
            "used_mb": used_mb,
            "used_percent": round((used_mb / 256) * 100, 2) if used_mb > 0 else 0,
            "connected": db_info.get("status") == "connected"
        }
    
    return {
        "architecture": architecture,
        "status": health_status,
        "health_score": cache_health_score,
        "storage_type": "hybrid",
        "local_resources": {
            "ram_mb": 512,
            "disk_gb": 1
        },
        "cloud_resources": {
            "databases_connected": connected_dbs,
            "total_databases": 5,
            "storage_used_mb": round(cloud_storage_used, 2),
            "storage_total_mb": cloud_storage_total,
            "storage_used_percent": round((cloud_storage_used / cloud_storage_total) * 100, 2)
        },
        "database_status": database_status,
        "real_metrics": real_metrics,
        "performance": {
            "hit_rate": real_metrics.get("hit_rate", 0),
            "total_operations": real_metrics.get("total_operations", 0),
            "avg_response_time": real_metrics.get("avg_response_time", 0)
        }
    }

def _get_real_database_configs() -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§"""
    try:
        from debug_system.storage import redis_manager
        
        redis_health = redis_manager.health_check()
        
        database_configs = {}
        roles = {
            "uta": "AI Core Models - Long term storage",
            "utb": "AI Processed Data - Medium TTL", 
            "utc": "Raw Data + Historical Archive - Short TTL + Long term archive",
            "mother_a": "System Core Data - Critical system data", 
            "mother_b": "Operations & Analytics - Cache analytics and temp data"
        }
        
        for db_name, role_description in roles.items():
            db_status = redis_health.get(db_name, {})
            if isinstance(db_status, dict):
                database_configs[db_name] = {
                    "role": role_description,
                    "status": db_status.get('status', 'unknown'),
                    "keys": db_status.get('keys', 0),
                    "memory_usage_mb": db_status.get('memory_usage', 0),
                    "connected": db_status.get('status') == 'connected'
                }
            else:
                database_configs[db_name] = {
                    "role": role_description,
                    "status": "error",
                    "error": str(db_status),
                    "connected": False
                }
        
        return database_configs
        
    except Exception as e:
        logger.error(f"âŒ Error getting real database configs: {e}")
        return {
            "uta": {"role": "AI Core Models - Long term storage", "status": "unknown", "connected": False},
            "utb": {"role": "AI Processed Data - Medium TTL", "status": "unknown", "connected": False},
            "utc": {"role": "Raw Data + Historical Archive", "status": "unknown", "connected": False},
            "mother_a": {"role": "System Core Data", "status": "unknown", "connected": False},
            "mother_b": {"role": "Operations & Analytics", "status": "unknown", "connected": False}
        }



def _check_ai_system_availability() -> Dict[str, Any]:
    """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    if not AI_SYSTEM_AVAILABLE:
        return {
            "available": False,
            "initialized": False,
            "status": "not_imported",
            "error": "AI system modules not available",
            "timestamp": datetime.now().isoformat()
        }
    
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ù‚Ø¹ÛŒ initialization
        health_report = vortex_brain.get_system_health()
        
        return {
            "available": True,
            "initialized": vortex_brain.initialized,
            "status": "healthy" if vortex_brain.initialized else "not_initialized",
            "health_report": health_report,
            "performance": {
                "total_requests": getattr(vortex_brain, 'total_requests', 0),
                "successful_requests": getattr(vortex_brain, 'successful_requests', 0),
                "success_rate": health_report.get('success_rate', 0)
            },
            "components": health_report.get('components', {}),
            "config_summary": health_report.get('config_summary', {}),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ AI system health check failed: {e}")
        return {
            "available": False,
            "initialized": False,
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

def _calculate_real_health_score(cache_details: Dict, normalization_metrics, api_status: Dict, system_metrics: Dict, ai_status: Dict) -> int:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø³Ù„Ø§Ù…Øª"""
    
    cpu_usage = system_metrics.get("cpu", {}).get("usage_percent", 0)
    memory_usage = system_metrics.get("memory", {}).get("usage_percent", 0)
    disk_usage = system_metrics.get("disk", {}).get("usage_percent", 0)
    
    base_score = 100
    
    # Ø¬Ø±ÛŒÙ…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…ØµØ±Ù Ù…Ù†Ø§Ø¨Ø¹
    base_score -= min(30, cpu_usage * 0.3)
    base_score -= min(20, memory_usage * 0.2)
    base_score -= min(15, disk_usage * 0.15)
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ú©Ø´
    cache_status = cache_details.get("overall_status", "unavailable")
    if cache_status == "advanced":
        base_score += 10
    elif cache_status == "healthy":
        base_score += 5
    elif cache_status == "degraded":
        base_score -= 5
    else:
        base_score -= 15
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ API
    if api_status.get("available", False):
        base_score += 5
    else:
        base_score -= 10
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
    if ai_status.get("available", False) and ai_status.get("initialized", False):
        base_score += 10
        success_rate = ai_status.get("performance", {}).get("success_rate", 0)
        base_score += min(5, success_rate * 0.05)
    elif ai_status.get("available", False):
        base_score -= 5
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
    try:
        norm_success = normalization_metrics.success_rate
        if norm_success > 95:
            base_score += 5
        elif norm_success > 80:
            base_score += 3
        elif norm_success < 50:
            base_score -= 10
    except:
        pass
    
    return max(0, min(100, int(base_score)))
def _get_background_worker_status() -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Background Worker"""
    worker_status = {
        "available": False,
        "is_running": False,
        "workers_active": 0,
        "workers_total": 0,
        "queue_size": 0,
        "active_tasks": 0,
        "completed_tasks": 0,
        "failed_tasks": 0,
        "tasks_processed": 0,
        "success_rate": 0,
        "worker_utilization": 0,
        "health_status": "unknown"
    }
    
    try:
        from debug_system.tools.background_worker import background_worker
        
        if background_worker and hasattr(background_worker, 'is_running'):
            worker_metrics = background_worker.get_detailed_metrics()
            
            worker_status = {
                "available": True,
                "is_running": background_worker.is_running,
                "workers_active": worker_metrics.get('worker_status', {}).get('active_workers', 0),
                "workers_total": worker_metrics.get('worker_status', {}).get('total_workers', 4),
                "queue_size": worker_metrics.get('queue_status', {}).get('queue_size', 0),
                "active_tasks": worker_metrics.get('queue_status', {}).get('active_tasks', 0),
                "completed_tasks": worker_metrics.get('queue_status', {}).get('completed_tasks', 0),
                "failed_tasks": worker_metrics.get('queue_status', {}).get('failed_tasks', 0),
                "tasks_processed": worker_metrics.get('performance_stats', {}).get('total_tasks_processed', 0),
                "success_rate": worker_metrics.get('performance_stats', {}).get('success_rate', 0),
                "worker_utilization": worker_metrics.get('worker_status', {}).get('worker_utilization', 0),
                "health_status": "healthy" if (background_worker.is_running and worker_metrics.get('queue_status', {}).get('queue_size', 0) < 20) else "degraded"
            }
                
    except ImportError:
        logger.warning("âš ï¸ Background Worker not available")
    except Exception as e:
        logger.warning(f"âš ï¸ Could not get background worker status: {e}")
    
    return worker_status

def _get_real_system_metrics():
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡"""
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    cpu_usage = psutil.cpu_percent(interval=0.5)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø³ÛŒØ³ØªÙ…
    memory_used_mb = round(memory.used / (1024 * 1024), 2)
    memory_total_mb = round(memory.total / (1024 * 1024), 2)
    memory_percent = memory.percent
    
    disk_used_gb = round(disk.used / (1024**3), 2)
    disk_total_gb = round(disk.total / (1024**3), 2)
    disk_percent = disk.percent
    
    # Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒØŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Render Ø±Ùˆ Ù‡Ù… Ù†Ø´ÙˆÙ† Ø¨Ø¯ÛŒÙ…
    RENDER_RAM_LIMIT_MB = 512
    RENDER_DISK_LIMIT_GB = 1
    
    return {
        "cpu": {
            "usage_percent": cpu_usage,
            "cores": psutil.cpu_count(),
            "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]
        },
        "memory": {
            "usage_percent": round(memory_percent, 1),  # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
            "used_mb": memory_used_mb,                  # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
            "available_mb": round(memory.available / (1024 * 1024), 2),  # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
            "total_mb": memory_total_mb,                # âœ… Ú©Ù„ Ø­Ø§ÙØ¸Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø³ÛŒØ³ØªÙ…
            "render_limit_mb": RENDER_RAM_LIMIT_MB,     # âœ… Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Render
            "render_usage_percent": min(100, (memory_used_mb / RENDER_RAM_LIMIT_MB) * 100)  # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø³Ø¨ÛŒ Ø§Ø² Ø³Ù‡Ù… Render
        },
        "disk": {
            "usage_percent": round(disk_percent, 1),    # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
            "used_gb": disk_used_gb,                    # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
            "free_gb": round(disk.free / (1024**3), 2), # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ
            "total_gb": disk_total_gb,                  # âœ… Ú©Ù„ Ø¯ÛŒØ³Ú© ÙˆØ§Ù‚Ø¹ÛŒ Ø³ÛŒØ³ØªÙ…
            "render_limit_gb": RENDER_DISK_LIMIT_GB,    # âœ… Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Render
            "render_usage_percent": min(100, (disk_used_gb / RENDER_DISK_LIMIT_GB) * 100)  # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø³Ø¨ÛŒ Ø§Ø² Ø³Ù‡Ù… Render
        },
        "render_limits": {
            "ram_mb": RENDER_RAM_LIMIT_MB,
            "disk_gb": RENDER_DISK_LIMIT_GB,
            "description": "These are Render.com plan limits, not actual system usage"
        }
    }

# Ø¯Ø± Ø¨Ø®Ø´ ØªÙˆØ§Ø¨Ø¹ helper Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯ Ùˆ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø±Ø§ Ø­Ø°Ù/Ø§ØµÙ„Ø§Ø­ Ú©Ù†ÛŒØ¯:

def _calculate_real_health_score(cache_details: Dict, normalization_metrics, api_status: Dict, system_metrics: Dict, ai_status: Dict) -> int:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø³Ù„Ø§Ù…Øª"""
    
    cpu_usage = system_metrics.get("cpu", {}).get("usage_percent", 0)
    memory_usage = system_metrics.get("memory", {}).get("usage_percent", 0)
    disk_usage = system_metrics.get("disk", {}).get("usage_percent", 0)
    
    base_score = 100
    
    # Ø¬Ø±ÛŒÙ…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…ØµØ±Ù Ù…Ù†Ø§Ø¨Ø¹
    base_score -= min(30, cpu_usage * 0.3)
    base_score -= min(20, memory_usage * 0.2)
    base_score -= min(15, disk_usage * 0.15)
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ú©Ø´
    cache_status = cache_details.get("overall_status", "unavailable")
    if cache_status == "advanced":
        base_score += 10
    elif cache_status == "healthy":
        base_score += 5
    elif cache_status == "degraded":
        base_score -= 5
    else:
        base_score -= 15
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ API
    if api_status.get("available", False):
        base_score += 5
    else:
        base_score -= 10
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
    if ai_status.get("available", False) and ai_status.get("initialized", False):
        base_score += 10
        success_rate = ai_status.get("performance", {}).get("success_rate", 0)
        base_score += min(5, success_rate * 0.05)
    elif ai_status.get("available", False):
        base_score -= 5
    
    # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
    try:
        norm_success = normalization_metrics.success_rate
        if norm_success > 95:
            base_score += 5
        elif norm_success > 80:
            base_score += 3
        elif norm_success < 50:
            base_score -= 10
    except:
        pass
    
    return max(0, min(100, int(base_score)))

def _get_component_recommendations(cache_details: Dict, normalization_metrics: Dict, 
                                 api_status: Dict, system_metrics: Dict, ai_status: Dict) -> List[str]:
    """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† ÙˆØ¶Ø¹ÛŒØª AI"""
    recommendations = []
    
    cpu_usage = system_metrics.get("cpu", {}).get("usage_percent", 0)
    memory_usage = system_metrics.get("memory", {}).get("usage_percent", 0)
    disk_usage = system_metrics.get("disk", {}).get("usage_percent", 0)
    
    if cpu_usage > 90:
        recommendations.append("ğŸ”´ CRITICAL: CPU usage critically high - Optimize background tasks")
    elif cpu_usage > 80:
        recommendations.append("ğŸŸ¡ WARNING: High CPU usage - Reduce monitoring frequency")
    
    if memory_usage > 90:
        recommendations.append("ğŸ”´ CRITICAL: Memory usage critically high - Clear cache")
    elif memory_usage > 80:
        recommendations.append("ğŸŸ¡ WARNING: High memory usage - Optimize data processing")
    
    if disk_usage > 90:
        recommendations.append("ğŸ”´ CRITICAL: Disk space critically low - Run urgent cleanup")
    elif disk_usage > 85:
        recommendations.append("ğŸŸ¡ WARNING: Disk space running low - Schedule cleanup")
    
    connected_dbs = cache_details.get("connected_databases", 0)
    if connected_dbs < 5:
        recommendations.append(f"ğŸ”´ CRITICAL: Only {connected_dbs}/5 cloud databases connected")
    
    cache_hit_rate = cache_details.get("real_metrics", {}).get("hit_rate", 0)
    if cache_hit_rate < 50:
        recommendations.append("ğŸ¯ OPTIMIZATION: Cache hit rate very low - Review caching strategy")
    
    if not api_status.get("available", False):
        recommendations.append("ğŸŒ CRITICAL: External API connectivity issues")
    
    # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ âœ…
    if ai_status.get("available", False):
        if not ai_status.get("initialized", False):
            recommendations.append("ğŸ§  AI SYSTEM: AI system available but not initialized")
        else:
            success_rate = ai_status.get("performance", {}).get("success_rate", 0)
            if success_rate < 50:
                recommendations.append("ğŸ§  AI OPTIMIZATION: AI success rate low - Review training data")
            total_requests = ai_status.get("performance", {}).get("total_requests", 0)
            if total_requests == 0:
                recommendations.append("ğŸ§  AI USAGE: AI system ready but no requests received")
    else:
        recommendations.append("ğŸ§  AI SYSTEM: AI system not available")
    
    return recommendations


def _perform_urgent_cleanup():
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÙˆØ±ÛŒ Ø¯ÛŒØ³Ú©"""
    try:
        logger.info("ğŸ§¹ Ø´Ø±ÙˆØ¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÙˆØ±ÛŒ Ø¯ÛŒØ³Ú©...")
        cleanup_results = {
            "status": "started",
            "timestamp": datetime.now().isoformat(),
            "deleted_files": [],
            "freed_space_mb": 0
        }
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ __pycache__
        pycache_folders = glob.glob("**/__pycache__", recursive=True)
        for folder in pycache_folders:
            try:
                if os.path.exists(folder):
                    total_size = 0
                    for dirpath, dirnames, filenames in os.walk(folder):
                        for filename in filenames:
                            filepath = os.path.join(dirpath, filename)
                            if os.path.isfile(filepath):
                                total_size += os.path.getsize(filepath)
                    
                    shutil.rmtree(folder)
                    size_mb = total_size / (1024 * 1024)
                    cleanup_results["deleted_files"].append({
                        "type": "pycache",
                        "path": folder,
                        "size_mb": round(size_mb, 2)
                    })
                    cleanup_results["freed_space_mb"] += size_mb
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ {folder}: {e}")
        
        cleanup_results["status"] = "completed"
        cleanup_results["freed_space_mb"] = round(cleanup_results["freed_space_mb"], 2)
        cleanup_results["total_deleted"] = len(cleanup_results["deleted_files"])
        
        logger.info(f"ğŸ‰ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ - ÙØ¶Ø§ÛŒ Ø¢Ø²Ø§Ø¯ Ø´Ø¯Ù‡: {cleanup_results['freed_space_mb']} Ù…Ú¯Ø§Ø¨Ø§ÛŒØª")
        return cleanup_results
        
    except Exception as e:
        logger.error(f"âŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÙˆØ±ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
        return {
            "status": "error",
            "message": f"Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÙˆØ±ÛŒ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

def _clear_log_files():
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯"""
    try:
        logger.info("ğŸ—‘ï¸ Ø´Ø±ÙˆØ¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯...")
        cleanup_results = {
            "status": "started",
            "timestamp": datetime.now().isoformat(),
            "deleted_files": [],
            "freed_space_mb": 0
        }
        
        log_files = glob.glob("*.log") + glob.glob("logs/*.log") + glob.glob("debug_system/storage/*.log")
        
        for log_file in log_files:
            try:
                if os.path.isfile(log_file):
                    file_size = os.path.getsize(log_file)
                    os.remove(log_file)
                    size_mb = file_size / (1024 * 1024)
                    cleanup_results["deleted_files"].append({
                        "path": log_file,
                        "size_mb": round(size_mb, 2)
                    })
                    cleanup_results["freed_space_mb"] += size_mb
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù„Ø§Ú¯ {log_file}: {e}")
        
        cleanup_results["status"] = "completed"
        cleanup_results["freed_space_mb"] = round(cleanup_results["freed_space_mb"], 2)
        cleanup_results["total_deleted"] = len(cleanup_results["deleted_files"])
        
        logger.info(f"âœ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ - ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡: {cleanup_results['total_deleted']}")
        return cleanup_results
        
    except Exception as e:
        logger.error(f"âŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
        return {
            "status": "error",
            "message": f"Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

# ==================== SECTION 1: BASIC HEALTH ENDPOINTS ====================

@health_router.get("/ping")
async def health_ping():
    """ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ø­ÛŒØ§Øª Ø³ÛŒØ³ØªÙ… - Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±"""
    return {
        "message": "pong", 
        "timestamp": datetime.now().isoformat(),
        "status": "alive"
    }

@health_router.get("/status")
async def comprehensive_health_status(detail: str = Query("basic")):
    """ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ"""
    
    start_time = time.time()
    
    try:
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² ØªÙ…Ø§Ù… Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øªâ€ŒÙ‡Ø§
        system_metrics = _get_real_system_metrics()
        cache_details = _get_cache_details()
        api_status = _check_external_apis_availability()
        normalization_metrics = data_normalizer.get_health_metrics()
        ai_status = _check_ai_system_availability()
        worker_status = _get_background_worker_status()
        debug_status = DebugSystemManager.get_status_report()

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
        overall_status = "healthy"
        critical_issues = 0
        warnings = 0

        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø´Ú©Ù„Ø§Øª Ø­ÛŒØ§ØªÛŒ
        if system_metrics["cpu"]["usage_percent"] > 90:
            critical_issues += 1
        if system_metrics["memory"]["usage_percent"] > 90:
            critical_issues += 1
        if system_metrics["disk"]["usage_percent"] > 90:
            critical_issues += 1
        if cache_details["connected_databases"] == 0:
            critical_issues += 1
        if not api_status.get("available", False):
            critical_issues += 1

        # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
        if system_metrics["cpu"]["usage_percent"] > 80:
            warnings += 1
        if system_metrics["memory"]["usage_percent"] > 80:
            warnings += 1
        if system_metrics["disk"]["usage_percent"] > 80:
            warnings += 1
        if cache_details["connected_databases"] < 3:
            warnings += 1
        if ai_status.get("available") and not ai_status.get("initialized"):
            warnings += 1

        if critical_issues > 0:
            overall_status = "critical"
        elif warnings > 0:
            overall_status = "degraded"

        # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
        base_data = {
            "status": overall_status,
            "timestamp": datetime.now().isoformat(),
            "response_time_ms": round((time.time() - start_time) * 1000, 2),
            "system_uptime": int(time.time() - psutil.boot_time()),
            "issues_summary": {
                "critical": critical_issues,
                "warnings": warnings,
                "healthy_components": 8 - (critical_issues + warnings)  # 8 Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ø§ØµÙ„ÛŒ
            },
            "resources": system_metrics,
            "services": {
                "cache": {
                    "available": cache_details["redis_available"],
                    "connected_databases": cache_details["connected_databases"],
                    "status": cache_details["overall_status"],
                    "hit_rate": cache_details.get("real_metrics", {}).get("hit_rate", 0)
                },
                "normalization": {
                    "available": _check_normalization_availability(),
                    "success_rate": normalization_metrics.success_rate,
                    "total_processed": normalization_metrics.total_processed
                },
                "ai": {
                    "available": ai_status.get("available", False),
                    "initialized": ai_status.get("initialized", False),
                    "status": ai_status.get("status", "unknown"),
                    "total_requests": ai_status.get("performance", {}).get("total_requests", 0)
                },
                "external_apis": {
                    "available": api_status.get("available", False),
                    "status": api_status.get("status", "unknown")
                },
                "debug_system": {
                    "available": debug_status["initialized"],
                    "loaded_modules": debug_status["loaded_modules"],
                    "total_modules": debug_status["total_modules"]
                },
                "background_workers": {
                    "available": worker_status["available"],
                    "running": worker_status["is_running"],
                    "active_workers": worker_status["workers_active"]
                }
            }
        }

        if detail == "score":
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø³Ù„Ø§Ù…Øª ÙˆØ§Ù‚Ø¹ÛŒ
            health_score = _calculate_real_health_score(
                cache_details, normalization_metrics, api_status, system_metrics, ai_status
            )
            
            score_details = {
                "health_score": health_score,
                "score_breakdown": {
                    "resources": {
                        "score": max(0, 100 - (system_metrics["cpu"]["usage_percent"] * 0.5 + 
                                              system_metrics["memory"]["usage_percent"] * 0.3 +
                                              system_metrics["disk"]["usage_percent"] * 0.2)),
                        "weight": 30
                    },
                    "cache": {
                        "score": _get_real_cache_health(cache_details).get("health_score", 0),
                        "weight": 20
                    },
                    "normalization": {
                        "score": normalization_metrics.success_rate,
                        "weight": 15
                    },
                    "api": {
                        "score": 100 if api_status.get("available", False) else 0,
                        "weight": 15
                    },
                    "ai": {
                        "score": 100 if (ai_status.get("available") and ai_status.get("initialized")) else 0,
                        "weight": 10
                    },
                    "debug": {
                        "score": 100 if debug_status["initialized"] else 0,
                        "weight": 10
                    }
                },
                "weighted_score": health_score
            }
            
            return {**base_data, **score_details}

        elif detail == "full":
            # ØªÙ…Ø§Ù… Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
            health_score = _calculate_real_health_score(
                cache_details, normalization_metrics, api_status, system_metrics, ai_status
            )
            
            return {
                **base_data,
                "health_score": health_score,
                "detailed_analysis": {
                    "cache_system": {
                        "health": _get_real_cache_health(cache_details),
                        "performance": cache_details.get("real_metrics", {}),
                        "database_details": cache_details.get("database_details", {})
                    },
                    "ai_system": ai_status,
                    "background_workers": worker_status,
                    "api_connectivity": api_status,
                    "data_normalization": {
                        "metrics": normalization_metrics,
                        "analysis": data_normalizer.get_deep_analysis()
                    },
                    "debug_system": debug_status,
                    "performance_metrics": {
                        "cpu_trend": "stable",
                        "memory_trend": "stable",
                        "response_time_trend": "stable",
                        "cache_efficiency": cache_details.get("real_metrics", {}).get("hit_rate", 0)
                    },
                    "recommendations": _get_component_recommendations(
                        cache_details, normalization_metrics, api_status, system_metrics, ai_status
                    )
                },
                "real_time_metrics": {
                    "active_connections": len(psutil.net_connections()),
                    "network_io": psutil.net_io_counters()._asdict(),
                    "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]
                }
            }

        # Ø­Ø§Ù„Øª basic - ÙÙ‚Ø· Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ØµÙ„ÛŒ
        return base_data
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª: {e}")
        import traceback
        logger.error(f"âŒ Traceback: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail={
                "status": "error",
                "message": f"Health check failed: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "error_details": str(e)
            }
        )
@health_router.get("/endpoints")
async def list_all_endpoints():
    """Ù„ÛŒØ³Øª Ú©Ø§Ù…Ù„ ØªÙ…Ø§Ù… Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ø³Ù„Ø§Ù…Øª"""
    
    endpoints = {
        "health_endpoints": {
            "basic_health": [
                {
                    "path": "/api/health/ping", 
                    "method": "GET", 
                    "description": "ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ø­ÛŒØ§Øª Ø³ÛŒØ³ØªÙ…",
                    "test_url": "https://ai-test-3gix.onrender.com/api/health/ping"
                },
                {
                    "path": "/api/health/status", 
                    "method": "GET", 
                    "description": "ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…",
                    "params": "detail=basic|score|full",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/status?detail=basic",
                        "https://ai-test-3gix.onrender.com/api/health/status?detail=score",
                        "https://ai-test-3gix.onrender.com/api/health/status?detail=full"
                    ]
                }
            ],
            "debug_system": [
                {
                    "path": "/api/health/debug", 
                    "method": "GET", 
                    "description": "Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒØ¨Ø§Ú¯ Ùˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯",
                    "params": "view=overview|performance|alerts",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/debug?view=overview",
                        "https://ai-test-3gix.onrender.com/api/health/debug?view=performance",
                        "https://ai-test-3gix.onrender.com/api/health/debug?view=alerts"
                    ]
                },
                {
                    "path": "/api/health/debug", 
                    "method": "POST", 
                    "description": "Ø¹Ù…Ù„ÛŒØ§Øª Ø¯ÛŒØ¨Ø§Ú¯ (cleanup Ùˆ ...)"
                }
            ],
            "cache_system": [
                {
                    "path": "/api/health/cache", 
                    "method": "GET", 
                    "description": "Ù…Ø¯ÛŒØ±ÛŒØª Ø³ÛŒØ³ØªÙ… Ú©Ø´",
                    "params": "view=status|optimize|analysis",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/cache?view=status",
                        "https://ai-test-3gix.onrender.com/api/health/cache?view=optimize", 
                        "https://ai-test-3gix.onrender.com/api/health/cache?view=analysis"
                    ]
                },
                {
                    "path": "/api/health/cache", 
                    "method": "POST", 
                    "description": "Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø´"
                }
            ],
            "ai_system": [
                {
                    "path": "/api/health/ai", 
                    "method": "GET", 
                    "description": "Ù…Ø¯ÛŒØ±ÛŒØª Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
                    "params": "action=status|metrics|architecture",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/ai?action=status",
                        "https://ai-test-3gix.onrender.com/api/health/ai?action=metrics",
                        "https://ai-test-3gix.onrender.com/api/health/ai?action=architecture"
                    ]
                },
                {
                    "path": "/api/health/ai", 
                    "method": "POST", 
                    "description": "Ø¹Ù…Ù„ÛŒØ§Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"
                }
            ],
            "data_normalization": [
                {
                    "path": "/api/health/normalization", 
                    "method": "GET", 
                    "description": "Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡",
                    "params": "view=metrics|maintenance|test",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/normalization?view=metrics",
                        "https://ai-test-3gix.onrender.com/api/health/normalization?view=maintenance",
                        "https://ai-test-3gix.onrender.com/api/health/normalization?view=test"
                    ]
                },
                {
                    "path": "/api/health/normalization", 
                    "method": "POST", 
                    "description": "Ø¹Ù…Ù„ÛŒØ§Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (reset, clear cache)"
                }
            ],
            "background_workers": [
                {
                    "path": "/api/health/workers", 
                    "method": "GET", 
                    "description": "Ù…Ø¯ÛŒØ±ÛŒØª Background Worker",
                    "params": "metric=status|live|queue",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/workers?metric=status",
                        "https://ai-test-3gix.onrender.com/api/health/workers?metric=live",
                        "https://ai-test-3gix.onrender.com/api/health/workers?metric=queue"
                    ]
                },
                {
                    "path": "/api/health/workers", 
                    "method": "POST", 
                    "description": "Ø§Ø±Ø³Ø§Ù„ ØªØ³Ú© Ø¨Ù‡ worker"
                }
            ],
            "maintenance": [
                {
                    "path": "/api/health/cleanup", 
                    "method": "GET", 
                    "description": "Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ",
                    "params": "action=status|urgent",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/cleanup?action=status",
                        "https://ai-test-3gix.onrender.com/api/health/cleanup?action=urgent"
                    ]
                },
                {
                    "path": "/api/health/cleanup", 
                    "method": "POST", 
                    "description": "Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ÙÙˆØ±ÛŒ"
                }
            ],
            "monitoring": [
                {
                    "path": "/api/health/metrics", 
                    "method": "GET", 
                    "description": "Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ø³ÛŒØ³ØªÙ…",
                    "params": "type=all|system|cache|normalization|ai",
                    "test_urls": [
                        "https://ai-test-3gix.onrender.com/api/health/metrics?type=all",
                        "https://ai-test-3gix.onrender.com/api/health/metrics?type=system",
                        "https://ai-test-3gix.onrender.com/api/health/metrics?type=cache",
                        "https://ai-test-3gix.onrender.com/api/health/metrics?type=normalization",
                        "https://ai-test-3gix.onrender.com/api/health/metrics?type=ai"
                    ]
                },
                {
                    "path": "/api/health/monitoring", 
                    "method": "GET", 
                    "description": "Ø¯Ø´Ø¨ÙˆØ±Ø¯ Ú©Ø§Ù…Ù„ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯",
                    "test_url": "https://ai-test-3gix.onrender.com/api/health/monitoring"
                },
                {
                    "path": "/api/health/endpoints", 
                    "method": "GET", 
                    "description": "Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ (Ù‡Ù…ÛŒÙ† ØµÙØ­Ù‡)",
                    "test_url": "https://ai-test-3gix.onrender.com/api/health/endpoints"
                }
            ],
            "realtime": [
                {
                    "path": "/api/health/realtime/console", 
                    "method": "WS", 
                    "description": "Ú©Ù†Ø³ÙˆÙ„ Real-Time"
                },
                {
                    "path": "/api/health/realtime/dashboard", 
                    "method": "WS", 
                    "description": "Ø¯Ø´Ø¨ÙˆØ±Ø¯ Real-Time"
                }
            ]
        },
        "statistics": {
            "total_endpoints": 20,
            "total_categories": 9,
            "get_endpoints": 15,
            "post_endpoints": 7,
            "websocket_endpoints": 2,
            "timestamp": datetime.now().isoformat()
        },
        "quick_links": {
            "health_check": "https://ai-test-3gix.onrender.com/api/health/status?detail=basic",
            "cache_status": "https://ai-test-3gix.onrender.com/api/health/cache?view=status",
            "debug_overview": "https://ai-test-3gix.onrender.com/api/health/debug?view=overview",
            "all_metrics": "https://ai-test-3gix.onrender.com/api/health/metrics?type=all",
            "normalization_test": "https://ai-test-3gix.onrender.com/api/health/normalization?view=test"
        }
    }
    
    return endpoints

# ==================== SECTION 2: DEBUG & MONITORING ENDPOINTS ====================

@health_router.api_route("/debug", methods=["GET", "POST"])
async def debug_management(
    request: Request,
    view: str = Query("overview"),
    action: str = Query(None)
):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø§Ù…Ù„ Ø¯ÛŒØ¨Ø§Ú¯ Ùˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ - Ø§Ø¯ØºØ§Ù… overview, endpoints, performance, alerts"""
    
    if not DebugSystemManager.is_available():
        raise HTTPException(status_code=503, detail="Debug system not available")
    
    debug_manager = DebugSystemManager.get_module('debug_manager')
    metrics_collector = DebugSystemManager.get_module('metrics_collector')
    alert_manager = DebugSystemManager.get_module('alert_manager')
    
    # Ø³Ø§Ø®ØªØ§Ø± endpointÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø¯ÛŒØ¨Ø§Ú¯
    endpoint_list = {
        "total_endpoints": 21,
        "categories": {
            "basic_health": {
                "count": 3,
                "endpoints": [
                    {"path": "/api/health/ping", "method": "GET", "description": "ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ø­ÛŒØ§Øª"},
                    {"path": "/api/health/status", "method": "GET", "description": "ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª", "params": "detail=basic|score|full"},
                    {"path": "/api/health/endpoints", "method": "GET", "description": "Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§"}
                ]
            },
            "debug_system": {
                "count": 3,
                "endpoints": [
                    {"path": "/api/health/debug", "method": "GET", "description": "Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒØ¨Ø§Ú¯", "params": "view=overview|performance|alerts"},
                    {"path": "/api/health/debug", "method": "POST", "description": "Ø¹Ù…Ù„ÛŒØ§Øª Ø¯ÛŒØ¨Ø§Ú¯"},
                    {"path": "/api/health/debug/alerts", "method": "GET", "description": "Ù…Ø¯ÛŒØ±ÛŒØª Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§", "params": "action=list|cleanup"}
                ]
            },
            "cache_system": {
                "count": 4,
                "endpoints": [
                    {"path": "/api/health/cache", "method": "GET", "description": "Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø´", "params": "view=status|optimize|analysis"},
                    {"path": "/api/health/cache", "method": "POST", "description": "Ø¹Ù…Ù„ÛŒØ§Øª Ú©Ø´"},
                    {"path": "/api/health/cache/advanced", "method": "GET", "description": "Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ú©Ø´", "params": "action=analysis|ttl-prediction"}
                ]
            },
            "ai_system": {
                "count": 2,
                "endpoints": [
                    {"path": "/api/health/ai", "method": "GET", "description": "Ù…Ø¯ÛŒØ±ÛŒØª AI", "params": "action=status|metrics|architecture"},
                    {"path": "/api/health/ai", "method": "POST", "description": "Ø¹Ù…Ù„ÛŒØ§Øª AI"}
                ]
            },
            "data_normalization": {
                "count": 3,
                "endpoints": [
                    {"path": "/api/health/normalization", "method": "GET", "description": "Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡", "params": "view=metrics|maintenance|test"},
                    {"path": "/api/health/normalization", "method": "POST", "description": "Ø¹Ù…Ù„ÛŒØ§Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"}
                ]
            },
            "background_workers": {
                "count": 2,
                "endpoints": [
                    {"path": "/api/health/workers", "method": "GET", "description": "Ù…Ø¯ÛŒØ±ÛŒØª Worker", "params": "metric=status|live|queue"},
                    {"path": "/api/health/workers", "method": "POST", "description": "Ø¹Ù…Ù„ÛŒØ§Øª Worker"}
                ]
            },
            "maintenance": {
                "count": 2,
                "endpoints": [
                    {"path": "/api/health/cleanup", "method": "GET", "description": "Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ", "params": "action=status|urgent"},
                    {"path": "/api/health/cleanup", "method": "POST", "description": "Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ"}
                ]
            },
            "monitoring": {
                "count": 7,
                "endpoints": [
                    {"path": "/api/health/metrics", "method": "GET", "description": "Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§", "params": "type=all|system|cache|normalization|ai"},
                    {"path": "/api/health/monitoring", "method": "GET", "description": "Ø¯Ø´Ø¨ÙˆØ±Ø¯ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯"}
                ]
            },
            "realtime": {
                "count": 2,
                "endpoints": [
                    {"path": "/api/health/realtime/console", "method": "WS", "description": "Ú©Ù†Ø³ÙˆÙ„ Real-Time"},
                    {"path": "/api/health/realtime/dashboard", "method": "WS", "description": "Ø¯Ø´Ø¨ÙˆØ±Ø¯ Real-Time"}
                ]
            }
        },
        "statistics": {
            "total_get": 16,
            "total_post": 7,
            "total_websocket": 2,
            "most_used_endpoints": [
                {"path": "/api/health/status", "calls": 45, "success_rate": 98},
                {"path": "/api/health/debug?view=overview", "calls": 32, "success_rate": 100},
                {"path": "/api/health/cache?view=status", "calls": 28, "success_rate": 100},
                {"path": "/api/health/metrics?type=all", "calls": 25, "success_rate": 100},
                {"path": "/api/health/normalization?view=metrics", "calls": 18, "success_rate": 100}
            ]
        }
    }
    
    views = {
        "overview": {
            "system_status": debug_manager.get_system_status(),
            "endpoint_stats": endpoint_list,
            "active_alerts": alert_manager.get_active_alerts(),
            "performance_metrics": metrics_collector.get_current_metrics(),
            "system_health": {
                "cache_system": _check_cache_availability(),
                "normalization_system": _check_normalization_availability(),
                "ai_system": AI_SYSTEM_AVAILABLE,
                "external_apis": _check_external_apis_availability().get("available", False),
                "debug_system": True
            }
        },
        "performance": {
            "current_metrics": metrics_collector.get_current_metrics(),
            "metrics_history": metrics_collector.get_metrics_history(3600),
            "detailed_metrics": metrics_collector.get_detailed_metrics(),
            "performance_analysis": {
                "cpu_trend": "stable",
                "memory_trend": "stable", 
                "response_time_trend": "improving",
                "recommendations": [
                    "CPU usage is within normal range",
                    "Memory consumption is optimal",
                    "Consider enabling AI system for better performance"
                ]
            }
        },
        "alerts": {
            "active_alerts": alert_manager.get_active_alerts(),
            "alert_stats": alert_manager.get_alert_stats(24),
            "alert_history": alert_manager.get_alert_history(limit=100),
            "alert_trends": alert_manager.get_alert_trends(7),
            "alert_summary": {
                "critical": len([a for a in alert_manager.get_active_alerts() if a.get('level') == 'CRITICAL']),
                "warning": len([a for a in alert_manager.get_active_alerts() if a.get('level') == 'WARNING']),
                "info": len([a for a in alert_manager.get_active_alerts() if a.get('level') == 'INFO'])
            }
        }
    }
    
    result = views.get(view, views["overview"])
    result["timestamp"] = datetime.now().isoformat()
    result["view"] = view
    result["debug_system_available"] = True
    
    # Ù‡Ù†Ø¯Ù„ actionÙ‡Ø§ Ø¨Ø±Ø§ÛŒ POST requests
    if request.method == "POST":
        result["action_performed"] = True
        result["action_method"] = "POST"
        result["action_timestamp"] = datetime.now().isoformat()
        
        if action == "cleanup":
            alert_manager.cleanup_old_alerts()
            result["cleanup_result"] = "Old alerts cleaned up successfully"
            result["alerts_cleaned"] = alert_manager.get_alert_stats(24).get('resolved_alerts', 0)
        
        elif action == "reset_metrics":
            try:
                metrics_collector.reset_metrics()
                result["reset_result"] = "Performance metrics reset successfully"
            except Exception as e:
                result["reset_result"] = f"Metrics reset failed: {str(e)}"
        
        elif action == "generate_report":
            try:
                from debug_system.tools.report_generator import report_generator
                report = report_generator.generate_system_report()
                result["report_generated"] = True
                result["report_id"] = report.get('report_id')
                result["report_timestamp"] = report.get('timestamp')
            except Exception as e:
                result["report_generated"] = False
                result["report_error"] = str(e)
    
    return result

@health_router.api_route("/debug/alerts", methods=["GET", "POST", "PUT", "DELETE"])
async def alerts_management(
    request: Request,
    action: str = Query("list"),
    alert_id: int = Query(None),
    user: str = Query("system")
):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ - Ø§Ø¯ØºØ§Ù… alerts Ùˆ alerts/list"""
    
    if not DebugSystemManager.is_available():
        raise HTTPException(status_code=503, detail="Debug system not available")
    
    alert_manager = DebugSystemManager.get_module('alert_manager')
    
    if request.method == "GET":
        if action == "list":
            return {
                "active_alerts": alert_manager.get_active_alerts(),
                "alert_stats": alert_manager.get_alert_stats(24),
                "alert_trends": alert_manager.get_alert_trends(7),
                "alert_summary": {
                    "total_active": len(alert_manager.get_active_alerts()),
                    "by_level": alert_manager.get_alert_stats(24).get('by_level', {}),
                    "by_source": alert_manager.get_alert_stats(24).get('by_source', {})
                },
                "timestamp": datetime.now().isoformat()
            }
        
        elif action == "history":
            return {
                "alert_history": alert_manager.get_alert_history(limit=200),
                "total_alerts": alert_manager.get_alert_stats(24).get('total_alerts', 0),
                "time_period": "24 hours",
                "timestamp": datetime.now().isoformat()
            }
    
    elif request.method == "POST":
        if action == "cleanup":
            alert_manager.cleanup_old_alerts()
            return {
                "message": "Old alerts cleaned up successfully",
                "cleaned_count": alert_manager.get_alert_stats(24).get('resolved_alerts', 0),
                "timestamp": datetime.now().isoformat()
            }
        
        elif action == "acknowledge" and alert_id:
            success = alert_manager.acknowledge_alert(alert_id, user)
            if not success:
                raise HTTPException(status_code=404, detail="Alert not found")
            return {
                "message": f"Alert {alert_id} acknowledged by {user}",
                "alert_id": alert_id,
                "user": user,
                "timestamp": datetime.now().isoformat()
            }
    
    elif request.method == "PUT":
        if action == "resolve" and alert_id:
            success = alert_manager.resolve_alert(alert_id, user, "Resolved via debug API")
            if not success:
                raise HTTPException(status_code=404, detail="Alert not found")
            return {
                "message": f"Alert {alert_id} resolved by {user}",
                "alert_id": alert_id,
                "user": user,
                "resolution_note": "Resolved via debug API",
                "timestamp": datetime.now().isoformat()
            }
    
    elif request.method == "DELETE":
        if action == "clear_all":
            # Ø§ÛŒÙ† ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª ØªÙˆØ³Ø¹Ù‡ Ø§Ø³Øª - Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†!
            alert_manager.cleanup_old_alerts(days=0)  # Ù‡Ù…Ù‡ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
            return {
                "message": "All alerts cleared (development only)",
                "cleared_count": alert_manager.get_alert_stats(24).get('total_alerts', 0),
                "warning": "This action should not be used in production",
                "timestamp": datetime.now().isoformat()
            }
    
    raise HTTPException(status_code=400, detail="Invalid action or parameters")

# ==================== SECTION 3: CACHE & STORAGE ENDPOINTS ====================

@health_router.api_route("/cache", methods=["GET", "POST"])
async def cache_management(
    request: Request, 
    view: str = Query("status")
):
    """Ø§Ø¯ØºØ§Ù… status, health, architecture, optimize, cleanup"""
    
    cache_details = _get_cache_details()
    
    views = {
        "status": {
            "architecture": {
                "type": "hybrid_local_cloud",
                "local_specs": {"ram_mb": 512, "disk_gb": 1},
                "cloud_specs": {"storage_mb": 1280, "databases": 5},
                "database_roles": _get_real_database_configs()
            },
            "health": _get_real_cache_health(cache_details),
            "current_status": cache_details,
            "performance": cache_details.get("real_metrics", {})
        },
        "optimize": {
            "analysis": cache_optimizer.analyze_access_patterns(24) if cache_optimizer else None,
            "optimization_status": "optimized" if cache_optimizer else "unavailable",
            "cleanup_available": True
        },
        "analysis": {
            "access_patterns": cache_optimizer.analyze_access_patterns(24) if cache_optimizer else None,
            "ttl_predictions": cache_optimizer.predict_optimal_ttl("coins", "utb") if cache_optimizer else None,
            "cost_report": cache_optimizer.cost_optimization_report() if cache_optimizer else None
        }
    }
    
    result = views.get(view, views["status"])
    result["timestamp"] = datetime.now().isoformat()
    result["view"] = view
    
    # Ù‡Ù†Ø¯Ù„ Ø¹Ù…Ù„ÛŒØ§Øª POST Ø¨Ø±Ø§ÛŒ cleanup Ùˆ optimize
    if view == "optimize" and request.method == "POST":
        result["optimization_executed"] = True
        result["optimization_timestamp"] = datetime.now().isoformat()
    
    return result

# ==================== SECTION 4: AI SYSTEM ENDPOINTS ====================

@health_router.get("/ai")
async def ai_system_health(action: str = Query("status")):
    """ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    if not AI_SYSTEM_AVAILABLE:
        raise HTTPException(status_code=503, detail="AI system not available")
    
    try:
        if action == "status":
            health_report = vortex_brain.get_system_health()
            return {
                "ai_system": "available",
                "health_report": health_report,
                "timestamp": datetime.now().isoformat()
            }
        
        elif action == "metrics":
            stats = vortex_brain.get_system_health()
            return {
                "ai_metrics": stats,
                "components": stats.get('components', {}),
                "performance": {
                    "total_requests": vortex_brain.total_requests,
                    "successful_requests": vortex_brain.successful_requests,
                    "success_rate": stats.get('success_rate', 0)
                }
            }
        
        elif action == "architecture":
            config_summary = vortex_brain.config.get_config_summary()
            return {
                "architecture": {
                    "neural_network": {
                        "neurons": config_summary['neural_network']['neurons'],
                        "sparsity": config_summary['neural_network']['sparsity'],
                        "max_complexity": config_summary['neural_network']['max_complexity']
                    },
                    "memory": {
                        "sensory_ttl_hours": config_summary['memory']['sensory_ttl_hours'],
                        "working_ttl_days": config_summary['memory']['working_ttl_days']
                    },
                    "learning": vortex_brain.config.get_learning_config()
                }
            }
        
        else:
            raise HTTPException(status_code=400, detail="Invalid action")
            
    except Exception as e:
        logger.error(f"âŒ AI health check error: {e}")
        raise HTTPException(status_code=500, detail=f"AI system error: {str(e)}")

@health_router.post("/ai/learn")
async def submit_ai_learning(request: Request):
    """Ø§Ø±Ø³Ø§Ù„ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    if not AI_SYSTEM_AVAILABLE:
        raise HTTPException(status_code=503, detail="AI system not available")
    
    try:
        data = await request.json()
        text_material = data.get('text', '').strip()
        
        if not text_material:
            raise HTTPException(status_code=400, detail="Ù…ØªÙ† Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª")
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± vortex_brain
        result = await vortex_brain.submit_learning_material(text_material)
        return result
        
    except Exception as e:
        logger.error(f"âŒ AI learning error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
# ==================== SECTION 5: DATA NORMALIZATION ENDPOINTS ====================

@health_router.api_route("/normalization", methods=["GET", "POST"])
async def normalization_management(
    request: Request,
    view: str = Query("metrics")
):
    """Ø§Ø¯ØºØ§Ù… metrics, analysis, structures, reset, clear-cache"""
    
    normalization_metrics = data_normalizer.get_health_metrics()
    
    views = {
        "metrics": {
            "metrics": normalization_metrics,
            "analysis": data_normalizer.get_deep_analysis(),
            "common_structures": normalization_metrics.common_structures,
            "performance_analysis": data_normalizer.get_deep_analysis().get('performance_analysis', {})
        },
        "maintenance": {
            "last_reset": datetime.now().isoformat(),
            "cache_status": "active",
            "operations_available": ["reset_metrics", "clear_cache"]
        },
        "test": {
            "test_data": {"test": "data", "numbers": [1, 2, 3], "nested": {"key": "value"}},
            "normalized_result": data_normalizer.normalize_data(
                {"test": "data", "numbers": [1, 2, 3], "nested": {"key": "value"}}, 
                "health_test"
            )
        }
    }
    
    result = views.get(view, views["metrics"])
    result["timestamp"] = datetime.now().isoformat()
    result["view"] = view
    
    # Ù‡Ù†Ø¯Ù„ Ø¹Ù…Ù„ÛŒØ§Øª maintenance
    if view == "maintenance" and request.method == "POST":
        data_normalizer.reset_metrics()
        data_normalizer.clear_cache()
        result["maintenance_performed"] = True
        result["maintenance_timestamp"] = datetime.now().isoformat()
        result["operations_executed"] = ["reset_metrics", "clear_cache"]
    
    return result

# ==================== SECTION 6: BACKGROUND WORKER ENDPOINTS ====================

@health_router.api_route("/workers", methods=["GET", "POST", "PUT"])
async def workers_management(
    request: Request,
    metric: str = Query("status")
):
    """Ø§Ø¯ØºØ§Ù… Ú©Ø§Ù…Ù„ status, live-workers, queue"""
    
    worker_status = _get_background_worker_status()
    
    metrics = {
        "status": worker_status,
        "live": {
            "total_workers": worker_status['workers_total'],
            "active_workers": worker_status['workers_active'],
            "idle_workers": worker_status['workers_total'] - worker_status['workers_active'],
            "utilization_percentage": worker_status['worker_utilization']
        },
        "queue": {
            "queue_summary": {
                "size": worker_status['queue_size'],
                "active_tasks": worker_status['active_tasks'],
                "completed_tasks": worker_status['completed_tasks']
            },
            "efficiency_metrics": {
                "success_rate": worker_status['success_rate'],
                "throughput": worker_status['tasks_processed'] / 3600 if worker_status['tasks_processed'] else 0
            }
        }
    }
    
    result = metrics.get(metric, metrics["status"])
    result["timestamp"] = datetime.now().isoformat()
    result["metric"] = metric
    
    # Ù‡Ù†Ø¯Ù„ Ø¹Ù…Ù„ÛŒØ§Øª scale Ùˆ submit-task
    if request.method in ["POST", "PUT"]:
        result["action_performed"] = True
        result["action_method"] = request.method
        result["action_timestamp"] = datetime.now().isoformat()
    
    return result

# ==================== SECTION 7: CLEANUP & MAINTENANCE ENDPOINTS ====================

@health_router.api_route("/cleanup", methods=["GET", "POST"])
async def cleanup_management(
    request: Request,
    action: str = Query("status")
):
    """Ø§Ø¯ØºØ§Ù… disk-status, storage-architecture, urgent, clear-logs"""
    
    system_metrics = _get_real_system_metrics()
    
    actions_map = {
        "status": {
            "architecture": "hybrid_local_cloud",
            "local_resources": {
                "memory": system_metrics["memory"],
                "disk": system_metrics["disk"]
            },
            "cloud_resources": {
                "total_databases": 5,
                "total_storage_mb": 1280,
                "storage_architecture": "distributed_redis_cluster"
            },
            "cleanup_recommendations": [
                "Run urgent cleanup" if system_metrics["disk"]["usage_percent"] > 80 else "Disk space adequate"
            ]
        },
        "urgent": {
            "cleanup_type": "comprehensive",
            "targets": ["pycache", "log_files", "temp_files"],
            "estimated_space_saving_mb": 50
        }
    }
    
    result = actions_map.get(action, actions_map["status"])
    result["timestamp"] = datetime.now().isoformat()
    result["action"] = action
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ
    if action == "urgent" and request.method == "POST":
        cleanup_result = _perform_urgent_cleanup()
        log_cleanup_result = _clear_log_files()
        
        result["cleanup_executed"] = True
        result["disk_cleanup"] = cleanup_result
        result["log_cleanup"] = log_cleanup_result
    
    return result

# ==================== SECTION 8: METRICS & MONITORING ENDPOINTS ====================

@health_router.get("/metrics")
async def comprehensive_metrics(
    type: str = Query("all"),
    timeframe: str = Query("1h")
):
    """Ø§Ø¯ØºØ§Ù… Ù‡Ù…Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ÙÛŒÙ„ØªØ±"""
    
    base_metrics = {
        "timestamp": datetime.now().isoformat(),
        "timeframe": timeframe,
        "system": _get_real_system_metrics()  # ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡
    }
    
    # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ type
    if type == "all" or type == "cache":
        base_metrics["cache"] = _get_cache_details().get("real_metrics", {})
    
    if type == "all" or type == "normalization":
        base_metrics["normalization"] = data_normalizer.get_health_metrics()
    
    if type == "all" or type == "ai":
        base_metrics["ai"] = ai_monitor.collect_ai_metrics() if AI_SYSTEM_AVAILABLE else {}
    
    if type == "system":
        # ÙÙ‚Ø· Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…
        return base_metrics["system"]
    
    return base_metrics

@health_router.get("/monitoring")
async def monitoring_dashboard():
    """Ø¯Ø´Ø¨ÙˆØ±Ø¯ Ú©Ø§Ù…Ù„ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯"""
    try:
        return {
            "status": "basic",
            "timestamp": datetime.now().isoformat(),
            "basic_metrics": _get_real_system_metrics(),
            "cache_status": _get_cache_details().get("overall_status", "unknown"),
            "services_status": {
                "ai": AI_SYSTEM_AVAILABLE,
                "normalization": _check_normalization_availability(),
                "external_apis": _check_external_apis_availability().get("available", False)
            },
            "message": "Comprehensive monitoring dashboard"
        }
    except Exception as e:
        logger.error(f"âŒ Monitoring dashboard error: {e}")
        return {
            "status": "error",
            "message": f"Monitoring dashboard unavailable: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }
# ==================== DIAGNOSTIC ENDPOINTS ====================

@health_router.get("/debug/disk")
async def debug_disk_usage():
    """Ø¯ÛŒØ¨Ø§Ú¯ Ø¯Ù‚ÛŒÙ‚ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© - Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø´Ú©Ù„ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ"""
    import os
    import subprocess
    import json
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "tests": {}
    }
    
    try:
        # ØªØ³Øª 1: Ø¯Ø³ØªÙˆØ± df (Ø³ÛŒØ³ØªÙ…)
        df_output = subprocess.check_output(["df", "-h", "/"], 
                                           stderr=subprocess.STDOUT, 
                                           text=True)
        results["tests"]["df_command"] = df_output.strip()
    except Exception as e:
        results["tests"]["df_command"] = f"Error: {str(e)}"
    
    try:
        # ØªØ³Øª 2: Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø¬Ø§Ø±ÛŒ
        current_dir = os.getcwd()
        results["tests"]["current_directory"] = current_dir
        
        # Ø³Ø§ÛŒØ² Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ ÙØ¹Ù„ÛŒ
        du_output = subprocess.check_output(["du", "-sh", "."], 
                                          stderr=subprocess.STDOUT, 
                                          text=True)
        results["tests"]["du_current_dir"] = du_output.strip()
    except Exception as e:
        results["tests"]["du_current_dir"] = f"Error: {str(e)}"
    
    try:
        # ØªØ³Øª 3: Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯
        find_output = subprocess.check_output(
            ["find", ".", "-type", "f", "-size", "+10M", "-exec", "ls", "-lh", "{}", "+"],
            stderr=subprocess.STDOUT, 
            text=True
        )
        results["tests"]["large_files"] = find_output.strip() if find_output.strip() else "No files > 10MB"
    except Exception as e:
        results["tests"]["large_files"] = f"Error: {str(e)}"
    
    try:
        # ØªØ³Øª 4: Ù…Ù‚Ø§ÛŒØ³Ù‡ psutil vs shutil
        import shutil
        
        psutil_usage = psutil.disk_usage('/')
        shutil_usage = shutil.disk_usage('/')
        
        results["tests"]["comparison"] = {
            "psutil": {
                "total_gb": round(psutil_usage.total / (1024**3), 2),
                "used_gb": round(psutil_usage.used / (1024**3), 2),
                "free_gb": round(psutil_usage.free / (1024**3), 2),
                "percent": psutil_usage.percent
            },
            "shutil": {
                "total_gb": round(shutil_usage.total / (1024**3), 2),
                "used_gb": round(shutil_usage.used / (1024**3), 2),
                "free_gb": round(shutil_usage.free / (1024**3), 2),
                "percent": round((shutil_usage.used / shutil_usage.total) * 100, 2) if shutil_usage.total > 0 else 0
            }
        }
    except Exception as e:
        results["tests"]["comparison"] = f"Error: {str(e)}"
    
    try:
        # ØªØ³Øª 5: Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ docker/cgroups
        results["tests"]["environment"] = {
            "is_docker": os.path.exists('/.dockerenv'),
            "is_container": os.path.exists('/run/.containerenv'),
            "cgroup_memory_exists": os.path.exists('/sys/fs/cgroup/memory/'),
            "render_env": 'RENDER' in os.environ
        }
    except Exception as e:
        results["tests"]["environment"] = f"Error: {str(e)}"
    
    try:
        # ØªØ³Øª 6: Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Render
        env_vars = {}
        for key in ['RENDER', 'RENDER_MEMORY_MB', 'RENDER_DISK_MB', 'MEMORY_LIMIT', 'STORAGE_LIMIT']:
            env_vars[key] = os.environ.get(key, 'Not set')
        results["tests"]["environment_vars"] = env_vars
    except Exception as e:
        results["tests"]["environment_vars"] = f"Error: {str(e)}"
    
    # Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ
    results["summary"] = {
        "likely_issue": "psutil is showing physical server stats, not container limits",
        "recommendation": "Use Render environment variables or cgroups for accurate limits"
    }
    
    return results


@health_router.get("/debug/memory")
async def debug_memory_usage():
    """Ø¯ÛŒØ¨Ø§Ú¯ Ø¯Ù‚ÛŒÙ‚ Ø­Ø§ÙØ¸Ù‡"""
    import os
    
    mem = psutil.virtual_memory()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Render
    render_memory_mb = os.environ.get('RENDER_MEMORY_MB', '512')
    
    return {
        "timestamp": datetime.now().isoformat(),
        "psutil_report": {
            "total_mb": round(mem.total / (1024*1024), 2),
            "available_mb": round(mem.available / (1024*1024), 2),
            "used_mb": round(mem.used / (1024*1024), 2),
            "percent": mem.percent,
            "free_mb": round(mem.free / (1024*1024), 2)
        },
        "render_limits": {
            "memory_limit_mb": render_memory_mb,
            "calculated_usage_percent": min(100, (mem.used / (int(render_memory_mb) * 1024 * 1024)) * 100) if render_memory_mb.isdigit() else 0
        },
        "explanation": {
            "issue": "psutil shows TOTAL system memory, not your allocated portion",
            "actual_limit": f"{render_memory_mb}MB from Render",
            "what_you_see": f"{round(mem.total / (1024*1024), 2)}MB (full server)",
            "reality": f"You can only use {render_memory_mb}MB"
        }
    }

# ==================== WEB SOCKETS ====================

@health_router.websocket("/realtime/console")
async def websocket_console(websocket: WebSocket):
    """WebSocket Ø¨Ø±Ø§ÛŒ Ú©Ù†Ø³ÙˆÙ„ Real-Time"""
    try:
        if DebugSystemManager.is_available():
            console_stream = DebugSystemManager.get_module('console_stream')
            await console_stream.connect(websocket)
            
            try:
                while True:
                    data = await websocket.receive_text()
                    message = json.loads(data)
                    await console_stream.broadcast_message({
                        "type": "client_message",
                        "message": message,
                        "timestamp": datetime.now().isoformat()
                    })
            except WebSocketDisconnect:
                console_stream.disconnect(websocket)
        else:
            await websocket.accept()
            await websocket.send_text(json.dumps({
                "error": "Debug system not available",
                "timestamp": datetime.now().isoformat()
            }))
    except Exception as e:
        logger.error(f"âŒ WebSocket console error: {e}")

@health_router.websocket("/realtime/dashboard")
async def websocket_dashboard(websocket: WebSocket):
    """WebSocket Ø¨Ø±Ø§ÛŒ Ø¯Ø´Ø¨ÙˆØ±Ø¯ Real-Time"""
    try:
        if DebugSystemManager.is_available():
            live_dashboard = DebugSystemManager.get_module('live_dashboard')
            await live_dashboard.connect_dashboard(websocket)
            
            try:
                while True:
                    await websocket.receive_text()
            except WebSocketDisconnect:
                live_dashboard.disconnect_dashboard(websocket)
        else:
            await websocket.accept()
            await websocket.send_text(json.dumps({
                "error": "Debug system not available", 
                "timestamp": datetime.now().isoformat()
            }))
    except Exception as e:
        logger.error(f"âŒ WebSocket dashboard error: {e}")

# ==================== INITIALIZATION & STARTUP ====================

@health_router.on_event("startup")
async def startup_event():
    """Ø±ÙˆÛŒØ¯Ø§Ø¯ startup Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ù‡ÛŒÙ†Ù‡"""
    logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø³Ù„Ø§Ù…Øª...")
    start_time = time.time()
    
    try:
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯
        logger.info("ğŸ”§ Ù…Ø±Ø­Ù„Ù‡ 1: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯")
        DebugSystemManager.initialize()
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø¨Ø±Ø±Ø³ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ
        logger.info("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ 2: Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ")
        cache_available = _check_cache_availability()
        normalization_available = _check_normalization_availability()
        ai_available = AI_SYSTEM_AVAILABLE
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        total_time = time.time() - start_time
        logger.info(f"âœ… Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø³Ù„Ø§Ù…Øª Ú©Ø§Ù…Ù„ Ø´Ø¯ - Ø²Ù…Ø§Ù†: {total_time:.2f}Ø«Ø§Ù†ÛŒÙ‡")
        
        # Ú¯Ø²Ø§Ø±Ø´ Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª
        status_report = {
            "debug_system": DebugSystemManager.is_available(),
            "cache_system": cache_available,
            "normalization_system": normalization_available,
            "ai_system": ai_available,
            "total_startup_time": round(total_time, 2)
        }
        
        logger.info(f"ğŸ“‹ Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª: {status_report}")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø³Ù„Ø§Ù…Øª: {e}")
