# routes/ml_routes/predictions.py
from fastapi import APIRouter, HTTPException, Body
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import torch

from ml_core import ml_model_manager, performance_tracker
from data_pipeline import feature_engineer

logger = logging.getLogger(__name__)

predictions_router = APIRouter(prefix="/api/ml/predict", tags=["ML Predictions"])

@predictions_router.post("/technical/{model_name}")
async def predict_technical(
    model_name: str,
    prediction_request: Dict[str, Any] = Body(...)
):
    """Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø² Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„
        if model_name not in ml_model_manager.active_models:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")
        
        logger.info(f"ğŸ¯ Making prediction with {model_name}")
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ
        input_data = await _prepare_prediction_input(prediction_request, model_name)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        start_time = datetime.now()
        prediction_result = await ml_model_manager.predict(model_name, input_data)
        inference_time = (datetime.now() - start_time).total_seconds()
        
        # Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
        performance_tracker.track_inference(
            model_name=model_name,
            inference_time=inference_time,
            confidence=prediction_result.get('confidence', 0.5),
            success=True,
            input_size=input_data.shape
        )
        
        # Ø³Ø§Ø®Øª Ù¾Ø§Ø³Ø®
        response = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "model": model_name,
            "inference_time_seconds": inference_time,
            "prediction": prediction_result
        }
        
        logger.info(f"âœ… Prediction completed in {inference_time:.3f}s")
        return response
        
    except Exception as e:
        logger.error(f"âŒ Prediction failed for {model_name}: {e}")
        
        # Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø®Ø·Ø§
        if 'performance_tracker' in locals():
            performance_tracker.track_inference(
                model_name=model_name,
                inference_time=0,
                confidence=0,
                success=False,
                input_size=(0,)
            )
        
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@predictions_router.post("/batch/technical")
async def batch_predict_technical(
    batch_request: Dict[str, Any] = Body(...)
):
    """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯"""
    try:
        model_name = batch_request.get('model_name', 'technical_analyzer')
        
        if model_name not in ml_model_manager.active_models:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")
        
        input_data_list = batch_request.get('data', [])
        
        if not input_data_list:
            raise HTTPException(status_code=400, detail="No data provided for batch prediction")
        
        logger.info(f"ğŸ¯ Starting batch prediction with {model_name} for {len(input_data_list)} items")
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        batch_tensors = []
        for i, data_item in enumerate(input_data_list):
            try:
                input_tensor = await _prepare_prediction_input(data_item, model_name)
                batch_tensors.append(input_tensor)
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to prepare item {i}: {e}")
                continue
        
        if not batch_tensors:
            raise HTTPException(status_code=400, detail="No valid data items found")
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        start_time = datetime.now()
        batch_results = await ml_model_manager.batch_predict(model_name, batch_tensors)
        total_time = (datetime.now() - start_time).total_seconds()
        
        # Ø³Ø§Ø®Øª Ù¾Ø§Ø³Ø®
        response = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "model": model_name,
            "total_processing_time": total_time,
            "average_time_per_prediction": total_time / len(batch_results),
            "successful_predictions": len(batch_results),
            "failed_predictions": len(input_data_list) - len(batch_results),
            "predictions": batch_results
        }
        
        logger.info(f"âœ… Batch prediction completed: {len(batch_results)} successful predictions")
        return response
        
    except Exception as e:
        logger.error(f"âŒ Batch prediction failed: {e}")
        raise HTTPException(status_code=500, detail=f"Batch prediction failed: {str(e)}")

@predictions_router.get("/confidence/{model_name}")
async def get_model_confidence(model_name: str):
    """Ø¯Ø±ÛŒØ§ÙØª Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† ÙØ¹Ù„ÛŒ Ù…Ø¯Ù„"""
    try:
        if model_name not in ml_model_manager.active_models:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
        from ml_core import performance_tracker
        performance_data = performance_tracker.get_model_performance(model_name, "1h")
        
        confidence_data = {
            "model": model_name,
            "timestamp": datetime.now().isoformat(),
            "current_confidence": performance_data.get('quality_metrics', {}).get('avg_confidence', 0),
            "confidence_trend": "stable",  # Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø² ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆØ¯
            "performance_metrics": {
                "success_rate": performance_data.get('summary', {}).get('success_rate', 0),
                "recent_inferences": performance_data.get('summary', {}).get('total_inferences', 0)
            }
        }
        
        return {
            "status": "success",
            "data": confidence_data
        }
        
    except Exception as e:
        logger.error(f"âŒ Error getting model confidence: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@predictions_router.post("/market/sentiment")
async def predict_market_sentiment(
    sentiment_request: Dict[str, Any] = Body(...)
):
    """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø± (placeholder Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø¢ÛŒÙ†Ø¯Ù‡)"""
    try:
        # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ placeholder Ø§Ø³Øª
        # ÙˆÙ‚ØªÛŒ Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ØŒ Ú©Ø§Ù…Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        
        news_data = sentiment_request.get('news_data', [])
        market_data = sentiment_request.get('market_data', {})
        
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        sentiment_score = _calculate_simple_sentiment(news_data, market_data)
        
        prediction_result = {
            "sentiment_score": sentiment_score,
            "sentiment_label": "positive" if sentiment_score > 0.6 else "negative" if sentiment_score < 0.4 else "neutral",
            "confidence": 0.75,  # placeholder
            "factors_considered": {
                "news_count": len(news_data),
                "market_trend": market_data.get('trend', 'unknown'),
                "analysis_method": "simple_heuristic"
            }
        }
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "data": prediction_result
        }
        
    except Exception as e:
        logger.error(f"âŒ Market sentiment prediction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@predictions_router.post("/custom/{model_name}")
async def custom_model_prediction(
    model_name: str,
    custom_request: Dict[str, Any] = Body(...)
):
    """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ø³ÙØ§Ø±Ø´ÛŒ"""
    try:
        if model_name not in ml_model_manager.active_models:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        input_data = custom_request.get('input_data')
        if input_data is None:
            raise HTTPException(status_code=400, detail="input_data is required")
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ± (Ø¨Ø³ØªÙ‡ Ø¨Ù‡ ÙØ±Ù…Øª Ø¯Ø§Ø¯Ù‡)
        input_tensor = _convert_custom_input(input_data, model_name)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        prediction_result = await ml_model_manager.predict(model_name, input_tensor)
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "model": model_name,
            "prediction": prediction_result
        }
        
    except Exception as e:
        logger.error(f"âŒ Custom prediction failed for {model_name}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def _prepare_prediction_input(prediction_data: Dict[str, Any], model_name: str) -> torch.Tensor:
    """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""
    try:
        if model_name == 'technical_analyzer':
            # Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÚ¯Ø± ØªÚ©Ù†ÛŒÚ©Ø§Ù„ØŒ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            if 'raw_data' in prediction_data:
                # Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ø¯Ø§Ø±ÛŒÙ…ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒÙ…
                from data_pipeline import feature_engineer
                engineered_features = feature_engineer.engineer_market_features({
                    'sources': {'custom_data': {'status': 'success', 'data': prediction_data['raw_data']}}
                })
            else:
                # Ø§Ú¯Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù‚Ø¨Ù„ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø´Ø¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…
                engineered_features = prediction_data.get('engineered_features', {})
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±
            return await _features_to_tensor(engineered_features)
            
        else:
            # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±ØŒ Ù…Ù†Ø·Ù‚ Ù…ØªÙØ§ÙˆØª
            raise ValueError(f"Model {model_name} not supported yet")
            
    except Exception as e:
        logger.error(f"âŒ Error preparing prediction input: {e}")
        raise

async def _features_to_tensor(engineered_features: Dict[str, Any]) -> torch.Tensor:
    """ØªØ¨Ø¯ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø´Ø¯Ù‡ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±"""
    import numpy as np
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ
    numeric_values = []
    
    def extract_numeric_values(data):
        if isinstance(data, dict):
            for value in data.values():
                if isinstance(value, (int, float)):
                    numeric_values.append(value)
                elif isinstance(value, dict):
                    extract_numeric_values(value)
                elif isinstance(value, list):
                    for item in value[:3]:  # ÙÙ‚Ø· 3 Ø¢ÛŒØªÙ… Ø§ÙˆÙ„ Ù„ÛŒØ³Øª
                        if isinstance(item, (int, float)):
                            numeric_values.append(item)
    
    extract_numeric_values(engineered_features)
    
    if not numeric_values:
        raise ValueError("No numeric values found in features")
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±
    values_array = np.array(numeric_values, dtype=np.float32)
    
    # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ú©Ù… Ø§Ø³ØªØŒ padding Ú©Ù†
    if len(values_array) < 50:
        values_array = np.pad(values_array, (0, 50 - len(values_array)), 'constant')
    elif len(values_array) > 100:  # Ø§Ú¯Ø± Ø²ÛŒØ§Ø¯ Ø§Ø³ØªØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ú©Ù†
        values_array = values_array[:100]
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø´Ú©Ù„ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (batch_size, sequence_length, features)
    input_tensor = torch.FloatTensor(values_array).unsqueeze(0).unsqueeze(0)
    return input_tensor

def _calculate_simple_sentiment(news_data: List[Dict], market_data: Dict) -> float:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø±"""
    if not news_data:
        return 0.5  # Ø®Ù†Ø«ÛŒ
    
    positive_keywords = ['ØµØ¹ÙˆØ¯', 'Ø±Ø´Ø¯', 'Ø³ÙˆØ¯', 'Ù…Ø«Ø¨Øª', 'Ù‚ÙˆÛŒ', 'Ø¨Ù‡Ø¨ÙˆØ¯', 'Ø®Ø±ÛŒØ¯']
    negative_keywords = ['Ù†Ø²ÙˆÙ„', 'Ø³Ù‚ÙˆØ·', 'Ø¶Ø±Ø±', 'Ù…Ù†ÙÛŒ', 'Ø¶Ø¹ÛŒÙ', 'Ø±ÛŒØ²Ø´', 'ÙØ±ÙˆØ´']
    
    sentiment_score = 0.5
    keyword_weight = 0.1
    
    for news_item in news_data:
        text = f"{news_item.get('title', '')} {news_item.get('description', '')}".lower()
        
        positive_count = sum(1 for keyword in positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text)
        
        total_keywords = positive_count + negative_count
        if total_keywords > 0:
            item_sentiment = positive_count / total_keywords
            # ÙˆØ²Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø²Ú¯ÛŒ Ø®Ø¨Ø±
            sentiment_score = (sentiment_score + item_sentiment * keyword_weight) / (1 + keyword_weight)
    
    return max(0.0, min(1.0, sentiment_score))

def _convert_custom_input(input_data: Any, model_name: str) -> torch.Tensor:
    """ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø³ÙØ§Ø±Ø´ÛŒ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±"""
    # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ù†ÙˆØ¹ Ù…Ø¯Ù„ Ùˆ ÙØ±Ù…Øª Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…ØªÙØ§ÙˆØª Ø¨Ø§Ø´Ø¯
    if isinstance(input_data, list):
        return torch.FloatTensor(input_data)
    elif isinstance(input_data, dict):
        # ØªØ¨Ø¯ÛŒÙ„ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ù‡ Ù„ÛŒØ³Øª Ù…Ù‚Ø§Ø¯ÛŒØ±
        values = list(input_data.values())
        return torch.FloatTensor(values)
    else:
        raise ValueError(f"Unsupported input data type: {type(input_data)}")
