from fastapi import APIRouter, HTTPException, Query
from datetime import datetime
from typing import List, Optional, Dict, Any
import logging
from complete_coinstats_manager import coin_stats_manager

logger = logging.getLogger(__name__)

news_router = APIRouter(prefix="/api/news", tags=["News"])

@news_router.get("/all", summary="Ø§Ø®Ø¨Ø§Ø± Ø¹Ù…ÙˆÙ…ÛŒ")
async def get_news(
    limit: int = Query(50, ge=1, le=100, description="ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± (Û± ØªØ§ Û±Û°Û°)"),
    page: int = Query(1, ge=1, description="Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡")
):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø² CoinStats API"""
    try:
        logger.info(f"ðŸ“° Fetching news - Limit: {limit}, Page: {page}")
        
        raw_data = coin_stats_manager.get_news(limit=limit)
        
        if "error" in raw_data:
            logger.error(f"âŒ News API error: {raw_data['error']}")
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        news_items = raw_data.get('data', [])
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ø®Ø¨Ø§Ø±
        processed_news = []
        for news_item in news_items:
            processed_news.append({
                'id': news_item.get('id'),
                'title': news_item.get('title'),
                'description': news_item.get('description'),
                'url': news_item.get('url'),
                'source': news_item.get('source'),
                'published_at': news_item.get('published_at', news_item.get('publishedAt')),
                'image_url': news_item.get('imageUrl'),
                'sentiment': _analyze_sentiment(news_item),
                'importance_score': _calculate_importance_score(news_item),
                'reliability_score': _calculate_reliability_score(news_item),
                'tags': news_item.get('tags', []),
                'categories': news_item.get('categories', []),
                'last_updated': datetime.now().isoformat()
            })
        
        # ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§Ø®Ø¨Ø§Ø±
        news_analysis = _analyze_news_collection(processed_news)
        
        response = {
            'status': 'success',
            'data': processed_news,
            'meta': {
                'total': len(processed_news),
                'limit': limit,
                'page': page,
                'analysis': news_analysis
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… News fetched successfully - Total: {len(processed_news)}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ðŸš¨ Unexpected error in news: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@news_router.get("/type/{news_type}", summary="Ø§Ø®Ø¨Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹")
async def get_news_by_type(
    news_type: str,
    limit: int = Query(10, ge=1, le=50, description="ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± (Û± ØªØ§ ÛµÛ°)"),
    page: int = Query(1, ge=1, description="Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡")
):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ - Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù"""
    try:
        # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†ÙˆØ¹ Ø®Ø¨Ø±
        valid_types = ["latest", "trending", "featured", "breaking", "analysis"]
        if news_type not in valid_types:
            raise HTTPException(status_code=400, detail=f"Invalid news type. Valid types: {valid_types}")
        
        logger.info(f"ðŸ“° Fetching {news_type} news - Limit: {limit}")
        
        raw_data = coin_stats_manager.get_news_by_type(news_type, limit=limit)
        
        if "error" in raw_data:
            logger.error(f"âŒ {news_type} news API error: {raw_data['error']}")
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        news_items = raw_data.get('data', [])
        
        processed_news = []
        for news_item in news_items:
            processed_news.append({
                'id': news_item.get('id'),
                'title': news_item.get('title'),
                'description': news_item.get('description'),
                'url': news_item.get('url'),
                'source': news_item.get('source'),
                'published_at': news_item.get('published_at', news_item.get('publishedAt')),
                'image_url': news_item.get('imageUrl'),
                'type': news_type,
                'sentiment': _analyze_sentiment(news_item),
                'importance_score': _calculate_importance_score(news_item),
                'reliability_score': _calculate_reliability_score(news_item),
                'tags': news_item.get('tags', []),
                'last_updated': datetime.now().isoformat()
            })
        
        response = {
            'status': 'success',
            'data': processed_news,
            'meta': {
                'type': news_type,
                'total': len(processed_news),
                'limit': limit,
                'page': page
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… {news_type} news fetched successfully - Total: {len(processed_news)}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ðŸš¨ Error in {news_type} news: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@news_router.get("/sources", summary="Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ")
async def get_news_sources():
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ù…Ø¹ØªØ¨Ø±"""
    try:
        logger.info("ðŸ“° Fetching news sources")
        
        raw_data = coin_stats_manager.get_news_sources()
        
        if "error" in raw_data:
            logger.error(f"âŒ News sources API error: {raw_data['error']}")
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        sources = raw_data.get('data', [])
        
        processed_sources = []
        for source in sources:
            reliability_score = _calculate_source_reliability(source)
            processed_sources.append({
                'id': source.get('id'),
                'name': source.get('name'),
                'url': source.get('url'),
                'description': source.get('description'),
                'language': source.get('language', 'en'),
                'country': source.get('country'),
                'category': source.get('category', 'crypto'),
                'reliability_score': reliability_score,
                'coverage': source.get('coverage', 'general'),
                'last_updated': datetime.now().isoformat()
            })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        processed_sources.sort(key=lambda x: x['reliability_score'], reverse=True)
        
        response = {
            'status': 'success',
            'data': processed_sources,
            'meta': {
                'total': len(processed_sources),
                'high_reliability_sources': len([s for s in processed_sources if s['reliability_score'] >= 4])
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… News sources fetched successfully - Total: {len(processed_sources)}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ðŸš¨ Error in news sources: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@news_router.get("/detail/{news_id}", summary="Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø¨Ø±")
async def get_news_detail(news_id: str):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„ ÛŒÚ© Ø®Ø¨Ø±"""
    try:
        logger.info(f"ðŸ“° Fetching news detail: {news_id}")
        
        raw_data = coin_stats_manager.get_news_detail(news_id)
        
        if "error" in raw_data:
            logger.error(f"âŒ News detail API error: {raw_data['error']}")
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        news_data = raw_data.get('data', {})
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø¨Ø±
        processed_detail = {
            'id': news_data.get('id'),
            'title': news_data.get('title'),
            'content': news_data.get('content', news_data.get('description')),
            'summary': _generate_advanced_summary(news_data),
            'url': news_data.get('url'),
            'source': news_data.get('source'),
            'author': news_data.get('author'),
            'published_at': news_data.get('published_at', news_data.get('publishedAt')),
            'image_url': news_data.get('imageUrl'),
            'sentiment': _analyze_sentiment(news_data),
            'importance_score': _calculate_importance_score(news_data),
            'reliability_score': _calculate_reliability_score(news_data),
            'key_points': _extract_key_points(news_data),
            'tags': news_data.get('tags', []),
            'categories': news_data.get('categories', []),
            'related_coins': _extract_related_coins(news_data),
            'reading_time': _estimate_reading_time(news_data),
            'last_updated': datetime.now().isoformat()
        }
        
        response = {
            'status': 'success',
            'data': processed_detail,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… News detail fetched successfully: {news_id}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ðŸš¨ Error in news detail {news_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@news_router.get("/analysis/sentiment", summary="ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø®Ø¨Ø§Ø±")
async def get_news_sentiment_analysis(
    limit: int = Query(20, ge=1, le=100, description="ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„")
):
    """ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§Ø®Ø¨Ø§Ø±"""
    try:
        logger.info(f"ðŸ“Š Analyzing news sentiment - Limit: {limit}")
        
        raw_data = coin_stats_manager.get_news(limit=limit)
        
        if "error" in raw_data:
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        news_items = raw_data.get('data', [])
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø®Ø¨Ø±
        sentiment_analysis = []
        for news_item in news_items:
            sentiment_data = {
                'id': news_item.get('id'),
                'title': news_item.get('title'),
                'sentiment': _analyze_sentiment(news_item),
                'confidence': _calculate_sentiment_confidence(news_item),
                'keywords': _extract_sentiment_keywords(news_item),
                'impact_score': _calculate_impact_score(news_item)
            }
            sentiment_analysis.append(sentiment_data)
        
        # ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        overall_sentiment = _calculate_overall_sentiment(sentiment_analysis)
        
        response = {
            'status': 'success',
            'data': sentiment_analysis,
            'analysis': {
                'overall_sentiment': overall_sentiment['sentiment'],
                'sentiment_distribution': overall_sentiment['distribution'],
                'average_confidence': overall_sentiment['average_confidence'],
                'total_analyzed': len(sentiment_analysis),
                'market_outlook': overall_sentiment['market_outlook']
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Sentiment analysis completed - Overall: {overall_sentiment['sentiment']}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ðŸš¨ Error in sentiment analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@news_router.get("/trending/topics", summary="Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¯Ø§Øº")
async def get_trending_topics(
    limit: int = Query(10, ge=1, le=50, description="ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª")
):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¯Ø§Øº Ùˆ ØªØ±Ù†Ø¯ Ø§Ø² Ø§Ø®Ø¨Ø§Ø±"""
    try:
        logger.info(f"ðŸ”¥ Extracting trending topics - Limit: {limit}")
        
        raw_data = coin_stats_manager.get_news(limit=50)  # Ø§Ø®Ø¨Ø§Ø± Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡ØªØ±
        
        if "error" in raw_data:
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        news_items = raw_data.get('data', [])
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        trending_topics = _extract_trending_topics(news_items, limit)
        
        response = {
            'status': 'success',
            'data': trending_topics,
            'meta': {
                'total_topics': len(trending_topics),
                'analysis_period': 'recent',
                'sources_analyzed': len(news_items)
            },
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Trending topics extracted - Total: {len(trending_topics)}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ðŸš¨ Error in trending topics: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

# ============================ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ============================

def _analyze_sentiment(news_item: Dict) -> str:
    """ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ø¨Ø± Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
    title = news_item.get('title', '').lower()
    description = news_item.get('description', '').lower()
    content = f"{title} {description}"
    
    if not content.strip():
        return "neutral"
    
    # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
    positive_words = {
        'bullish': 3, 'surge': 3, 'rally': 3, 'gain': 2, 'positive': 2, 
        'growth': 2, 'soar': 3, 'moon': 3, 'breakout': 2, 'uptrend': 2,
        'profit': 2, 'success': 2, 'adoption': 2, 'innovation': 1, 'partnership': 1
    }
    
    negative_words = {
        'bearish': 3, 'drop': 3, 'crash': 3, 'loss': 2, 'negative': 2, 
        'decline': 2, 'suffer': 2, 'dump': 3, 'plunge': 3, 'downtrend': 2,
        'risk': 2, 'warning': 2, 'concern': 1, 'volatility': 1, 'regulation': 1
    }
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²
    positive_score = sum(score for word, score in positive_words.items() if word in content)
    negative_score = sum(score for word, score in negative_words.items() if word in content)
    
    # ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù…ØªÛŒØ§Ø²
    score_diff = positive_score - negative_score
    
    if score_diff >= 3:
        return "strongly_bullish"
    elif score_diff >= 1:
        return "bullish"
    elif score_diff <= -3:
        return "strongly_bearish"
    elif score_diff <= -1:
        return "bearish"
    else:
        return "neutral"

def _calculate_importance_score(news_item: Dict) -> int:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ù‡Ù…ÛŒØª Ø®Ø¨Ø± (Û°-Û±Û°)"""
    score = 0
    
    # Ù…Ù†Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±
    reliable_sources = {
        'cointelegraph': 3, 'decrypt': 3, 'coindesk': 3, 'bloomberg': 4,
        'reuters': 4, 'benzinga': 2, 'newsbtc': 2, 'cryptopotato': 2
    }
    
    source = news_item.get('source', '').lower()
    for rel_source, points in reliable_sources.items():
        if rel_source in source:
            score += points
            break
    
    # Ø·ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† (Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…Ù‡Ù…â€ŒØªØ±Ù†Ø¯)
    title_length = len(news_item.get('title', ''))
    if title_length > 80:
        score += 2
    elif title_length > 50:
        score += 1
    
    # ÙˆØ¬ÙˆØ¯ ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„
    if news_item.get('description') and len(news_item['description']) > 100:
        score += 2
    
    # ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
    important_tags = ['bitcoin', 'ethereum', 'regulation', 'adoption', 'defi', 'nft']
    tags = news_item.get('tags', [])
    if any(tag in important_tags for tag in tags):
        score += 2
    
    return min(score, 10)

def _calculate_reliability_score(news_item: Dict) -> int:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø®Ø¨Ø± (Û±-Ûµ)"""
    source = news_item.get('source', '').lower()
    
    reliability_scores = {
        'cointelegraph': 5, 'decrypt': 4, 'coindesk': 4, 'bloomberg': 5,
        'reuters': 5, 'benzinga': 3, 'newsbtc': 3, 'cryptopotato': 3,
        'dailyhodl': 3, 'cryptoslate': 3
    }
    
    for rel_source, score in reliability_scores.items():
        if rel_source in source:
            return score
    
    return 2  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡

def _calculate_source_reliability(source: Dict) -> int:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…Ù†Ø¨Ø¹"""
    source_name = source.get('name', '').lower()
    
    reliability_scores = {
        'cointelegraph': 5, 'decrypt': 4, 'coindesk': 4, 'bloomberg': 5,
        'reuters': 5, 'benzinga': 3, 'newsbtc': 3, 'cryptopotato': 3
    }
    
    for rel_source, score in reliability_scores.items():
        if rel_source in source_name:
            return score
    
    return 2

def _generate_advanced_summary(news_item: Dict) -> str:
    """ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø®Ø¨Ø±"""
    content = news_item.get('content') or news_item.get('description') or news_item.get('title', '')
    
    if not content:
        return "No summary available"
    
    # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ (Ø¯Ø± Ù†Ø³Ø®Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² NLP Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
    if len(content) > 200:
        return content[:197] + '...'
    return content

def _extract_key_points(news_item: Dict) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø®Ø¨Ø±"""
    content = news_item.get('content') or news_item.get('description') or ''
    title = news_item.get('title', '')
    
    if not content and not title:
        return ["No key points available"]
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…Ù„Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø³Ø§Ø¯Ù‡
    sentences = content.split('.')
    key_points = []
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù†Ú©ØªÙ‡ Ø§ÙˆÙ„
    if title:
        key_points.append(title)
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Û²-Û³ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
    for sentence in sentences[:3]:
        sentence = sentence.strip()
        if len(sentence) > 20 and sentence not in key_points:
            key_points.append(sentence)
    
    return key_points if key_points else [content[:100] + '...' if content else "No content"]

def _extract_related_coins(news_item: Dict) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø² Ø®Ø¨Ø±"""
    content = f"{news_item.get('title', '')} {news_item.get('description', '')}".lower()
    
    crypto_keywords = [
        'bitcoin', 'btc', 'ethereum', 'eth', 'solana', 'sol', 'cardano', 'ada',
        'binance', 'bnb', 'ripple', 'xrp', 'polkadot', 'dot', 'dogecoin', 'doge'
    ]
    
    related_coins = []
    for coin in crypto_keywords:
        if coin in content:
            related_coins.append(coin)
    
    return list(set(related_coins))  # Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ

def _estimate_reading_time(news_item: Dict) -> str:
    """ØªØ®Ù…ÛŒÙ† Ø²Ù…Ø§Ù† Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø®Ø¨Ø±"""
    content = news_item.get('content') or news_item.get('description') or ''
    word_count = len(content.split())
    
    # ÙØ±Ø¶: Û²Û°Û° Ú©Ù„Ù…Ù‡ Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡
    minutes = max(1, round(word_count / 200))
    return f"{minutes} min"

def _analyze_news_collection(news_items: List[Dict]) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§Ø®Ø¨Ø§Ø±"""
    if not news_items:
        return {"message": "No news to analyze"}
    
    sentiment_count = {
        'strongly_bullish': 0,
        'bullish': 0, 
        'neutral': 0,
        'bearish': 0,
        'strongly_bearish': 0
    }
    
    total_importance = 0
    total_reliability = 0
    
    for news in news_items:
        sentiment = news.get('sentiment', 'neutral')
        sentiment_count[sentiment] = sentiment_count.get(sentiment, 0) + 1
        total_importance += news.get('importance_score', 0)
        total_reliability += news.get('reliability_score', 0)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§
    avg_importance = total_importance / len(news_items)
    avg_reliability = total_reliability / len(news_items)
    
    # ØªØ¹ÛŒÛŒÙ† Ø§Ø­Ø³Ø§Ø³Ø§Øª ØºØ§Ù„Ø¨
    dominant_sentiment = max(sentiment_count.items(), key=lambda x: x[1])[0]
    
    return {
        'total_news': len(news_items),
        'sentiment_distribution': sentiment_count,
        'dominant_sentiment': dominant_sentiment,
        'average_importance': round(avg_importance, 2),
        'average_reliability': round(avg_reliability, 2),
        'high_importance_news': len([n for n in news_items if n.get('importance_score', 0) >= 7]),
        'high_reliability_news': len([n for n in news_items if n.get('reliability_score', 0) >= 4])
    }

def _calculate_sentiment_confidence(news_item: Dict) -> float:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ²Ø§Ù† Ø§Ø·Ù…ÛŒÙ†Ø§Ù† ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª"""
    content = f"{news_item.get('title', '')} {news_item.get('description', '')}".lower()
    
    if not content.strip():
        return 0.5
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
    positive_keywords = ['bullish', 'surge', 'rally', 'gain', 'positive', 'growth', 'soar']
    negative_keywords = ['bearish', 'drop', 'crash', 'loss', 'negative', 'decline', 'suffer']
    
    positive_count = sum(1 for word in positive_keywords if word in content)
    negative_count = sum(1 for word in negative_keywords if word in content)
    total_keywords = positive_count + negative_count
    
    if total_keywords == 0:
        return 0.3  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù¾Ø§ÛŒÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø®Ù†Ø«ÛŒ
    
    return min(total_keywords / 10, 0.9)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ Û°-Û°.Û¹

def _extract_sentiment_keywords(news_item: Dict) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª"""
    content = f"{news_item.get('title', '')} {news_item.get('description', '')}".lower()
    
    sentiment_keywords = [
        'bullish', 'bearish', 'surge', 'crash', 'rally', 'drop', 
        'gain', 'loss', 'positive', 'negative', 'growth', 'decline'
    ]
    
    return [word for word in sentiment_keywords if word in content]

def _calculate_impact_score(news_item: Dict) -> int:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªØ§Ø«ÛŒØ± Ø®Ø¨Ø±"""
    score = news_item.get('importance_score', 0) + news_item.get('reliability_score', 0)
    return min(score, 10)

def _calculate_overall_sentiment(sentiment_data: List[Dict]) -> Dict[str, Any]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú©Ù„ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§Ø®Ø¨Ø§Ø±"""
    sentiment_distribution = {
        'strongly_bullish': 0,
        'bullish': 0,
        'neutral': 0, 
        'bearish': 0,
        'strongly_bearish': 0
    }
    
    total_confidence = 0
    
    for item in sentiment_data:
        sentiment = item.get('sentiment', 'neutral')
        sentiment_distribution[sentiment] = sentiment_distribution.get(sentiment, 0) + 1
        total_confidence += item.get('confidence', 0)
    
    # ØªØ¹ÛŒÛŒÙ† Ø§Ø­Ø³Ø§Ø³Ø§Øª ØºØ§Ù„Ø¨
    dominant_sentiment = max(sentiment_distribution.items(), key=lambda x: x[1])[0]
    avg_confidence = total_confidence / len(sentiment_data) if sentiment_data else 0
    
    # ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø²Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø­Ø³Ø§Ø³Ø§Øª
    bull_count = sentiment_distribution['strongly_bullish'] + sentiment_distribution['bullish']
    bear_count = sentiment_distribution['strongly_bearish'] + sentiment_distribution['bearish']
    
    if bull_count > bear_count * 1.5:
        market_outlook = "bullish"
    elif bear_count > bull_count * 1.5:
        market_outlook = "bearish" 
    else:
        market_outlook = "neutral"
    
    return {
        'sentiment': dominant_sentiment,
        'distribution': sentiment_distribution,
        'average_confidence': round(avg_confidence, 2),
        'market_outlook': market_outlook
    }

def _extract_trending_topics(news_items: List[Dict], limit: int = 10) -> List[Dict]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¯Ø§Øº Ùˆ ØªØ±Ù†Ø¯"""
    from collections import Counter
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§
    all_keywords = []
    crypto_terms = [
        'bitcoin', 'ethereum', 'defi', 'nft', 'web3', 'metaverse', 'dao',
        'layer2', 'scaling', 'regulation', 'adoption', 'institutional',
        'bull market', 'bear market', 'halving', 'mining', 'staking'
    ]
    
    for news in news_items:
        title = news.get('title', '').lower()
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±ØªØ¨Ø·
        for term in crypto_terms:
            if term in title:
                all_keywords.append(term)
    
    # Ø´Ù…Ø§Ø±Ø´ Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
    topic_counter = Counter(all_keywords)
    trending_topics = []
    
    for topic, count in topic_counter.most_common(limit):
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Øª ØªØ±Ù†Ø¯
        intensity = min(count / len(news_items) * 100, 100)
        
        trending_topics.append({
            'topic': topic,
            'frequency': count,
            'intensity': round(intensity, 1),
            'trend_level': 'high' if intensity > 30 else 'medium' if intensity > 15 else 'low'
        })
    
    return trending_topics
