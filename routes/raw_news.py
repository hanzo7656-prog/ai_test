from fastapi import APIRouter, HTTPException, Query
from datetime import datetime
from typing import List, Optional, Dict, Any
import logging
from complete_coinstats_manager import coin_stats_manager

logger = logging.getLogger(__name__)

raw_news_router = APIRouter(prefix="/api/raw/news", tags=["Raw News"])

@raw_news_router.get("/all", summary="Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¹Ù…ÙˆÙ…ÛŒ")
async def get_raw_news(limit: int = Query(50, ge=1, le=100)):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø² CoinStats API - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        raw_data = coin_stats_manager.get_news()
        
        if "error" in raw_data:
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø³Ù…Øª Ø³Ø±ÙˆØ±
        news_items = raw_data.get('result', [])[:limit]
        
        # ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ
        news_analysis = _analyze_news_data(news_items)
        
        return {
            'status': 'success',
            'data_type': 'raw_news_feed',
            'source': 'coinstats_api',
            'api_version': 'v1',
            'timestamp': datetime.now().isoformat(),
            'limit_applied': limit,
            'total_available': len(raw_data.get('result', [])),
            'analysis': news_analysis,
            'data': {
                'result': news_items,
                'meta': raw_data.get('meta', {})
            }
        }
        
    except Exception as e:
        logger.error(f"Error in raw news: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@raw_news_router.get("/type/{news_type}", summary="Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡")
async def get_raw_news_by_type(
    news_type: str,
    limit: int = Query(10, ge=1, le=50)
):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø² CoinStats API - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        raw_data = coin_stats_manager.get_news_by_type(news_type)
        
        if "error" in raw_data:
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬
        news_items = raw_data.get('result', [])[:limit]
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡
        category_analysis = _analyze_news_by_category(news_items, news_type)
        
        return {
            'status': 'success',
            'data_type': 'raw_categorized_news',
            'source': 'coinstats_api',
            'api_version': 'v1',
            'news_type': news_type,
            'timestamp': datetime.now().isoformat(),
            'limit_applied': limit,
            'total_available': len(raw_data.get('result', [])),
            'category_analysis': category_analysis,
            'data': {
                'result': news_items,
                'meta': raw_data.get('meta', {})
            }
        }
        
    except Exception as e:
        logger.error(f"Error in raw news by type {news_type}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@raw_news_router.get("/sources", summary="Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ")
async def get_raw_news_sources():
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø§Ø² CoinStats API"""
    try:
        raw_data = coin_stats_manager.get_news_sources()
        
        if "error" in raw_data:
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        # ğŸ”¥ Ø±ÙØ¹ Ù…Ø´Ú©Ù„: Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡
        if isinstance(raw_data, list):
            sources_list = raw_data
        else:
            sources_list = []
        
        # ØªØ­Ù„ÛŒÙ„ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ
        sources_analysis = _analyze_news_sources(sources_list)
        
        return {
            'status': 'success',
            'data_type': 'raw_news_sources',
            'source': 'coinstats_api',
            'api_version': 'v1',
            'timestamp': datetime.now().isoformat(),
            'analysis': sources_analysis,
            'data': {
                'sources': sources_list,
                'raw_response': raw_data
            }
        }
        
    except Exception as e:
        logger.error(f"Error in raw news sources: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@raw_news_router.get("/detail/{news_id}", summary="Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø¨Ø±")
async def get_raw_news_detail(news_id: str):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„ ÛŒÚ© Ø®Ø¨Ø± Ø§Ø² CoinStats API - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        raw_data = coin_stats_manager.get_news_detail(news_id)
        
        if "error" in raw_data:
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        # ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø¨Ø±
        content_analysis = _analyze_news_content(raw_data)
        
        return {
            'status': 'success',
            'data_type': 'raw_news_detail',
            'source': 'coinstats_api',
            'api_version': 'v1',
            'news_id': news_id,
            'timestamp': datetime.now().isoformat(),
            'content_analysis': content_analysis,
            'data': raw_data
        }
        
    except Exception as e:
        logger.error(f"Error in raw news detail {news_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@raw_news_router.get("/sentiment-analysis", summary="ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø®Ø¨Ø§Ø±")
async def get_news_sentiment_analysis(
    limit: int = Query(20, ge=1, le=50),
    news_type: str = Query(None, description="Ù†ÙˆØ¹ Ø®Ø¨Ø±: handpicked, trending, latest, bullish, bearish")
):
    """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ - Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        if news_type:
            raw_data = coin_stats_manager.get_news_by_type(news_type)
        else:
            raw_data = coin_stats_manager.get_news()
        
        if "error" in raw_data:
            raise HTTPException(status_code=500, detail=raw_data["error"])
        
        news_items = raw_data.get('result', [])[:limit]
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
        sentiment_analysis = _perform_sentiment_analysis(news_items)
        
        return {
            'status': 'success',
            'data_type': 'news_sentiment_analysis',
            'source': 'coinstats_api',
            'api_version': 'v1',
            'timestamp': datetime.now().isoformat(),
            'parameters': {
                'limit': limit,
                'news_type': news_type or 'all'
            },
            'sentiment_analysis': sentiment_analysis,
            'sample_data': news_items[:5]  # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
        }
        
    except Exception as e:
        logger.error(f"Error in news sentiment analysis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@raw_news_router.get("/metadata", summary="Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø§Ø®Ø¨Ø§Ø± Ùˆ Ù…Ù†Ø§Ø¨Ø¹")
async def get_news_metadata():
    """Ø¯Ø±ÛŒØ§ÙØª Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø®Ø¨Ø§Ø± Ùˆ Ù…Ù†Ø§Ø¨Ø¹ - Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        return {
            'status': 'success',
            'data_type': 'news_metadata',
            'source': 'coinstats_api',
            'api_version': 'v1',
            'timestamp': datetime.now().isoformat(),
            'available_endpoints': [
                {
                    'endpoint': '/api/raw/news/all',
                    'description': 'Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¹Ù…ÙˆÙ…ÛŒ',
                    'parameters': ['limit'],
                    'use_case': 'ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§Ø²Ø§Ø±'
                },
                {
                    'endpoint': '/api/raw/news/type/{news_type}',
                    'description': 'Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡',
                    'parameters': ['news_type', 'limit'],
                    'news_types': ['handpicked', 'trending', 'latest', 'bullish', 'bearish']
                },
                {
                    'endpoint': '/api/raw/news/sources',
                    'description': 'Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ',
                    'parameters': [],
                    'use_case': 'ØªØ­Ù„ÛŒÙ„ Ø§Ø¹ØªØ¨Ø§Ø± Ùˆ ØªÙˆØ²ÛŒØ¹ Ù…Ù†Ø§Ø¨Ø¹'
                },
                {
                    'endpoint': '/api/raw/news/detail/{news_id}',
                    'description': 'Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„ Ø®Ø¨Ø±',
                    'parameters': ['news_id'],
                    'use_case': 'ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù‚ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø¨Ø±'
                },
                {
                    'endpoint': '/api/raw/news/sentiment-analysis',
                    'description': 'ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø®Ø¨Ø§Ø±',
                    'parameters': ['limit', 'news_type'],
                    'use_case': 'Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª'
                }
            ],
            'news_categories': {
                'handpicked': 'Ø§Ø®Ø¨Ø§Ø± Ù…Ù†ØªØ®Ø¨ Ùˆ Ù…Ù‡Ù…',
                'trending': 'Ø§Ø®Ø¨Ø§Ø± Ø¯Ø§Øº Ùˆ Ù¾Ø±Ø·Ø±ÙØ¯Ø§Ø±',
                'latest': 'Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø±',
                'bullish': 'Ø§Ø®Ø¨Ø§Ø± Ù…Ø«Ø¨Øª Ùˆ ØµØ¹ÙˆØ¯ÛŒ',
                'bearish': 'Ø§Ø®Ø¨Ø§Ø± Ù…Ù†ÙÛŒ Ùˆ Ù†Ø²ÙˆÙ„ÛŒ'
            },
            'data_structure': {
                'news_item': {
                    'id': 'Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§ÛŒ Ø®Ø¨Ø±',
                    'title': 'Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±',
                    'description': 'Ø®Ù„Ø§ØµÙ‡ Ø®Ø¨Ø±',
                    'url': 'Ù„ÛŒÙ†Ú© Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ',
                    'source': 'Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±',
                    'publishedAt': 'Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø±',
                    'tags': 'ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ',
                    'content': 'Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„ (Ø¯Ø± Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø¨Ø±)'
                },
                'news_source': {
                    'id': 'Ø´Ù†Ø§Ø³Ù‡ Ù…Ù†Ø¨Ø¹',
                    'name': 'Ù†Ø§Ù… Ù…Ù†Ø¨Ø¹',
                    'url': 'Ø¢Ø¯Ø±Ø³ ÙˆØ¨Ø³Ø§ÛŒØª',
                    'coverage': 'Ù¾ÙˆØ´Ø´ Ù…ÙˆØ¶ÙˆØ¹ÛŒ'
                }
            },
            'field_descriptions': _get_news_field_descriptions()
        }
        
    except Exception as e:
        logger.error(f"Error in news metadata: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@raw_news_router.get("/debug/manager", summary="Ø¯ÛŒØ¨Ø§Ú¯ Ù…Ø¯ÛŒØ± Ø§Ø®Ø¨Ø§Ø±")
async def debug_news_manager():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú¯Ø´ØªÛŒ Ø§Ø² coin_stats_manager"""
    try:
        # ØªØ³Øª ØªÙ…Ø§Ù… Ù…ØªØ¯Ù‡Ø§
        news_data = coin_stats_manager.get_news()
        news_by_type = coin_stats_manager.get_news_by_type("latest")
        sources_data = coin_stats_manager.get_news_sources()
        
        return {
            'status': 'debug',
            'timestamp': datetime.now().isoformat(),
            'get_news_structure': {
                'type': type(news_data).__name__,
                'keys': list(news_data.keys()) if isinstance(news_data, dict) else 'not_dict',
                'sample': str(news_data)[:200] if news_data else 'empty'
            },
            'get_news_by_type_structure': {
                'type': type(news_by_type).__name__,
                'keys': list(news_by_type.keys()) if isinstance(news_by_type, dict) else 'not_dict',
                'sample': str(news_by_type)[:200] if news_by_type else 'empty'
            },
            'get_news_sources_structure': {
                'type': type(sources_data).__name__,
                'is_list': isinstance(sources_data, list),
                'length': len(sources_data) if isinstance(sources_data, list) else 'not_list',
                'sample': sources_data[:3] if isinstance(sources_data, list) and sources_data else 'empty'
            }
        }
        
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
# ============================ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ============================

def _analyze_news_data(news_items: List[Dict]) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ"""
    if not news_items:
        return {'analysis': 'no_news_data_available'}
    
    # ØªØ­Ù„ÛŒÙ„ Ù…Ù†Ø§Ø¨Ø¹
    sources = {}
    tags = {}
    sentiment_distribution = {
        'positive': 0,
        'negative': 0,
        'neutral': 0
    }
    
    for news in news_items:
        # ØªØ­Ù„ÛŒÙ„ Ù…Ù†Ø§Ø¨Ø¹
        source = news.get('source', 'Unknown')
        sources[source] = sources.get(source, 0) + 1
        
        # ØªØ­Ù„ÛŒÙ„ ØªÚ¯â€ŒÙ‡Ø§
        news_tags = news.get('tags', [])
        for tag in news_tags:
            tags[tag] = tags.get(tag, 0) + 1
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡
        sentiment = _analyze_basic_sentiment(news)
        sentiment_distribution[sentiment] += 1
    
    # ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù†ÛŒ
    timestamps = [news.get('publishedAt') for news in news_items if news.get('publishedAt')]
    recent_news = len([ts for ts in timestamps if _is_recent(ts)])
    
    return {
        'total_news': len(news_items),
        'source_distribution': {
            'total_sources': len(sources),
            'top_sources': dict(sorted(sources.items(), key=lambda x: x[1], reverse=True)[:5])
        },
        'tag_analysis': {
            'total_tags': len(tags),
            'popular_tags': dict(sorted(tags.items(), key=lambda x: x[1], reverse=True)[:10])
        },
        'sentiment_distribution': sentiment_distribution,
        'temporal_analysis': {
            'recent_news': recent_news,
            'coverage_period': f"{len(timestamps)} items with timestamps"
        },
        'content_analysis': {
            'average_title_length': sum(len(news.get('title', '')) for news in news_items) / len(news_items),
            'has_descriptions': len([news for news in news_items if news.get('description')]),
            'has_urls': len([news for news in news_items if news.get('url')])
        }
    }

def _analyze_news_by_category(news_items: List[Dict], category: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡"""
    base_analysis = _analyze_news_data(news_items)
    
    # ØªØ­Ù„ÛŒÙ„ Ù…Ø®ØªØµ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
    category_specific = {
        'category': category,
        'category_characteristics': _get_category_characteristics(category),
        'expected_sentiment': _get_expected_sentiment_for_category(category)
    }
    
    return {**base_analysis, **category_specific}

def _analyze_news_sources(sources: List[Dict]) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ"""
    if not sources:
        return {'analysis': 'no_sources_data_available'}
    
    # ØªØ­Ù„ÛŒÙ„ Ù¾ÙˆØ´Ø´ Ù…ÙˆØ¶ÙˆØ¹ÛŒ
    coverage_types = {}
    for source in sources:
        coverage = source.get('coverage', 'general')
        coverage_types[coverage] = coverage_types.get(coverage, 0) + 1
    
    return {
        'total_sources': len(sources),
        'coverage_distribution': coverage_types,
        'source_reliability_metrics': {
            'has_urls': len([s for s in sources if s.get('url')]),
            'has_names': len([s for s in sources if s.get('name')]),
            'unique_sources': len(set(s.get('id') for s in sources if s.get('id')))
        }
    }

def _analyze_news_content(news_detail: Dict) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø¨Ø±"""
    if not news_detail:
        return {'analysis': 'no_content_available'}
    
    title = news_detail.get('title', '')
    description = news_detail.get('description', '')
    content = news_detail.get('content', '')
    
    # ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ØªÙ†ÛŒ
    text_analysis = {
        'title_length': len(title),
        'description_length': len(description),
        'content_length': len(content),
        'total_text_length': len(title) + len(description) + len(content),
        'has_content': bool(content.strip()),
        'has_description': bool(description.strip())
    }
    
    # ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ±
    advanced_sentiment = _analyze_advanced_sentiment(title + ' ' + description + ' ' + content)
    
    # ØªØ­Ù„ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
    keywords = _extract_keywords(title + ' ' + description)
    
    return {
        'text_analysis': text_analysis,
        'sentiment_analysis': advanced_sentiment,
        'keyword_analysis': {
            'extracted_keywords': keywords[:10],
            'total_keywords': len(keywords)
        },
        'content_quality': {
            'score': _rate_content_quality(news_detail),
            'factors': _get_content_quality_factors(news_detail)
        }
    }

def _perform_sentiment_analysis(news_items: List[Dict]) -> Dict[str, Any]:
    """Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±ÙˆÛŒ Ø§Ø®Ø¨Ø§Ø±"""
    sentiment_results = []
    
    for news in news_items:
        sentiment = _analyze_advanced_sentiment(
            news.get('title', '') + ' ' + news.get('description', '')
        )
        sentiment_results.append({
            'news_id': news.get('id'),
            'title': news.get('title'),
            'sentiment': sentiment
        })
    
    # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª
    sentiment_counts = {
        'very_positive': 0,
        'positive': 0,
        'neutral': 0,
        'negative': 0,
        'very_negative': 0
    }
    
    for result in sentiment_results:
        sentiment_counts[result['sentiment']['overall']] += 1
    
    return {
        'total_analyzed': len(sentiment_results),
        'sentiment_distribution': sentiment_counts,
        'dominant_sentiment': max(sentiment_counts, key=sentiment_counts.get),
        'sentiment_confidence': sum(result['sentiment']['confidence'] for result in sentiment_results) / len(sentiment_results) if sentiment_results else 0,
        'detailed_results': sentiment_results[:10]  # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬
    }

def _analyze_basic_sentiment(news: Dict) -> str:
    """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ"""
    text = (news.get('title', '') + ' ' + news.get('description', '')).lower()
    
    positive_words = ['bullish', 'surge', 'rally', 'gain', 'positive', 'growth', 'up', 'rise', 'profit']
    negative_words = ['bearish', 'drop', 'crash', 'loss', 'negative', 'decline', 'down', 'fall', 'warning']
    
    positive_count = sum(1 for word in positive_words if word in text)
    negative_count = sum(1 for word in negative_words if word in text)
    
    if positive_count > negative_count:
        return 'positive'
    elif negative_count > positive_count:
        return 'negative'
    else:
        return 'neutral'

def _analyze_advanced_sentiment(text: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ± (Ù‚Ø§Ø¨Ù„ Ú¯Ø³ØªØ±Ø´ Ø¨Ø§ ML)"""
    text_lower = text.lower()
    
    # Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø³ØªØ±Ø¯Ù‡â€ŒØªØ± Ú©Ù„Ù…Ø§Øª (Ù‚Ø§Ø¨Ù„ Ú¯Ø³ØªØ±Ø´)
    very_positive_words = ['surge', 'skyrocket', 'explode', 'breakout', 'record high', 'massive']
    positive_words = ['rise', 'gain', 'growth', 'bullish', 'positive', 'optimistic', 'recovery']
    negative_words = ['drop', 'fall', 'decline', 'bearish', 'negative', 'warning', 'concern']
    very_negative_words = ['crash', 'collapse', 'plummet', 'disaster', 'crisis', 'panic']
    
    very_positive = sum(1 for word in very_positive_words if word in text_lower)
    positive = sum(1 for word in positive_words if word in text_lower)
    negative = sum(1 for word in negative_words if word in text_lower)
    very_negative = sum(1 for word in very_negative_words if word in text_lower)
    
    scores = {
        'very_positive': very_positive * 2,
        'positive': positive,
        'neutral': 0,
        'negative': negative,
        'very_negative': very_negative * 2
    }
    
    max_sentiment = max(scores, key=scores.get)
    total_score = sum(scores.values())
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
    confidence = min(total_score / 10, 1.0) if total_score > 0 else 0.0
    
    return {
        'overall': max_sentiment if total_score > 0 else 'neutral',
        'confidence': round(confidence, 2),
        'score_breakdown': scores,
        'trigger_words': _extract_trigger_words(text_lower, 
            very_positive_words + positive_words + negative_words + very_negative_words)
    }

def _extract_keywords(text: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø³Ø§Ø¯Ù‡"""
    # Ú©Ù„Ù…Ø§Øª Ù…ØªØ¯Ø§ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    
    words = text.lower().split()
    keywords = [word for word in words if len(word) > 3 and word not in stop_words]
    
    # Ø´Ù…Ø§Ø±Ø´ ØªÚ©Ø±Ø§Ø±
    from collections import Counter
    return [word for word, count in Counter(keywords).most_common(20)]

def _extract_trigger_words(text: str, trigger_list: List[str]) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ù…Ø­Ø±Ú© Ø§Ø­Ø³Ø§Ø³Ø§Øª"""
    found_triggers = []
    for trigger in trigger_list:
        if trigger in text:
            found_triggers.append(trigger)
    return found_triggers

def _is_recent(timestamp: str) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨ÙˆØ¯Ù† Ø®Ø¨Ø±"""
    try:
        from datetime import datetime, timezone
        news_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        current_time = datetime.now(timezone.utc)
        time_diff = current_time - news_time
        return time_diff.days < 1  # Ø¬Ø¯ÛŒØ¯ØªØ± Ø§Ø² 24 Ø³Ø§Ø¹Øª
    except:
        return False

def _get_category_characteristics(category: str) -> str:
    """ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø®Ø¨Ø±ÛŒ"""
    characteristics = {
        'handpicked': 'Ø§Ø®Ø¨Ø§Ø± Ù…Ù†ØªØ®Ø¨ Ø¨Ø§ Ø§Ù‡Ù…ÛŒØª Ø¨Ø§Ù„Ø§',
        'trending': 'Ø§Ø®Ø¨Ø§Ø± Ù¾Ø±Ø·Ø±ÙØ¯Ø§Ø± Ùˆ Ø¯Ø§Øº',
        'latest': 'Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ ØªØ£Ú©ÛŒØ¯ Ø¨Ø± ØªØ§Ø²Ú¯ÛŒ',
        'bullish': 'Ø§Ø®Ø¨Ø§Ø± Ù…Ø«Ø¨Øª Ùˆ Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ú©Ù†Ù†Ø¯Ù‡',
        'bearish': 'Ø§Ø®Ø¨Ø§Ø± Ù‡Ø´Ø¯Ø§Ø±Ø¯Ù‡Ù†Ø¯Ù‡ Ùˆ Ù…Ø­ØªØ§Ø·Ø§Ù†Ù‡'
    }
    return characteristics.get(category, 'Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ')

def _get_expected_sentiment_for_category(category: str) -> str:
    """Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ"""
    expected = {
        'bullish': 'positive',
        'bearish': 'negative',
        'handpicked': 'neutral',  # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…ØªØ¹Ø§Ø¯Ù„
        'trending': 'varies',
        'latest': 'varies'
    }
    return expected.get(category, 'neutral')

def _rate_content_quality(news: Dict) -> float:
    """Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ú©ÛŒÙÛŒØª Ù…Ø­ØªÙˆØ§"""
    score = 0.0
    
    if news.get('title'):
        score += 0.3
    if news.get('description'):
        score += 0.3
    if news.get('content'):
        score += 0.4
    if news.get('url'):
        score += 0.1
    if news.get('publishedAt'):
        score += 0.1
    
    return min(score, 1.0)

def _get_content_quality_factors(news: Dict) -> List[str]:
    """Ø¹ÙˆØ§Ù…Ù„ Ú©ÛŒÙÛŒØª Ù…Ø­ØªÙˆØ§"""
    factors = []
    
    if news.get('title'):
        factors.append('has_title')
    if news.get('description'):
        factors.append('has_description')
    if news.get('content'):
        factors.append('has_content')
    if news.get('url'):
        factors.append('has_source_url')
    if news.get('publishedAt'):
        factors.append('has_timestamp')
    if news.get('tags'):
        factors.append('has_tags')
    
    return factors

def _get_news_field_descriptions() -> Dict[str, str]:
    """ØªÙˆØ¶ÛŒØ­Ø§Øª ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ"""
    return {
        'id': 'Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§ÛŒ Ø®Ø¨Ø±',
        'title': 'Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±',
        'description': 'Ø®Ù„Ø§ØµÙ‡ ÛŒØ§ Ú†Ú©ÛŒØ¯Ù‡ Ø®Ø¨Ø±',
        'content': 'Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„ Ø®Ø¨Ø±',
        'url': 'Ù„ÛŒÙ†Ú© Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ',
        'source': 'Ù†Ø§Ù… Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±',
        'publishedAt': 'Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± Ø®Ø¨Ø±',
        'author': 'Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø®Ø¨Ø±',
        'tags': 'ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ Ù…Ø±ØªØ¨Ø·',
        'coverage': 'Ø­ÙˆØ²Ù‡ Ù¾ÙˆØ´Ø´ Ø®Ø¨Ø±ÛŒ'
    }
