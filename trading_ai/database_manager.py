# database_manager.py - Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…

import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import os
import json

logger = logging.getLogger(__name__)

class TradingDatabase:
    """Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
    
    def __init__(self):
        self.data_dir = "./trading_data"
        os.makedirs(self.data_dir, exist_ok=True)
        self.raw_data_cache = {}
        self.cache_expiry = 300  # 5 Ø¯Ù‚ÛŒÙ‚Ù‡
        logger.info("ğŸš€ Trading Database Initialized - Raw Data Mode")

    def get_historical_data(self, symbol: str, days: int = 365) -> pd.DataFrame:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ - Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
        try:
            logger.info(f"ğŸ“Š Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ {symbol} Ø¨Ø±Ø§ÛŒ {days} Ø±ÙˆØ²")

            # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            file_path = os.path.join(self.data_dir, f"{symbol}_historical.csv")
            if os.path.exists(file_path):
                df = pd.read_csv(file_path, index_col='timestamp', parse_dates=True)
                if len(df) >= days:
                    logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø´ Ø´Ø¯Ù‡ {symbol} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {len(df)} Ø±Ú©ÙˆØ±Ø¯")
                    return df.tail(days)

            # Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†
            return self._generate_sample_data(symbol, days)

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol}: {e}")
            return self._generate_sample_data(symbol, days)

    def _generate_sample_data(self, symbol: str, days: int) -> pd.DataFrame:
        """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ ÙˆØ§Ù‚Ø¹ÛŒâ€ŒØªØ± Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
        np.random.seed(hash(symbol) % 1000)  # seed Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ÛŒÙ…Ø¨Ù„
        
        dates = pd.date_range(
            end=pd.Timestamp.now(),
            periods=days,
            freq='D'
        )

        # Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ÛŒÙ…Ø¨Ù„
        base_prices = {
            'bitcoin': 45000,
            'ethereum': 2500,
            'solana': 100,
            'binance-coin': 300,
            'cardano': 0.5,
            'ripple': 0.6,
            'default': 100
        }

        base_price = base_prices.get(symbol.lower(), base_prices['default'])
        prices = [base_price]

        for i in range(1, days):
            # ØªØºÛŒÛŒØ±Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒâ€ŒØªØ± Ø¨Ø§ Ø±ÙˆÙ†Ø¯
            volatility = 0.02 + (hash(symbol) % 10) / 100  # Ù†ÙˆØ³Ø§Ù† Ù…ØªÙØ§ÙˆØª
            trend = np.sin(i / 30) * 0.001  # Ø±ÙˆÙ†Ø¯ Ø³ÛŒÙ†ÙˆØ³ÛŒ
            change = np.random.normal(trend, volatility)
            new_price = prices[-1] * (1 + change)
            prices.append(max(new_price, base_price * 0.1))  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‚ÛŒÙ…Øª Ù…Ù†ÙÛŒ

        # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… OHLCV
        df = pd.DataFrame({
            'timestamp': dates,
            'open': [p * (1 + np.random.normal(0, 0.005)) for p in prices],
            'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'close': prices,
            'volume': [abs(np.random.normal(1000000, 500000)) for _ in range(days)]
        })

        df.set_index('timestamp', inplace=True)

        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø¹Ø¯ÛŒ
        file_path = os.path.join(self.data_dir, f"{symbol}_historical.csv")
        df.to_csv(file_path)

        logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ {symbol} ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {len(df)} Ø±Ú©ÙˆØ±Ø¯")
        return df

    def save_market_data(self, symbol: str, data: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø®Ø§Ù…"""
        try:
            file_path = os.path.join(self.data_dir, f"{symbol}_market.json")
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† metadata
            enhanced_data = {
                'symbol': symbol,
                'saved_at': datetime.now().isoformat(),
                'data_source': 'raw_market_data',
                'raw_data': data
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, indent=2, ensure_ascii=False)
                
            logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± {symbol} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol}: {e}")

    def get_market_data(self, symbol: str) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø®Ø§Ù…"""
        try:
            file_path = os.path.join(self.data_dir, f"{symbol}_market.json")
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Ø¨Ø±Ø±Ø³ÛŒ ØªØ§Ø²Ú¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ú©Ù…ØªØ± Ø§Ø² 1 Ø³Ø§Ø¹Øª)
                saved_time = datetime.fromisoformat(data.get('saved_at', ''))
                if datetime.now() - saved_time < timedelta(hours=1):
                    logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± {symbol} Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯")
                    return data
                else:
                    logger.warning(f"âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± {symbol} Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol}: {e}")
            
        return None

    def save_technical_analysis(self, symbol: str, analysis_data: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
        try:
            file_path = os.path.join(self.data_dir, f"{symbol}_analysis.json")
            
            enhanced_analysis = {
                'symbol': symbol,
                'analyzed_at': datetime.now().isoformat(),
                'analysis_type': 'technical',
                'raw_indicators': analysis_data,
                'data_quality': self._assess_analysis_quality(analysis_data)
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(enhanced_analysis, f, indent=2, ensure_ascii=False)
                
            logger.info(f"âœ… ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ {symbol} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù„ÛŒÙ„ {symbol}: {e}")

    def get_technical_analysis(self, symbol: str) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        try:
            file_path = os.path.join(self.data_dir, f"{symbol}_analysis.json")
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
                    
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ­Ù„ÛŒÙ„ {symbol}: {e}")
            
        return None

    def _assess_analysis_quality(self, analysis_data: Dict) -> Dict[str, Any]:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„"""
        quality_metrics = {
            'completeness': 0.0,
            'freshness': 1.0,  # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            'consistency': 0.8,
            'overall_score': 0.0
        }
        
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            required_fields = ['trend', 'levels', 'indicators']
            present_fields = [field for field in required_fields if field in analysis_data]
            quality_metrics['completeness'] = len(present_fields) / len(required_fields)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ú©Ù„ÛŒ
            quality_metrics['overall_score'] = round(
                (quality_metrics['completeness'] * 0.4 +
                 quality_metrics['freshness'] * 0.3 +
                 quality_metrics['consistency'] * 0.3), 3
            )
            
            quality_metrics['quality_level'] = (
                'high' if quality_metrics['overall_score'] > 0.8 else
                'medium' if quality_metrics['overall_score'] > 0.5 else 'low'
            )
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª: {e}")
            
        return quality_metrics

    def get_database_stats(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        try:
            stats = {
                'total_symbols': 0,
                'total_files': 0,
                'total_size_mb': 0,
                'data_types': {},
                'last_updated': datetime.now().isoformat()
            }
            
            if os.path.exists(self.data_dir):
                for file in os.listdir(self.data_dir):
                    if file.endswith(('.csv', '.json')):
                        stats['total_files'] += 1
                        file_path = os.path.join(self.data_dir, file)
                        stats['total_size_mb'] += os.path.getsize(file_path) / (1024 * 1024)
                        
                        # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
                        if 'historical' in file:
                            stats['data_types']['historical'] = stats['data_types'].get('historical', 0) + 1
                        elif 'market' in file:
                            stats['data_types']['market'] = stats['data_types'].get('market', 0) + 1
                        elif 'analysis' in file:
                            stats['data_types']['analysis'] = stats['data_types'].get('analysis', 0) + 1
                
                # ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯
                symbols = set()
                for file in os.listdir(self.data_dir):
                    if '_' in file:
                        symbol = file.split('_')[0]
                        symbols.add(symbol)
                stats['total_symbols'] = len(symbols)
                
            stats['total_size_mb'] = round(stats['total_size_mb'], 2)
            return stats
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: {e}")
            return {'error': str(e)}

    def clear_old_data(self, days_old: int = 30):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        try:
            cutoff_time = datetime.now() - timedelta(days=days_old)
            deleted_files = 0
            
            for file in os.listdir(self.data_dir):
                file_path = os.path.join(self.data_dir, file)
                if os.path.isfile(file_path):
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time < cutoff_time:
                        os.remove(file_path)
                        deleted_files += 1
                        logger.info(f"ğŸ§¹ ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯: {file}")
            
            logger.info(f"âœ… {deleted_files} ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯")
            return deleted_files
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ: {e}")
            return 0

    def backup_database(self, backup_dir: str = "./backup"):
        """Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        try:
            import shutil
            import datetime
            
            if not os.path.exists(backup_dir):
                os.makedirs(backup_dir)
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø¨Ø§ timestamp
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = os.path.join(backup_dir, f"trading_db_backup_{timestamp}")
            
            # Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ
            if os.path.exists(self.data_dir):
                shutil.copytree(self.data_dir, backup_path)
                logger.info(f"âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯: {backup_path}")
                return backup_path
            else:
                logger.warning("âš ï¸ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ: {e}")
            return None

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ù„ÙˆØ¨Ø§Ù„
trading_db = TradingDatabase()
