# sparse_technical_analyzer.py - ØªØ­Ù„ÛŒÙ„â€ŒÚ¯Ø± ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import math
import pandas as pd
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class SparseConfig:
    """Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
    total_neurons: int = 2500
    connections_per_neuron: int = 50
    temporal_sequence: int = 60
    input_features: int = 5
    hidden_size: int = 128
    specialty_groups: Dict = None
    
    def __post_init__(self):
        if self.specialty_groups is None:
            self.specialty_groups = {
                "support_resistance": 800,
                "trend_detection": 700,
                "pattern_recognition": 600,
                "volume_analysis": 400
            }

class SparseTechnicalNeuron(nn.Module):
    """Ù†ÙˆØ±ÙˆÙ† Ø§Ø³Ù¾Ø§Ø±Ø³ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
    def __init__(self, neuron_id: int, specialty: str, config: SparseConfig):
        super().__init__()
        self.neuron_id = neuron_id
        self.specialty = specialty
        self.config = config

        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªØ®ØµØµÛŒ Ø³Ø¨Ú©
        self.sensitivity = nn.Parameter(torch.randn(1) * 0.1 + 1.0)
        self.threshold = nn.Parameter(torch.randn(1) * 0.1 + 0.6)

        # Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³ - ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
        self.connection_weights = nn.Parameter(
            torch.randn(config.connections_per_neuron) * 0.1
        )
        self.connection_indices = None  # Ø¨Ø¹Ø¯Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯

    def set_connections(self, indices: torch.Tensor):
        """ØªÙ†Ø¸ÛŒÙ… Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³"""
        self.connection_indices = indices

    def forward(self, x: torch.Tensor, all_activations: torch.Tensor) -> torch.Tensor:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³"""
        if self.connection_indices is None:
            return torch.tensor(0.0)

        # Ø¬Ù…Ø¹ Ø¢ÙˆØ±ÛŒ ÙØ¹Ø§Ù„ÛŒØª Ù†ÙˆØ±ÙˆÙ† Ù‡Ø§ÛŒ Ù…ØªØµÙ„
        connected_activations = all_activations[:, self.connection_indices]

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù† Ø¯Ø§Ø±
        weighted_input = torch.sum(connected_activations * self.connection_weights, dim=1)

        # ÙØ¹Ø§Ù„ Ø³Ø§Ø²ÛŒ ØªØ®ØµØµÛŒ
        if self.specialty == "support_resistance":
            output = torch.sigmoid(weighted_input * self.sensitivity - self.threshold)
        elif self.specialty == "trend_detection":
            output = torch.tanh(weighted_input * self.sensitivity)
        elif self.specialty == "pattern_recognition":
            output = torch.relu(weighted_input * self.sensitivity - self.threshold)
        else:  # volume_analysis
            output = torch.sigmoid(weighted_input * self.sensitivity)

        return output

class SparseTechnicalNetwork(nn.Module):
    """Ø´Ø¨Ú©Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
    
    def __init__(self, config: SparseConfig):
        super().__init__()
        self.config = config

        # Ù„Ø§ÛŒÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ù…Ø§Ù†ÛŒ
        self.temporal_processor = nn.LSTM(
            input_size=config.input_features,
            hidden_size=config.hidden_size,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
            bidirectional=False
        )

        # Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        self.feature_projection = nn.Sequential(
            nn.Linear(config.hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.Tanh()
        )

        # Ø§ÛŒØ¬Ø§Ø¯ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³
        self.neurons = nn.ModuleList()
        self._initialize_sparse_neurons()
        self._initialize_sparse_connections()

        # Ù„Ø§ÛŒÙ‡ Ø§Ø¯ØºØ§Ù… Ù‡ÙˆØ´Ù…Ù†Ø¯
        self.integrator = nn.Sequential(
            nn.Linear(config.total_neurons, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.Tanh()
        )

        # Ø³Ø±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ØªØ®ØµØµÛŒ
        self.output_heads = nn.ModuleDict({
            'trend_strength': nn.Linear(32, 3),  # ØµØ¹ÙˆØ¯ÛŒØŒ Ù†Ø²ÙˆÙ„ÛŒØŒ Ø®Ù†Ø«ÛŒ
            'pattern_signals': nn.Linear(32, 6),  # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
            'key_levels': nn.Linear(32, 4),  # Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ
            'market_volatility': nn.Linear(32, 1),  # Ù†ÙˆØ³Ø§Ù† Ø¨Ø§Ø²Ø§Ø±
            'signal_confidence': nn.Linear(32, 1)  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„
        })

        logger.info(f"ğŸ§  Ø´Ø¨Ú©Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨Ø§ {config.total_neurons} Ù†ÙˆØ±ÙˆÙ† Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        logger.info(f"ğŸ”— Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³: {config.connections_per_neuron} per neuron")
        logger.info("ğŸ“Š Ø­Ø§Ù„Øª Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… ÙØ¹Ø§Ù„")

    def _initialize_sparse_neurons(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù†ÙˆØ±ÙˆÙ† Ù‡Ø§ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
        neuron_id = 0
        for specialty, count in self.config.specialty_groups.items():
            for i in range(count):
                self.neurons.append(
                    SparseTechnicalNeuron(neuron_id, specialty, self.config)
                )
                neuron_id += 1

    def _initialize_sparse_connections(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
        total_neurons = self.config.total_neurons

        for i, neuron in enumerate(self.neurons):
            # Ø§ØªØµØ§Ù„Ø§Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ
            connections = self._get_smart_connections(i, neuron.specialty)
            neuron.set_connections(connections)

    def _get_smart_connections(self, neuron_idx: int, specialty: str) -> torch.Tensor:
        """Ø§ØªØµØ§Ù„Ø§Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ®ØµØµ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª"""
        connections = set()
        total_neurons = self.config.total_neurons

        # 1. Ø§ØªØµØ§Ù„Ø§Øª Ø¯Ø±ÙˆÙ†â€ŒÚ¯Ø±ÙˆÙ‡ÛŒ (60%)
        specialty_start, specialty_count = self._get_specialty_range(specialty)
        in_group_count = int(self.config.connections_per_neuron * 0.6)

        in_group_indices = torch.randperm(specialty_count)[:in_group_count] + specialty_start
        connections.update(in_group_indices.tolist())

        # 2. Ø§ØªØµØ§Ù„Ø§Øª Ø¨ÛŒÙ†â€ŒÚ¯Ø±ÙˆÙ‡ÛŒ (40%)
        cross_group_count = self.config.connections_per_neuron - len(connections)

        if specialty == "support_resistance":
            target_specialty = "trend_detection"
        elif specialty == "trend_detection":
            target_specialty = "pattern_recognition"
        elif specialty == "pattern_recognition":
            target_specialty = "volume_analysis"
        else:  # volume_analysis
            target_specialty = "support_resistance"

        target_start, target_count = self._get_specialty_range(target_specialty)
        cross_indices = torch.randperm(target_count)[:cross_group_count] + target_start

        connections.update(cross_indices.tolist())

        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±
        return torch.tensor(list(connections)[:self.config.connections_per_neuron])

    def _get_specialty_range(self, specialty: str) -> Tuple[int, int]:
        """Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© ØªØ®ØµØµ"""
        start_idx = 0
        for spec, count in self.config.specialty_groups.items():
            if spec == specialty:
                return start_idx, count
            start_idx += count
        return 0, 0

    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
        batch_size = x.shape[0]

        # 1. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø§ LSTM
        temporal_out, (hidden, cell) = self.temporal_processor(x)
        temporal_features = hidden[-1]  # Ø¢Ø®Ø±ÛŒÙ† hidden state

        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…
        base_features = self.feature_projection(temporal_features)

        # 3. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³
        all_activations = base_features.unsqueeze(1).repeat(1, self.config.total_neurons, 1)
        neuron_outputs = []

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ 100 ØªØ§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±Ø¹Øª
        batch_size_neurons = 100
        for i in range(0, len(self.neurons), batch_size_neurons):
            batch_neurons = self.neurons[i:i+batch_size_neurons]
            batch_outputs = []

            for neuron in batch_neurons:
                neuron_out = neuron(base_features, all_activations)
                batch_outputs.append(neuron_out.unsqueeze(1))

            neuron_outputs.extend(batch_outputs)

        # ØªØ±Ú©ÛŒØ¨ Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§
        all_neuron_outputs = torch.cat(neuron_outputs, dim=1)

        # 4. Ø§Ø¯ØºØ§Ù… Ù‡ÙˆØ´Ù…Ù†Ø¯ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§
        integrated = self.integrator(all_neuron_outputs)

        # 5. ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        outputs = {}
        for name, head in self.output_heads.items():
            outputs[name] = head(integrated)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ù„ÛŒ
        outputs['overall_confidence'] = torch.sigmoid(
            outputs['signal_confidence']
        ).squeeze(-1)

        # ÙØ¹Ø§Ù„ÛŒØª Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ
        outputs['specialty_activities'] = self._calculate_specialty_activities(
            all_neuron_outputs
        )

        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…
        outputs['raw_data_metrics'] = {
            'input_shape': x.shape,
            'processed_sequences': batch_size,
            'timestamp': datetime.now().isoformat()
        }

        return outputs

    def _calculate_specialty_activities(self, neuron_outputs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ¹Ø§Ù„ÛŒØª Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø± Ú¯Ø±ÙˆÙ‡ ØªØ®ØµØµÛŒ"""
        activities = {}
        start_idx = 0

        for specialty, count in self.config.specialty_groups.items():
            end_idx = start_idx + count
            specialty_outputs = neuron_outputs[:, start_idx:end_idx]
            activities[specialty] = specialty_outputs.mean(dim=1)
            start_idx = end_idx

        return activities

    def analyze_raw_market_data(self, raw_data: Dict) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø®Ø§Ù…"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…
            processed_data = self._preprocess_raw_data(raw_data)
            
            if processed_data is None:
                return {
                    'error': 'Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ù†Ø§Ù…Ø¹ØªØ¨Ø±',
                    'confidence': 0.0,
                    'raw_data_quality': 'poor'
                }

            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±
            input_tensor = torch.FloatTensor(processed_data).unsqueeze(0)
            
            # ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Ø´Ø¨Ú©Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³
            with torch.no_grad():
                analysis_results = self.forward(input_tensor)
            
            # ØªÙØ³ÛŒØ± Ù†ØªØ§ÛŒØ¬
            interpreted_results = self._interpret_analysis(analysis_results, raw_data)
            
            return {
                **interpreted_results,
                'raw_data_used': True,
                'processing_timestamp': datetime.now().isoformat(),
                'model_confidence': analysis_results['overall_confidence'].item()
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…: {e}")
            return {
                'error': str(e),
                'confidence': 0.0,
                'raw_data_quality': 'error'
            }

    def _preprocess_raw_data(self, raw_data: Dict) -> Optional[np.ndarray]:
        """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø´Ø¨Ú©Ù‡"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ Ø®Ø§Ù…
            price_data = raw_data.get('prices', [])
            volume_data = raw_data.get('volumes', [])
            
            if len(price_data) < self.config.temporal_sequence:
                logger.warning("âŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ù†Ø§Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„")
                return None
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            normalized_prices = self._normalize_data(price_data)
            normalized_volumes = self._normalize_data(volume_data) if volume_data else np.zeros_like(normalized_prices)
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
            sequences = []
            for i in range(len(normalized_prices) - self.config.temporal_sequence + 1):
                sequence = np.column_stack([
                    normalized_prices[i:i+self.config.temporal_sequence],
                    normalized_volumes[i:i+self.config.temporal_sequence] if len(volume_data) > 0 else np.zeros(self.config.temporal_sequence)
                ])
                sequences.append(sequence)
            
            return np.array(sequences)
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…: {e}")
            return None

    def _normalize_data(self, data: List[float]) -> np.ndarray:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
        data_array = np.array(data)
        if len(data_array) == 0:
            return np.array([])
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ min-max
        if np.max(data_array) != np.min(data_array):
            normalized = (data_array - np.min(data_array)) / (np.max(data_array) - np.min(data_array))
        else:
            normalized = np.ones_like(data_array) * 0.5
            
        return normalized

    def _interpret_analysis(self, analysis_results: Dict, raw_data: Dict) -> Dict[str, Any]:
        """ØªÙØ³ÛŒØ± Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ Ø´Ø¨Ú©Ù‡"""
        try:
            # ØªÙØ³ÛŒØ± Ø±ÙˆÙ†Ø¯
            trend_probs = torch.softmax(analysis_results['trend_strength'], dim=-1)
            trend_labels = ['ØµØ¹ÙˆØ¯ÛŒ', 'Ù†Ø²ÙˆÙ„ÛŒ', 'Ø®Ù†Ø«ÛŒ']
            dominant_trend = trend_labels[torch.argmax(trend_probs).item()]
            
            # ØªÙØ³ÛŒØ± Ø§Ù„Ú¯ÙˆÙ‡Ø§
            pattern_probs = torch.softmax(analysis_results['pattern_signals'], dim=-1)
            pattern_labels = ['Ø³Ø± Ùˆ Ø´Ø§Ù†Ù‡', 'Ø¯Ùˆ Ù‚Ù„Ù‡', 'Ø¯Ùˆ Ø¯Ø±Ù‡', 'Ù…Ø«Ù„Ø«', 'Ú©Ù†Ø¬', 'Ú©Ø§Ù†Ø§Ù„']
            dominant_pattern = pattern_labels[torch.argmax(pattern_probs).item()]
            
            # ØªÙØ³ÛŒØ± Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ
            key_levels = analysis_results['key_levels'].squeeze().tolist()
            
            return {
                'trend_analysis': {
                    'direction': dominant_trend,
                    'confidence': trend_probs.max().item(),
                    'probabilities': trend_probs.squeeze().tolist()
                },
                'pattern_analysis': {
                    'detected_pattern': dominant_pattern,
                    'confidence': pattern_probs.max().item(),
                    'all_patterns': dict(zip(pattern_labels, pattern_probs.squeeze().tolist()))
                },
                'key_levels': {
                    'support': key_levels[0] if len(key_levels) > 0 else 0,
                    'resistance': key_levels[1] if len(key_levels) > 1 else 0,
                    'breakout_level': key_levels[2] if len(key_levels) > 2 else 0,
                    'rejection_level': key_levels[3] if len(key_levels) > 3 else 0
                },
                'market_metrics': {
                    'volatility': analysis_results['market_volatility'].item(),
                    'signal_confidence': analysis_results['signal_confidence'].item(),
                    'overall_confidence': analysis_results['overall_confidence'].item()
                },
                'specialty_activities': {
                    k: v.item() for k, v in analysis_results['specialty_activities'].items()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙØ³ÛŒØ± Ù†ØªØ§ÛŒØ¬: {e}")
            return {
                'error': 'Ø®Ø·Ø§ Ø¯Ø± ØªÙØ³ÛŒØ± Ù†ØªØ§ÛŒØ¬',
                'trend_analysis': {'direction': 'Ù†Ø§Ù…Ø´Ø®Øµ', 'confidence': 0.0},
                'pattern_analysis': {'detected_pattern': 'Ù‡ÛŒÚ†', 'confidence': 0.0}
            }

class TechnicalAnalysisTrainer:
    """Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡Ù†Ø¯Ù‡ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
    
    def __init__(self, config: SparseConfig):
        self.config = config
        self.model = SparseTechnicalNetwork(config)
        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        self.criterion = nn.CrossEntropyLoss()
        
        logger.info("ğŸ¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡Ù†Ø¯Ù‡ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")

    def train_on_historical_data(self, symbols: List[str], epochs: int = 50):
        """Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø®Ø§Ù…"""
        # Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
        logger.info(f"ğŸ“š Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ {len(symbols)} Ù†Ù…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ {epochs} Ø¯ÙˆØ±Ù‡")
        # TODO: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø¢Ù…ÙˆØ²Ø´
        
    def analyze_market(self, market_data: torch.Tensor) -> Dict:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ø¨Ø§Ø²Ø§Ø± Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
        self.model.eval()
        with torch.no_grad():
            return self.model(market_data)

# ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯
def test_sparse_architecture():
    """ØªØ³Øª Ú©Ø§Ù…Ù„ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
    config = SparseConfig()
    model = SparseTechnicalNetwork(config)

    # ØªØ³Øª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    sample_data = torch.randn(32, 60, 5)  # 32 Ù†Ù…ÙˆÙ†Ù‡ØŒ 60 Ú©Ù†Ø¯Ù„ØŒ 5 ÙˆÛŒÚ˜Ú¯ÛŒ
    outputs = model(sample_data)

    print("âœ… Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³ ØªØ±Ú©ÛŒØ¨ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯!")
    print(f"ğŸ§  Ù†ÙˆØ±ÙˆÙ† Ù‡Ø§: {config.total_neurons}")
    print(f"ğŸ”— Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³: {config.connections_per_neuron} per neuron")
    print(f"ğŸ“Š Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ú©Ù„: {sum(p.numel() for p in model.parameters())}")

    print("\nğŸ“ˆ Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
    for key, value in outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
        elif isinstance(value, dict):
            print(f"  {key}: {len(value)} Ú¯Ø±ÙˆÙ‡ ØªØ®ØµØµÛŒ")

    # ØªØ³Øª Ø³Ø±Ø¹Øª
    import time
    start_time = time.time()
    
    for _ in range(100):
        _ = model(sample_data)
        
    end_time = time.time()
    avg_time = (end_time - start_time) / 100 * 1000  # Ù…ÛŒÙ„ÛŒ Ø«Ø§Ù†ÛŒÙ‡

    print(f"\nâš¡ Ø³Ø±Ø¹Øª Ù…ØªÙˆØ³Ø·: {avg_time:.1f}ms per ØªØ­Ù„ÛŒÙ„")
    print(f"ğŸ“Š {1000/avg_time:.0f} ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø«Ø§Ù†ÙŠÙ‡")

    return model

if __name__ == "__main__":
    analyzer = test_sparse_architecture()
