import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import math

logger = logging.getLogger(__name__)

@dataclass
class SparseConfig:
    """Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
    total_neurons: int = 2500
    connections_per_neuron: int = 50
    temporal_sequence: int = 60
    input_features: int = 5
    hidden_size: int = 128
    specialty_groups: Dict = None
    
    def __post_init__(self):
        if self.specialty_groups is None:
            self.specialty_groups = {
                "support_resistance": 800,
                "trend_detection": 700, 
                "pattern_recognition": 600,
                "volume_analysis": 400
            }

class SparseTechnicalNeuron(nn.Module):
    """Ù†ÙˆØ±ÙˆÙ† Ø§Ø³Ù¾Ø§Ø±Ø³ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
    
    def __init__(self, neuron_id: int, specialty: str, config: SparseConfig):
        super().__init__()
        self.neuron_id = neuron_id
        self.specialty = specialty
        self.config = config
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªØ®ØµØµÛŒ Ø³Ø¨Ú©
        self.sensitivity = nn.Parameter(torch.randn(1) * 0.1 + 1.0)
        self.threshold = nn.Parameter(torch.randn(1) * 0.1 + 0.6)
        
        # Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³ - ÙÙ‚Ø· ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
        self.connection_weights = nn.Parameter(
            torch.randn(config.connections_per_neuron) * 0.1
        )
        self.connection_indices = None  # Ø¨Ø¹Ø¯Ø§Ù‹ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        
    def set_connections(self, indices: torch.Tensor):
        """ØªÙ†Ø¸ÛŒÙ… Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³"""
        self.connection_indices = indices
        
    def forward(self, x: torch.Tensor, all_activations: torch.Tensor) -> torch.Tensor:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³"""
        if self.connection_indices is None:
            return torch.tensor(0.0)
            
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ÙØ¹Ø§Ù„ÛŒØª Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªØµÙ„
        connected_activations = all_activations[:, self.connection_indices]
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù†â€ŒØ¯Ø§Ø±
        weighted_input = torch.sum(connected_activations * self.connection_weights, dim=1)
        
        # ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ØªØ®ØµØµÛŒ
        if self.specialty == "support_resistance":
            output = torch.sigmoid(weighted_input * self.sensitivity - self.threshold)
        elif self.specialty == "trend_detection":
            output = torch.tanh(weighted_input * self.sensitivity)
        elif self.specialty == "pattern_recognition":
            output = torch.relu(weighted_input * self.sensitivity - self.threshold)
        else:  # volume_analysis
            output = torch.sigmoid(weighted_input * self.sensitivity)
            
        return output

class SparseTechnicalNetwork(nn.Module):
    """Ø´Ø¨Ú©Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
    
    def __init__(self, config: SparseConfig):
        super().__init__()
        self.config = config
        
        # Ù„Ø§ÛŒÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ù…Ø§Ù†ÛŒ
        self.temporal_processor = nn.LSTM(
            input_size=config.input_features,
            hidden_size=config.hidden_size,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
            bidirectional=False
        )
        
        # Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        self.feature_projection = nn.Sequential(
            nn.Linear(config.hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.Tanh()
        )
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³
        self.neurons = nn.ModuleList()
        self._initialize_sparse_neurons()
        self._initialize_sparse_connections()
        
        # Ù„Ø§ÛŒÙ‡ Ø§Ø¯ØºØ§Ù… Ù‡ÙˆØ´Ù…Ù†Ø¯
        self.integrator = nn.Sequential(
            nn.Linear(config.total_neurons, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.Tanh()
        )
        
        # Ø³Ø±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ØªØ®ØµØµÛŒ
        self.output_heads = nn.ModuleDict({
            'trend_strength': nn.Linear(32, 3),  # ØµØ¹ÙˆØ¯ÛŒØŒ Ù†Ø²ÙˆÙ„ÛŒØŒ Ø®Ù†Ø«ÛŒ
            'pattern_signals': nn.Linear(32, 6), # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
            'key_levels': nn.Linear(32, 4),      # Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ
            'market_volatility': nn.Linear(32, 1), # Ù†ÙˆØ³Ø§Ù† Ø¨Ø§Ø²Ø§Ø±
            'signal_confidence': nn.Linear(32, 1)  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„
        })
        
        logger.info(f"ğŸ§  Ø´Ø¨Ú©Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨Ø§ {config.total_neurons} Ù†ÙˆØ±ÙˆÙ† Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        logger.info(f"ğŸ”— Ø§ØªØµØ§Ù„Ø§Øª: {config.connections_per_neuron} per neuron")
        logger.info(f"ğŸ’¾ Ø­Ø§ÙØ¸Ù‡ ØªØ®Ù…ÛŒÙ†ÛŒ: ~70MB")
        
    def _initialize_sparse_neurons(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
        neuron_id = 0
        for specialty, count in self.config.specialty_groups.items():
            for i in range(count):
                self.neurons.append(
                    SparseTechnicalNeuron(neuron_id, specialty, self.config)
                )
                neuron_id += 1
    
    def _initialize_sparse_connections(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
        total_neurons = self.config.total_neurons
        
        for i, neuron in enumerate(self.neurons):
            # Ø§ØªØµØ§Ù„Ø§Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ
            connections = self._get_smart_connections(i, neuron.specialty)
            neuron.set_connections(connections)
    
    def _get_smart_connections(self, neuron_idx: int, specialty: str) -> torch.Tensor:
        """Ø§ØªØµØ§Ù„Ø§Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ®ØµØµ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª"""
        connections = set()
        total_neurons = self.config.total_neurons
        
        # 1. Ø§ØªØµØ§Ù„Ø§Øª Ø¯Ø±ÙˆÙ†â€ŒÚ¯Ø±ÙˆÙ‡ÛŒ (60%)
        specialty_start, specialty_count = self._get_specialty_range(specialty)
        in_group_count = int(self.config.connections_per_neuron * 0.6)
        
        in_group_indices = torch.randperm(specialty_count)[:in_group_count] + specialty_start
        connections.update(in_group_indices.tolist())
        
        # 2. Ø§ØªØµØ§Ù„Ø§Øª Ø¨ÛŒÙ†â€ŒÚ¯Ø±ÙˆÙ‡ÛŒ Ù…Ø±ØªØ¨Ø· (40%)
        cross_group_count = self.config.connections_per_neuron - len(connections)
        
        if specialty == "support_resistance":
            target_specialty = "trend_detection"
        elif specialty == "trend_detection":
            target_specialty = "pattern_recognition"
        elif specialty == "pattern_recognition":
            target_specialty = "volume_analysis"
        else:  # volume_analysis
            target_specialty = "support_resistance"
            
        target_start, target_count = self._get_specialty_range(target_specialty)
        cross_indices = torch.randperm(target_count)[:cross_group_count] + target_start
        connections.update(cross_indices.tolist())
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±
        return torch.tensor(list(connections)[:self.config.connections_per_neuron])
    
    def _get_specialty_range(self, specialty: str) -> Tuple[int, int]:
        """Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© ØªØ®ØµØµ"""
        start_idx = 0
        for spec, count in self.config.specialty_groups.items():
            if spec == specialty:
                return start_idx, count
            start_idx += count
        return 0, 0
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
        batch_size = x.shape[0]
        
        # 1. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø§ LSTM
        temporal_out, (hidden, cell) = self.temporal_processor(x)
        temporal_features = hidden[-1]  # Ø¢Ø®Ø±ÛŒÙ† hidden state
        
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
        base_features = self.feature_projection(temporal_features)
        
        # 3. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³
        all_activations = base_features.unsqueeze(1).repeat(1, self.config.total_neurons)
        neuron_outputs = []
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ 100 ØªØ§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±Ø¹Øª
        batch_size = 100
        for i in range(0, len(self.neurons), batch_size):
            batch_neurons = self.neurons[i:i+batch_size]
            batch_outputs = []
            
            for neuron in batch_neurons:
                neuron_out = neuron(base_features, all_activations)
                batch_outputs.append(neuron_out.unsqueeze(1))
            
            neuron_outputs.extend(batch_outputs)
        
        # ØªØ±Ú©ÛŒØ¨ Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§
        all_neuron_outputs = torch.cat(neuron_outputs, dim=1)
        
        # 4. Ø§Ø¯ØºØ§Ù… Ù‡ÙˆØ´Ù…Ù†Ø¯ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§
        integrated = self.integrator(all_neuron_outputs)
        
        # 5. ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        outputs = {}
        for name, head in self.output_heads.items():
            outputs[name] = head(integrated)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ù„ÛŒ
        outputs['overall_confidence'] = torch.sigmoid(
            outputs['signal_confidence']
        ).squeeze(-1)
        
        # ÙØ¹Ø§Ù„ÛŒØª Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ
        outputs['specialty_activities'] = self._calculate_specialty_activities(
            all_neuron_outputs
        )
        
        return outputs
    
    def _calculate_specialty_activities(self, neuron_outputs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ¹Ø§Ù„ÛŒØª Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø± Ú¯Ø±ÙˆÙ‡ ØªØ®ØµØµÛŒ"""
        activities = {}
        start_idx = 0
        
        for specialty, count in self.config.specialty_groups.items():
            end_idx = start_idx + count
            specialty_outputs = neuron_outputs[:, start_idx:end_idx]
            activities[specialty] = specialty_outputs.mean(dim=1)
            start_idx = end_idx
            
        return activities

class TechnicalAnalysisTrainer:
    """Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯Ù‡Ù†Ø¯Ù‡ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„"""
    
    def __init__(self, config: SparseConfig):
        self.config = config
        self.model = SparseTechnicalNetwork(config)
        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        
    def train_on_historical_data(self, symbols: List[str], epochs: int = 50):
        """Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ"""
        # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
        pass
    
    def analyze_market(self, market_data: torch.Tensor) -> Dict:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ø¨Ø§Ø²Ø§Ø± Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ"""
        self.model.eval()
        with torch.no_grad():
            return self.model(market_data)

# ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯
def test_sparse_architecture():
    """ØªØ³Øª Ú©Ø§Ù…Ù„ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
    config = SparseConfig()
    model = SparseTechnicalNetwork(config)
    
    # ØªØ³Øª Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    sample_data = torch.randn(32, 60, 5)  # 32 Ù†Ù…ÙˆÙ†Ù‡ØŒ 60 Ú©Ù†Ø¯Ù„ØŒ 5 ÙˆÛŒÚ˜Ú¯ÛŒ
    outputs = model(sample_data)
    
    print("âœ… Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³ ØªØ±Ú©ÛŒØ¨ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯!")
    print(f"ğŸ§  Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§: {config.total_neurons}")
    print(f"ğŸ”— Ø§ØªØµØ§Ù„Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ø³: {config.connections_per_neuron} per neuron")
    print(f"ğŸ’¾ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ú©Ù„: {sum(p.numel() for p in model.parameters()):,}")
    
    print("\nğŸ“Š Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
    for key, value in outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
        elif isinstance(value, dict):
            print(f"  {key}: {len(value)} Ú¯Ø±ÙˆÙ‡ ØªØ®ØµØµÛŒ")
    
    # ØªØ³Øª Ø³Ø±Ø¹Øª
    import time
    start_time = time.time()
    for _ in range(100):
        _ = model(sample_data)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / 100 * 1000  # Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡
    print(f"\nâš¡ Ø³Ø±Ø¹Øª Ù…ØªÙˆØ³Ø·: {avg_time:.1f}ms per ØªØ­Ù„ÛŒÙ„")
    print(f"ğŸ¯ ÙØ±ÛŒÙ…â€ŒØ±ÛŒØª: {1000/avg_time:.0f} ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø«Ø§Ù†ÛŒÙ‡")
    
    return model

if __name__ == "__main__":
    analyzer = test_sparse_architecture()
