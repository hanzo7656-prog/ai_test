# model_trainer.py - Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡Ù†Ø¯Ù‡ Ù…Ø¯Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from typing import Dict, List, Optional
import logging
import joblib
from datetime import datetime
from trading_ai.database_manager import trading_db
from trading_ai.advanced_technical_engine import technical_engine
from trading_ai.sparse_technical_analyzer import SparseTechnicalNetwork, SparseConfig

logger = logging.getLogger(__name__)

class SparseModelTrainer:
    """Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡Ù†Ø¯Ù‡ Ù…Ø¯Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
    
    def __init__(self):
        self.config = SparseConfig()
        self.model = None
        self.scaler = None
        self.training_history = []
        logger.info("ğŸš€ Sparse Model Trainer Initialized - Raw Data Mode")

    def train_technical_analysis(self, symbols: List[str], epochs: int = 30) -> Dict:
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø±ÙˆÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
        try:
            all_sequences = []
            all_labels = []
            raw_data_quality = {}

            # Ø¬Ù…Ø¹ Ø¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø§Ø² ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§
            for symbol in symbols:
                sequences, labels = technical_engine.prepare_training_data(symbol)
                
                if sequences is not None:
                    all_sequences.append(sequences)
                    all_labels.append(labels)
                    
                    # Ø«Ø¨Øª Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…
                    raw_data_quality[symbol] = {
                        'sequences_count': len(sequences),
                        'data_points': sequences.shape[0] * sequences.shape[1] if len(sequences.shape) > 1 else 0,
                        'quality_score': self._calculate_sequence_quality(sequences)
                    }
                    
                    logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: {len(sequences)} Ù†Ù…ÙˆÙ†Ù‡")

            if not all_sequences:
                logger.error("âŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                return {}

            # ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§
            X_train = np.concatenate(all_sequences, axis=0)
            y_train = np.concatenate(all_labels, axis=0)

            logger.info(f"ğŸ“Š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ ØªØ±Ú©ÛŒØ¨ Ø´Ø¯: {X_train.shape}")

            # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³
            self.model = SparseTechnicalNetwork(self.config)

            # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
            training_results = self._train_model(X_train, y_train, epochs)

            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡
            self._save_model(symbols[0], training_results, raw_data_quality)

            logger.info("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
            return training_results

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„: {e}")
            return {}

    def _calculate_sequence_quality(self, sequences: np.ndarray) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…"""
        try:
            if sequences.size == 0:
                return 0.0
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø±
            invalid_count = np.isnan(sequences).sum() + np.isinf(sequences).sum()
            validity_score = 1.0 - (invalid_count / sequences.size)
            
            # Ø¨Ø±Ø±Ø³ÛŒ ØªÙ†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            variance_score = min(np.var(sequences) / 10, 1.0)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ø±ÛŒØ§Ù†Ø³
            
            # Ù†Ù…Ø±Ù‡ Ú©ÛŒÙÛŒØª ØªØ±Ú©ÛŒØ¨ÛŒ
            quality_score = (validity_score * 0.7) + (variance_score * 0.3)
            return round(quality_score, 3)
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ø¯Ù†Ø¨Ø§Ù„Ù‡: {e}")
            return 0.0

    def _train_model(self, X: np.ndarray, y: np.ndarray, epochs: int) -> Dict:
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù‡"""
        # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ validation
        split_idx = int(0.8 * len(X))
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]

        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±
        X_train_tensor = torch.FloatTensor(X_train)
        X_val_tensor = torch.FloatTensor(X_val)
        y_train_tensor = torch.LongTensor(y_train)
        y_val_tensor = torch.LongTensor(y_val)

        # Ø§ÛŒØ¬Ø§Ø¯ DataLoader
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´
        optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss()

        # Ø¢Ù…ÙˆØ²Ø´
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience = 5
        patience_counter = 0
        best_model_state = None

        for epoch in range(epochs):
            # ÙØ§Ø² Ø¢Ù…ÙˆØ²Ø´
            self.model.train()
            epoch_loss = 0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs['trend_strength'], batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                epoch_loss += loss.item()

            # ÙØ§Ø² Validation
            self.model.eval()
            with torch.no_grad():
                val_outputs = self.model(X_val_tensor)
                val_loss = criterion(val_outputs['trend_strength'], y_val_tensor)

            train_losses.append(epoch_loss / len(train_loader))
            val_losses.append(val_loss.item())

            # Early Stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    logger.info(f"â¹ï¸ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø± Ø¯ÙˆØ±Ù‡ {epoch}")
                    break

            if epoch % 5 == 0:
                logger.info(f"ğŸ“ˆ Ø¯ÙˆØ±Ù‡ {epoch}: Train_Loss={train_losses[-1]:.4f}, Val_Loss={val_losses[-1]:.4f}")

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
        if best_model_state:
            self.model.load_state_dict(best_model_state)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ù†Ù‡Ø§ÛŒÛŒ
        with torch.no_grad():
            val_outputs = self.model(X_val_tensor)
            _, predicted = torch.max(val_outputs['trend_strength'], 1)
            accuracy = (predicted == y_val_tensor).float().mean().item()

        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'final_accuracy': accuracy,
            'best_val_loss': best_val_loss,
            'epochs_trained': epoch + 1,
            'training_samples': len(X_train),
            'validation_samples': len(X_val)
        }

    def _save_model(self, symbol: str, results: Dict, raw_data_quality: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø¨Ø§ metadata Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…"""
        import os
        os.makedirs("./models", exist_ok=True)

        model_path = f"./models/sparse_technical_{symbol}.pth"

        torch.save({
            'model_state_dict': self.model.state_dict(),
            'training_results': results,
            'config': self.config.__dict__,
            'trained_at': datetime.now().isoformat(),
            'model_type': 'sparse_technical_analyzer',
            'raw_data_quality': raw_data_quality,
            'data_sources': ['CoinStats', 'WebSocket', 'Historical'],
            'input_features': self.config.input_features,
            'sequence_length': self.config.sequence_length
        }, model_path)

        logger.info(f"ğŸ’¾ Ù…Ø¯Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {model_path}")

    def load_model(self, model_path: str):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡"""
        checkpoint = torch.load(model_path)

        self.config = SparseConfig(**checkpoint['config'])
        self.model = SparseTechnicalNetwork(self.config)
        self.model.load_state_dict(checkpoint['model_state_dict'])

        # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
        raw_data_quality = checkpoint.get('raw_data_quality', {})
        if raw_data_quality:
            logger.info("ğŸ“Š Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ù…Ø¯Ù„:")
            for symbol, quality in raw_data_quality.items():
                logger.info(f"  {symbol}: {quality['sequences_count']} Ù†Ù…ÙˆÙ†Ù‡ - Ú©ÛŒÙÛŒØª: {quality['quality_score']}")

        logger.info(f"âœ… Ù…Ø¯Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {model_path}")

    def evaluate_model(self, test_symbols: List[str]) -> Dict[str, Any]:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ ØªØ³Øª"""
        try:
            evaluation_results = {}
            
            for symbol in test_symbols:
                # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
                sequences, labels = technical_engine.prepare_training_data(symbol)
                
                if sequences is None or len(sequences) == 0:
                    logger.warning(f"âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª Ø¨Ø±Ø§ÛŒ {symbol} Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
                    continue
                
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ù†Ø³ÙˆØ±
                X_test = torch.FloatTensor(sequences)
                y_test = torch.LongTensor(labels)
                
                # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
                self.model.eval()
                with torch.no_grad():
                    outputs = self.model(X_test)
                    _, predicted = torch.max(outputs['trend_strength'], 1)
                    accuracy = (predicted == y_test).float().mean().item()
                    
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§ÛŒØ± Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
                    confidence_scores = torch.softmax(outputs['trend_strength'], dim=1).max(dim=1).values
                    avg_confidence = confidence_scores.mean().item()
                
                evaluation_results[symbol] = {
                    'accuracy': accuracy,
                    'avg_confidence': avg_confidence,
                    'test_samples': len(sequences),
                    'prediction_distribution': torch.bincount(predicted).tolist()
                }
                
                logger.info(f"ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ {symbol}: Ø¯Ù‚Øª={accuracy:.3f}, Ø§Ø·Ù…ÛŒÙ†Ø§Ù†={avg_confidence:.3f}")
            
            return evaluation_results
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„: {e}")
            return {}

    def get_training_status(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø¢Ù…ÙˆØ²Ø´"""
        status = {
            'model_loaded': self.model is not None,
            'config': self.config.__dict__ if hasattr(self, 'config') else {},
            'last_training': self.training_history[-1] if self.training_history else None,
            'raw_data_mode': True,
            'available_features': self.config.input_features,
            'sequence_length': self.config.sequence_length
        }
        
        if self.model is not None:
            status['model_parameters'] = sum(p.numel() for p in self.model.parameters())
            status['model_specialties'] = self.config.specialty_groups
            
        return status

    def fine_tune_model(self, new_symbols: List[str], fine_tune_epochs: int = 10) -> Dict:
        """Fine-tuning Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯"""
        try:
            if self.model is None:
                logger.error("âŒ Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Fine-tuning Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡")
                return {}
            
            logger.info(f"ğŸ”§ Fine-tuning Ù…Ø¯Ù„ Ø±ÙˆÛŒ {len(new_symbols)} Ù†Ù…Ø§Ø¯ Ø¬Ø¯ÛŒØ¯")
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            new_sequences = []
            new_labels = []
            
            for symbol in new_symbols:
                sequences, labels = technical_engine.prepare_training_data(symbol)
                if sequences is not None:
                    new_sequences.append(sequences)
                    new_labels.append(labels)
                    logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} Ø¨Ø±Ø§ÛŒ Fine-tuning Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
            
            if not new_sequences:
                logger.error("âŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Fine-tuning ÛŒØ§ÙØª Ù†Ø´Ø¯")
                return {}
            
            # ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            X_new = np.concatenate(new_sequences, axis=0)
            y_new = np.concatenate(new_labels, axis=0)
            
            # Fine-tuning Ø¨Ø§ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
            optimizer = optim.AdamW(self.model.parameters(), lr=0.0001, weight_decay=0.001)
            criterion = nn.CrossEntropyLoss()
            
            X_tensor = torch.FloatTensor(X_new)
            y_tensor = torch.LongTensor(y_new)
            
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(dataset, batch_size=16, shuffle=True)
            
            fine_tune_losses = []
            
            for epoch in range(fine_tune_epochs):
                self.model.train()
                epoch_loss = 0
                
                for batch_X, batch_y in dataloader:
                    optimizer.zero_grad()
                    outputs = self.model(batch_X)
                    loss = criterion(outputs['trend_strength'], batch_y)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
                    optimizer.step()
                    epoch_loss += loss.item()
                
                fine_tune_losses.append(epoch_loss / len(dataloader))
                
                if epoch % 2 == 0:
                    logger.info(f"ğŸ”§ Fine-tuning Ø¯ÙˆØ±Ù‡ {epoch}: Loss={fine_tune_losses[-1]:.4f}")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Fine-tuned Ø´Ø¯Ù‡
            self._save_model("fine_tuned", {
                'fine_tune_losses': fine_tune_losses,
                'fine_tune_epochs': fine_tune_epochs,
                'fine_tune_symbols': new_symbols
            }, {})
            
            return {
                'fine_tune_losses': fine_tune_losses,
                'final_loss': fine_tune_losses[-1],
                'trained_symbols': new_symbols,
                'total_samples': len(X_new)
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Fine-tuning Ù…Ø¯Ù„: {e}")
            return {}

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ù„ÙˆØ¨Ø§Ù„
model_trainer = SparseModelTrainer()
