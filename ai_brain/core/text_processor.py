import re
import numpy as np
from typing import List, Dict, Any, Tuple
import logging
from collections import Counter
import string

logger = logging.getLogger(__name__)

class TextProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù…ØªÙ† Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.vocab = {}
        self.reverse_vocab = {}
        self.vocab_size = 0
        self.max_vocab_size = config.get('max_vocab_size', 2000)
        
        # Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù Ù¾Ø§ÛŒÙ‡ (ÙØ§Ø±Ø³ÛŒ + Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
        self.stop_words = self._initialize_stop_words()
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ intent Ù¾Ø§ÛŒÙ‡
        self.intent_patterns = self._initialize_intent_patterns()
        
        logger.info("ðŸš€ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ù…ØªÙ† Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def _initialize_stop_words(self) -> set:
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù"""
        persian_stop_words = {
            'Ø¯Ø±', 'Ø¨Ø§', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø±Ø§', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ùˆ', 'Ø¨Ø±Ø§ÛŒ',
            'ØªØ§', 'Ø§Ø³Øª', 'Ø¨ÙˆØ¯', 'Ø´Ø¯', 'Ù‡Ø§ÛŒ', 'ØªØ±ÛŒÙ†', 'ØªØ±', 'Ù…ÛŒØ´ÙˆØ¯', 'Ø´ÙˆØ¯',
            'Ù‡Ø§ÛŒØ´', 'Ø§Ù†Ø¯', 'Ú©Ø±Ø¯', 'Ú©Ø±Ø¯Ù†', 'Ú©Ù†ÛŒØ¯', 'Ú¯ÛŒØ±ÛŒ', 'Ú¯ÛŒØ±ÛŒÛŒ', 'Ù‡Ø§'
        }
        
        english_stop_words = {
            'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'by', 'is', 'are', 'was', 'were', 'and', 'or', 'but', 'not'
        }
        
        return persian_stop_words.union(english_stop_words)
    
    def _initialize_intent_patterns(self) -> Dict[str, List[str]]:
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù†ÛŒØ§Øª Ù¾Ø§ÛŒÙ‡"""
        return {
            'health_check': [
                r'Ø³Ù„Ø§Ù…Øª', r'ÙˆØ¶Ø¹ÛŒØª', r'status', r'health', r'Ú†Ø·ÙˆØ±Ù‡', r'Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ù‡',
                r'Ø³ÛŒØ³ØªÙ…', r'system'
            ],
            'price_request': [
                r'Ù‚ÛŒÙ…Øª', r'Ù†Ø±Ø®', r'price', r'value', r'cost', r'Ú†Ù†Ø¯Ù‡',
                r'Ú†Ù‚Ø¯Ø±Ù‡', r'Ø¨ÛŒØªÚ©ÙˆÛŒÙ†', r'Ø§ØªØ±ÛŒÙˆÙ…', r'bitcoin', r'ethereum'
            ],
            'news_request': [
                r'Ø§Ø®Ø¨Ø§Ø±', r'Ø®Ø¨Ø±', r'news', r'ØªØ§Ø²Ù‡', r'Ø¬Ø¯ÛŒØ¯', r'latest',
                r'Ø¢Ù¾Ø¯ÛŒØª', r'update'
            ],
            'list_request': [
                r'Ù„ÛŒØ³Øª', r'list', r'Ù†Ù…Ø§ÛŒØ´', r'show', r'Ù‡Ù…Ù‡', r'all',
                r'Ø§Ø±Ø²Ù‡Ø§', r'coins', r'Ù†Ù…Ø§Ø¯Ù‡Ø§'
            ],
            'cache_status': [
                r'Ú©Ø´', r'cache', r'Ø­Ø§ÙØ¸Ù‡', r'memory', r'Ø°Ø®ÛŒØ±Ù‡', r'storage'
            ],
            'fear_greed': [
                r'ØªØ±Ø³', r'Ø·Ù…Ø¹', r'fear', r'greed', r'Ø´Ø§Ø®Øµ', r'index',
                r'Ø§Ø­Ø³Ø§Ø³Ø§Øª', r'sentiment'
            ]
        }
    
    def preprocess_text(self, text: str) -> List[str]:
        """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±"""
        if not text or not isinstance(text, str):
            return []
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
        text = self._normalize_text(text)
        
        # ØªØ¬Ø²ÛŒÙ‡ Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª
        words = self._tokenize(text)
        
        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù Ùˆ Ú©ÙˆØªØ§Ù‡
        filtered_words = [
            word for word in words 
            if (word not in self.stop_words and 
                len(word) > 1 and 
                not word.isdigit())
        ]
        
        logger.debug(f"ðŸ”¤ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†: '{text}' â†’ {len(filtered_words)} ØªÙˆÚ©Ù†")
        return filtered_words
    
    def _normalize_text(self, text: str) -> str:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†"""
        # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
        text = re.sub(r'[!ØŸ?ØŒ,;Ø›]', ' ', text)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
        text = re.sub(r'\s+', ' ', text)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© (Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
        text = text.lower()
        
        return text.strip()
    
    def _tokenize(self, text: str) -> List[str]:
        """ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ"""
        # Ø§Ù„Ú¯ÙˆÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
        tokens = re.findall(r'[a-zA-Z]+|[\u0600-\u06FF]+|[0-9]+', text)
        return tokens
    
    def text_to_vector(self, tokens: List[str], vector_size: int = 1000) -> np.ndarray:
        """ØªØ¨Ø¯ÛŒÙ„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ"""
        vector = np.zeros(vector_size)
        
        if not tokens:
            return vector
        
        # Ø§ÛŒØ¬Ø§Ø¯/Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø§ÛŒØ±Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†
        self._update_vocab(tokens)
        
        # ØªÙˆØ²ÛŒØ¹ ÛŒÚ©Ù†ÙˆØ§Ø®Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ¶Ø§ÛŒ Ø¨Ø±Ø¯Ø§Ø±
        for token in tokens:
            # Ù‡Ø´ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ ÛŒÚ©Ù†ÙˆØ§Ø®Øª
            hash_val = hash(token) % vector_size
            vector[hash_val] += 1
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        if np.sum(vector) > 0:
            vector = vector / np.sum(vector)
        
        return vector
    
    def _update_vocab(self, tokens: List[str]):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø§ÛŒØ±Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†"""
        for token in tokens:
            if token not in self.vocab and self.vocab_size < self.max_vocab_size:
                self.vocab[token] = self.vocab_size
                self.reverse_vocab[self.vocab_size] = token
                self.vocab_size += 1
    
    def detect_intent(self, text: str) -> Tuple[str, float]:
        """ØªØ´Ø®ÛŒØµ Ù†ÛŒØ§Øª Ø§Ø² Ù…ØªÙ†"""
        tokens = self.preprocess_text(text)
        text_lower = text.lower()
        
        intent_scores = {}
        
        for intent, patterns in self.intent_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    score += 1
            
            # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
            for token in tokens:
                if any(pattern in token for pattern in patterns if len(pattern) > 2):
                    score += 0.5
            
            if score > 0:
                intent_scores[intent] = score
        
        if intent_scores:
            best_intent = max(intent_scores.items(), key=lambda x: x[1])
            confidence = min(best_intent[1] / 5.0, 1.0)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ 0-1
            return best_intent[0], confidence
        
        return 'unknown', 0.0
    
    def extract_parameters(self, text: str, intent: str) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø§Ø² Ù…ØªÙ†"""
        params = {}
        tokens = self.preprocess_text(text)
        text_lower = text.lower()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¹Ø¯Ø§Ø¯
        numbers = re.findall(r'\d+', text)
        if numbers:
            params['limit'] = int(numbers[0])
        
        # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        if any(word in text_lower for word in ['Ù‚ÛŒÙ…Øª', 'price', 'Ù†Ø±Ø®']):
            params['sort_by'] = 'price'
        elif any(word in text_lower for word in ['Ø­Ø¬Ù…', 'volume']):
            params['sort_by'] = 'volume'
        elif any(word in text_lower for word in ['Ø§Ø±Ø²Ø´', 'market', 'Ù…Ø§Ø±Ú©Øª']):
            params['sort_by'] = 'marketCap'
        
        # ØªØ´Ø®ÛŒØµ Ø¬Ù‡Øª Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        if any(word in text_lower for word in ['Ù†Ø²ÙˆÙ„ÛŒ', 'desc', 'Ú©Ù…']):
            params['sort_dir'] = 'desc'
        else:
            params['sort_dir'] = 'asc'
        
        logger.debug(f"ðŸŽ¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {params}")
        return params
    
    def estimate_complexity(self, text: str) -> int:
        """ØªØ®Ù…ÛŒÙ† Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¸Ø±ÙÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´"""
        tokens = self.preprocess_text(text)
        
        # Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯
        unique_tokens = len(set(tokens))
        
        # Ø¬Ø±ÛŒÙ…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ
        length_penalty = max(0, len(tokens) - 10) * 0.5
        
        complexity = unique_tokens + length_penalty
        
        logger.debug(f"ðŸ“Š Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù…ØªÙ†: {complexity} (ØªÙˆÚ©Ù†â€ŒÙ‡Ø§: {len(tokens)})")
        return int(complexity)
    
    def get_processor_stats(self) -> Dict[str, Any]:
        """Ø¢Ù…Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±"""
        return {
            'vocab_size': self.vocab_size,
            'max_vocab_size': self.max_vocab_size,
            'known_intents': len(self.intent_patterns),
            'stop_words_count': len(self.stop_words)
        }
