import re
import time
import logging
from typing import Dict, List, Any, Set, Tuple
from collections import Counter, defaultdict
import heapq

logger = logging.getLogger(__name__)

class KnowledgeCompressor:
    """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø² Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ù†Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ¶Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.compression_threshold = config.get('compression_threshold', 0.8)
        self.min_importance_to_keep = config.get('min_importance_to_keep', 0.3)
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.concept_patterns = defaultdict(int)
        self.redundant_data_cache = set()
        
        # Ø¢Ù…Ø§Ø± ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.compression_stats = {
            'total_compressions': 0,
            'space_saved_mb': 0.0,
            'last_compression_time': 0
        }
        
        logger.info("ğŸš€ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø² Ø¯Ø§Ù†Ø´ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def compress_knowledge(self, knowledge_data: Dict[str, Any]) -> Dict[str, Any]:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ø´"""
        if not knowledge_data:
            return {}
        
        original_size = self._calculate_data_size(knowledge_data)
        
        # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡
        compressed_data = {}
        
        for key, value in knowledge_data.items():
            if isinstance(value, str):
                compressed_data[key] = self._compress_text(value)
            elif isinstance(value, dict):
                compressed_data[key] = self._compress_dict(value)
            elif isinstance(value, list):
                compressed_data[key] = self._compress_list(value)
            else:
                compressed_data[key] = value
        
        # Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù… Ø§Ù‡Ù…ÛŒØª
        compressed_data = self._remove_low_importance_data(compressed_data)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
        compressed_size = self._calculate_data_size(compressed_data)
        space_saved = original_size - compressed_size
        
        if space_saved > 0:
            self.compression_stats['total_compressions'] += 1
            self.compression_stats['space_saved_mb'] += space_saved / (1024 * 1024)
            self.compression_stats['last_compression_time'] = time.time()
            
            logger.info(f"ğŸ“¦ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ù†Ø´: {original_size/1024:.1f}KB â†’ {compressed_size/1024:.1f}KB")
        
        return compressed_data
    
    def _compress_text(self, text: str) -> str:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†"""
        if len(text) < 100:
            return text
        
        # Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        text = re.sub(r'\s+', ' ', text.strip())
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        text = self._replace_patterns(text)
        
        # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ
        if len(text) > 500:
            sentences = text.split('.')
            if len(sentences) > 3:
                # Ø­ÙØ¸ Û³ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ùˆ Ø¢Ø®Ø±
                compressed = '.'.join(sentences[:2] + ['...'] + sentences[-2:])
                return compressed
        
        return text
    
    def _compress_dict(self, data_dict: Dict[str, Any]) -> Dict[str, Any]:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ"""
        compressed = {}
        
        for key, value in data_dict.items():
            # Ø­ÙØ¸ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…Ù‡Ù…
            if self._is_important_key(key):
                compressed[key] = value
            elif isinstance(value, (str, dict, list)):
                # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÚ†ÛŒØ¯Ù‡
                compressed_val = self.compress_knowledge({key: value}).get(key, value)
                if self._should_keep_data(key, compressed_val):
                    compressed[key] = compressed_val
            else:
                # Ø­ÙØ¸ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø³Ø§Ø¯Ù‡
                compressed[key] = value
        
        return compressed
    
    def _compress_list(self, data_list: List[Any]) -> List[Any]:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù„ÛŒØ³Øª"""
        if not data_list:
            return []
        
        # Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©ØŒ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ø²Ù… Ù†ÛŒØ³Øª
        if len(data_list) <= 10:
            return data_list
        
        # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù„ÛŒØ³Øª
        compressed_list = []
        for item in data_list:
            if isinstance(item, (dict, list)):
                compressed_item = self.compress_knowledge({'item': item}).get('item', item)
                compressed_list.append(compressed_item)
            else:
                compressed_list.append(item)
        
        # Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù„ÛŒØ³Øª Ø¨Ø²Ø±Ú¯ Ø§Ø³ØªØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡
        if len(compressed_list) > 20:
            # Ø­ÙØ¸ Ø§ÙˆÙ„ÛŒÙ†ØŒ Ø¢Ø®Ø±ÛŒÙ† Ùˆ Ú†Ù†Ø¯ Ø¢ÛŒØªÙ… Ù…ÛŒØ§Ù†ÛŒ
            sampled_list = (
                compressed_list[:5] + 
                [f"...({len(compressed_list)-10} Ù…ÙˆØ§Ø±Ø¯)"] + 
                compressed_list[-5:]
            )
            return sampled_list
        
        return compressed_list
    
    def _replace_patterns(self, text: str) -> str:
        """Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø± Ù…ØªÙ†"""
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ Ø¯Ø± Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§Ø±Ø¨Ø±
        patterns = {
            r'Ù‚ÛŒÙ…Øª\s+(Ø¨ÛŒØªÚ©ÙˆÛŒÙ†|Ø§ØªØ±ÛŒÙˆÙ…|bitcoin|ethereum)': 'Ù‚ÛŒÙ…Øª_Ø§Ø±Ø²',
            r'Ù„ÛŒØ³Øª\s+(\d+)\s+Ø§Ø±Ø²': 'Ù„ÛŒØ³Øª_Ø§Ø±Ø²',
            r'ÙˆØ¶Ø¹ÛŒØª\s+Ø³ÛŒØ³ØªÙ…': 'Ø³Ù„Ø§Ù…Øª_Ø³ÛŒØ³ØªÙ…',
            r'Ø§Ø®Ø¨Ø§Ø±\s+(Ø¬Ø¯ÛŒØ¯|ØªØ§Ø²Ù‡|Ø¢Ø®Ø±ÛŒÙ†)': 'Ø§Ø®Ø¨Ø§Ø±_Ø¬Ø¯ÛŒØ¯',
        }
        
        for pattern, replacement in patterns.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _is_important_key(self, key: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù‡Ù…ÛŒØª Ú©Ù„ÛŒØ¯"""
        important_keys = {
            'intent', 'concept', 'pattern', 'essential', 'core', 'mastery',
            'timestamp', 'type', 'user_id', 'success', 'confidence'
        }
        
        return key in important_keys or any(imp in key.lower() for imp in important_keys)
    
    def _should_keep_data(self, key: str, value: Any) -> bool:
        """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ ÛŒØ§ Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡"""
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª Ø¯Ø§Ø¯Ù‡
        importance_score = self._calculate_importance(key, value)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ
        return importance_score >= self.min_importance_to_keep
    
    def _calculate_importance(self, key: str, value: Any) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª Ø¯Ø§Ø¯Ù‡"""
        importance = 0.0
        
        # Ø§Ù‡Ù…ÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ú©Ù„ÛŒØ¯
        if self._is_important_key(key):
            importance += 0.5
        
        # Ø§Ù‡Ù…ÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù…Ù‚Ø¯Ø§Ø±
        if isinstance(value, str):
            if len(value) > 50:  # Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø§Ù‡Ù…ÛŒØª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯
                importance += 0.2
        elif isinstance(value, (int, float)):
            importance += 0.1  # Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ù‡Ù…ÛŒØª Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯
        elif isinstance(value, (dict, list)):
            if len(str(value)) > 100:  # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
                importance += 0.3
        
        # Ø§Ù‡Ù…ÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±Ú©Ø§Ù†Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
        if hasattr(value, 'get') and callable(getattr(value, 'get')):
            access_count = value.get('access_count', 0)
            importance += min(access_count * 0.1, 0.5)
        
        return min(importance, 1.0)
    
    def _remove_low_importance_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù… Ø§Ù‡Ù…ÛŒØª"""
        important_data = {}
        
        for key, value in data.items():
            if self._should_keep_data(key, value):
                important_data[key] = value
            else:
                logger.debug(f"ğŸ—‘ï¸ Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡ Ú©Ù… Ø§Ù‡Ù…ÛŒØª: {key}")
        
        removed_count = len(data) - len(important_data)
        if removed_count > 0:
            logger.info(f"ğŸ§¹ Ø­Ø°Ù {removed_count} Ø¯Ø§Ø¯Ù‡ Ú©Ù… Ø§Ù‡Ù…ÛŒØª")
        
        return important_data
    
    def extract_core_concepts(self, knowledge_data: Dict[str, Any]) -> Set[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ§Ù‡ÛŒÙ… Ø§ØµÙ„ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ø´"""
        concepts = set()
        
        for key, value in knowledge_data.items():
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ú©Ù„ÛŒØ¯Ù‡Ø§
            key_concepts = self._extract_concepts_from_text(str(key))
            concepts.update(key_concepts)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ØªÙ†ÛŒ
            if isinstance(value, str):
                value_concepts = self._extract_concepts_from_text(value)
                concepts.update(value_concepts)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ ØªÙˆ Ø¯Ø± ØªÙˆ
            elif isinstance(value, dict):
                nested_concepts = self.extract_core_concepts(value)
                concepts.update(nested_concepts)
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, dict):
                        nested_concepts = self.extract_core_concepts(item)
                        concepts.update(nested_concepts)
        
        return concepts
    
    def _extract_concepts_from_text(self, text: str) -> Set[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ§Ù‡ÛŒÙ… Ø§Ø² Ù…ØªÙ†"""
        if not isinstance(text, str):
            return set()
        
        # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()
        
        # ÙÛŒÙ„ØªØ± Ú©Ù„Ù…Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ø¹Ù…ÙˆÙ…ÛŒ
        concepts = set()
        for word in words:
            if (len(word) >= 3 and 
                not word.isdigit() and 
                word not in self._get_common_words()):
                concepts.add(word)
        
        return concepts
    
    def _get_common_words(self) -> Set[str]:
        """Ú©Ù„Ù…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ú©Ù… Ø§Ù‡Ù…ÛŒØª"""
        return {
            'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'is', 'are', 'was', 'were', 'and', 'or', 'but', 'not', 'this',
            'that', 'these', 'those', 'have', 'has', 'had', 'do', 'does',
            'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might'
        }
    
    def _calculate_data_size(self, data: Any) -> int:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¨Ø§ÛŒØª"""
        if data is None:
            return 0
        
        import sys
        return sys.getsizeof(str(data))
    
    def optimize_memory_layout(self, memory_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† Ø­Ø§ÙØ¸Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±"""
        if not memory_data:
            return {}
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±Ú©Ø§Ù†Ø³ Ø¯Ø³ØªØ±Ø³ÛŒ (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
        sorted_data = {}
        
        for key, value in memory_data.items():
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¯Ø³ØªØ±Ø³ÛŒ
            access_score = value.get('access_count', 0)
            importance_score = value.get('importance', 0.1)
            total_score = access_score + (importance_score * 10)
            
            sorted_data[key] = (total_score, value)
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø²
        sorted_items = sorted(sorted_data.items(), key=lambda x: x[1][0], reverse=True)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
        optimized_data = {}
        for key, (score, value) in sorted_items:
            optimized_data[key] = value
        
        logger.debug(f"ğŸ”§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†ÛŒØ¯Ù…Ø§Ù† {len(optimized_data)} Ø¢ÛŒØªÙ…")
        return optimized_data
    
    def get_compression_stats(self) -> Dict[str, Any]:
        """Ø¢Ù…Ø§Ø± ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        return {
            'total_compressions': self.compression_stats['total_compressions'],
            'total_space_saved_mb': round(self.compression_stats['space_saved_mb'], 2),
            'last_compression_time': self.compression_stats['last_compression_time'],
            'compression_threshold': self.compression_threshold,
            'min_importance_to_keep': self.min_importance_to_keep
        }
