"""
Ù…Ø¯ÛŒØ±ÛŒØª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
"""

import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class DataCompressor:
    """Ù…Ø¯ÛŒØ±ÛŒØª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
    
    def __init__(self, cache_dir: str = "./github_db_data"):
        self.cache_dir = Path(cache_dir)
        
    def compress_old_files(self, days_threshold: int = 7) -> int:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        compressed_count = 0
        live_data_dir = self.cache_dir / "live_data"
        
        if not live_data_dir.exists():
            return 0
        
        for json_file in live_data_dir.glob("*.json"):
            try:
                # Ø¨Ø±Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® ÙØ§ÛŒÙ„
                file_time = datetime.fromtimestamp(json_file.stat().st_mtime)
                if datetime.now() - file_time > timedelta(days=days_threshold):
                    
                    # Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡
                    compressed_file = self.cache_dir / "compressed_data" / f"{json_file.stem}.json.gz"
                    
                    with gzip.open(compressed_file, 'wt', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False)
                    
                    # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ
                    json_file.unlink()
                    compressed_count += 1
                    
                    logger.info(f"ğŸ“¦ ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡ Ø´Ø¯: {json_file.name}")
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ {json_file}: {e}")
        
        return compressed_count
    
    def get_compression_stats(self) -> Dict[str, Any]:
        """Ø¢Ù…Ø§Ø± ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        compressed_dir = self.cache_dir / "compressed_data"
        
        if not compressed_dir.exists():
            return {"compressed_files": 0, "total_size_mb": 0}
        
        compressed_files = list(compressed_dir.glob("*.gz"))
        total_size = sum(f.stat().st_size for f in compressed_files)
        
        return {
            "compressed_files": len(compressed_files),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "last_compression": datetime.now().isoformat()
        }
