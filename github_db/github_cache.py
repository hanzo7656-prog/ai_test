"""
Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± GitHub Ø¨Ø±Ø§ÛŒ VortexAI
Ù…Ø¯ÛŒØ±ÛŒØª Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒØŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ùˆ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
"""

import os
import json
import gzip
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import logging
from pathlib import Path
import shutil

logger = logging.getLogger(__name__)

class GitHubDBCache:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ GitHub"""
    
    def __init__(self, repo_path: str = "./github_db_data"):
        self.repo_path = Path(repo_path)
        self.setup_directories()
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.compression_threshold_days = 7  # Ø¨Ø¹Ø¯ Ø§Ø² 7 Ø±ÙˆØ² ÙØ´Ø±Ø¯Ù‡ Ø´ÙˆØ¯
        self.cleanup_threshold_days = 30     # Ø¨Ø¹Ø¯ Ø§Ø² 30 Ø±ÙˆØ² Ù¾Ø§Ú© Ø´ÙˆØ¯
        
    def setup_directories(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§"""
        directories = [
            "live_data",
            "compressed_data", 
            "metadata",
            "batch_progress",
            "symbols_list"
        ]
        
        for dir_name in directories:
            dir_path = self.repo_path / dir_name
            dir_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {dir_path}")
    
    def save_live_data(self, symbol: str, data: Dict[str, Any]) -> bool:
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù†Ø¯Ù‡"""
        try:
            symbol_file = self.repo_path / "live_data" / f"{symbol.lower()}.json"
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªØ§Ø¯ÛŒØªØ§
            enriched_data = {
                "symbol": symbol,
                "last_updated": datetime.now().isoformat(),
                "data": data,
                "version": "1.0"
            }
            
            with open(symbol_file, 'w', encoding='utf-8') as f:
                json.dump(enriched_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ Ø¯Ø§Ø¯Ù‡ Ø²Ù†Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {symbol}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ {symbol}: {e}")
            return False
    
    def get_live_data(self, symbol: str) -> Optional[Dict[str, Any]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù†Ø¯Ù‡"""
        try:
            symbol_file = self.repo_path / "live_data" / f"{symbol.lower()}.json"
            
            if not symbol_file.exists():
                return None
            
            with open(symbol_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Ø¨Ø±Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø§Ù†Ù‚Ø¶Ø§
            last_updated = datetime.fromisoformat(data['last_updated'])
            if datetime.now() - last_updated > timedelta(minutes=10):
                logger.warning(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ {symbol} Ù‚Ø¯ÛŒÙ…ÛŒ Ø§Ø³Øª")
                return None
            
            return data['data']
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡ {symbol}: {e}")
            return None
    
    def compress_old_data(self, symbol: str) -> bool:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        try:
            symbol_file = self.repo_path / "live_data" / f"{symbol.lower()}.json"
            compressed_dir = self.repo_path / "compressed_data"
            
            if not symbol_file.exists():
                return False
            
            # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡ ÙØ¹Ù„ÛŒ
            with open(symbol_file, 'r', encoding='utf-8') as f:
                current_data = json.load(f)
            
            last_updated = datetime.fromisoformat(current_data['last_updated'])
            
            # Ø¨Ø±Ø±Ø³ÛŒæ˜¯å¦éœ€è¦ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
            if datetime.now() - last_updated < timedelta(days=self.compression_threshold_days):
                return False
            
            # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            compressed_file = compressed_dir / f"{symbol.lower()}_{timestamp}.json.gz"
            
            with gzip.open(compressed_file, 'wt', encoding='utf-8') as f:
                json.dump(current_data, f, ensure_ascii=False)
            
            # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ
            symbol_file.unlink()
            
            logger.info(f"ğŸ“¦ Ø¯Ø§Ø¯Ù‡ {symbol} ÙØ´Ø±Ø¯Ù‡ Ø´Ø¯: {compressed_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ {symbol}: {e}")
            return False
    
    def save_batch_progress(self, batch_id: str, progress: Dict[str, Any]) -> bool:
        """Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ´Ø±ÙØª Ø§Ø³Ú©Ù† Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ"""
        try:
            progress_file = self.repo_path / "batch_progress" / f"{batch_id}.json"
            
            progress_data = {
                "batch_id": batch_id,
                "last_updated": datetime.now().isoformat(),
                "progress": progress
            }
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, indent=2, ensure_ascii=False)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ´Ø±ÙØª {batch_id}: {e}")
            return False
    
    def get_batch_progress(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ´Ø±ÙØª Ø§Ø³Ú©Ù† Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ"""
        try:
            progress_file = self.repo_path / "batch_progress" / f"{batch_id}.json"
            
            if not progress_file.exists():
                return None
            
            with open(progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ù¾ÛŒØ´Ø±ÙØª {batch_id}: {e}")
            return None
    
    def save_symbols_list(self, symbols: List[str], list_name: str = "top_500") -> bool:
        """Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª Ø§Ø±Ø²Ù‡Ø§"""
        try:
            symbols_file = self.repo_path / "symbols_list" / f"{list_name}.json"
            
            symbols_data = {
                "name": list_name,
                "count": len(symbols),
                "last_updated": datetime.now().isoformat(),
                "symbols": symbols
            }
            
            with open(symbols_file, 'w', encoding='utf-8') as f:
                json.dump(symbols_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“‹ Ù„ÛŒØ³Øª Ø§Ø±Ø²Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {list_name} ({len(symbols)} Ø§Ø±Ø²)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª Ø§Ø±Ø²Ù‡Ø§: {e}")
            return False
    
    def get_symbols_list(self, list_name: str = "top_500") -> Optional[List[str]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ø§Ø±Ø²Ù‡Ø§"""
        try:
            symbols_file = self.repo_path / "symbols_list" / f"{list_name}.json"
            
            if not symbols_file.exists():
                return None
            
            with open(symbols_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('symbols', [])
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ù„ÛŒØ³Øª Ø§Ø±Ø²Ù‡Ø§: {e}")
            return None
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Ø¢Ù…Ø§Ø± Ú©Ø´"""
        try:
            live_data_dir = self.repo_path / "live_data"
            compressed_dir = self.repo_path / "compressed_data"
            
            live_files = list(live_data_dir.glob("*.json"))
            compressed_files = list(compressed_dir.glob("*.gz"))
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¬Ù…
            live_size = sum(f.stat().st_size for f in live_files)
            compressed_size = sum(f.stat().st_size for f in compressed_files)
            
            return {
                "live_files_count": len(live_files),
                "live_size_mb": round(live_size / (1024 * 1024), 2),
                "compressed_files_count": len(compressed_files),
                "compressed_size_mb": round(compressed_size / (1024 * 1024), 2),
                "total_size_mb": round((live_size + compressed_size) / (1024 * 1024), 2),
                "last_updated": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ú©Ø´: {e}")
            return {}
    
    def cleanup_old_data(self) -> int:
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù‚Ø¯ÛŒÙ…ÛŒ"""
        try:
            compressed_dir = self.repo_path / "compressed_data"
            deleted_count = 0
            
            for compressed_file in compressed_dir.glob("*.gz"):
                file_time = datetime.fromtimestamp(compressed_file.stat().st_mtime)
                
                if datetime.now() - file_time > timedelta(days=self.cleanup_threshold_days):
                    compressed_file.unlink()
                    deleted_count += 1
                    logger.info(f"ğŸ—‘ï¸ ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ Ø­Ø°Ù Ø´Ø¯: {compressed_file.name}")
            
            return deleted_count
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ: {e}")
            return 0
