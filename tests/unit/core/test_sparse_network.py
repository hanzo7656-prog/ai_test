# ğŸ“ tests/unit/core/test_sparse_network.py

import pytest
import torch
import numpy as np
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))

from src.core.spiking_transformer.spiking_attention import SpikingSelfAttention
from src.core.spiking_transformer.spiking_ffn import SpikingFeedForward
from src.core.spiking_transformer.transformer_block import SpikingTransformerBlock
from src.core.optimization.sparse_optimizer import SparseOptimizer

class TestSparseNetwork:
    """ØªØ³Øª Ø´Ø¨Ú©Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³ Ùˆ Spiking Transformer"""
    
    def setup_method(self):
        self.d_model = 64
        self.n_heads = 4
        self.seq_len = 10
        self.batch_size = 2
        
        self.attention = SpikingSelfAttention(
            d_model=self.d_model,
            n_heads=self.n_heads,
            seq_len=self.seq_len
        )
        
        self.ffn = SpikingFeedForward(d_model=self.d_model, d_ff=256)
        self.transformer_block = SpikingTransformerBlock(
            d_model=self.d_model,
            n_heads=self.n_heads,
            seq_len=self.seq_len
        )
        
        self.sparse_optimizer = SparseOptimizer(sparsity_level=0.9)
    
    def test_spiking_attention_forward(self):
        """ØªØ³Øª forward pass attention Ø§Ø³Ù¾Ø§ÛŒÚ©ÛŒÙ†Ú¯"""
        x = torch.randn(self.batch_size, self.seq_len, self.d_model)
        
        output = self.attention(x)
        
        assert output.shape == (self.batch_size, self.seq_len, self.d_model)
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
    
    def test_spiking_ffn_forward(self):
        """ØªØ³Øª forward pass FFN Ø§Ø³Ù¾Ø§ÛŒÚ©ÛŒÙ†Ú¯"""
        x = torch.randn(self.batch_size, self.seq_len, self.d_model)
        
        output = self.ffn(x)
        
        assert output.shape == (self.batch_size, self.seq_len, self.d_model)
        assert not torch.isnan(output).any()
    
    def test_transformer_block_forward(self):
        """ØªØ³Øª forward pass Ø¨Ù„ÙˆÚ© Transformer Ú©Ø§Ù…Ù„"""
        x = torch.randn(self.batch_size, self.seq_len, self.d_model)
        
        output = self.transformer_block(x)
        
        assert output.shape == (self.batch_size, self.seq_len, self.d_model)
        assert not torch.isnan(output).any()
    
    def test_spike_generation(self):
        """ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ Ø§Ø³Ù¾Ø§ÛŒÚ©â€ŒÙ‡Ø§"""
        x = torch.randn(self.batch_size, self.seq_len, self.d_model)
        
        # Ø§Ø¬Ø±Ø§ÛŒ forward Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø§Ø³Ù¾Ø§ÛŒÚ©
        self.attention(x)
        spike_stats = self.attention.get_spike_statistics()
        
        assert 'total_spikes' in spike_stats
        assert 'spike_rates' in spike_stats
        assert spike_stats['total_spikes'] >= 0
    
    def test_membrane_dynamics(self):
        """ØªØ³Øª Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© membrane"""
        # ØªØ³Øª reset states
        self.attention.reset_states()
        
        # membrane Ø¨Ø§ÛŒØ¯ ØµÙØ± Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
        assert torch.all(self.attention.membrane_q == 0)
        assert torch.all(self.attention.membrane_k == 0)
        assert torch.all(self.attention.membrane_v == 0)
    
    def test_sparse_optimization(self):
        """ØªØ³Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
        # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
        class SimpleModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(10, 5)
        
        model = SimpleModel()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³ÛŒØªÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        sparsity_before = self.sparse_optimizer.calculate_sparsity(model)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø§Ø³Ù¾Ø§Ø±Ø³ÛŒØªÛŒ
        self.sparse_optimizer.apply_sparsity(model)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø³Ù¾Ø§Ø±Ø³ÛŒØªÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        sparsity_after = self.sparse_optimizer.calculate_sparsity(model)
        
        assert sparsity_after['sparsity_percentage'] >= sparsity_before['sparsity_percentage']
        assert sparsity_after['zero_parameters'] >= sparsity_before['zero_parameters']
    
    def test_memory_efficiency(self):
        """ØªØ³Øª Ú©Ø§Ø±Ø§ÛŒÛŒ Ø­Ø§ÙØ¸Ù‡"""
        import psutil
        process = psutil.Process()
        
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ú†Ù†Ø¯ÛŒÙ† inference
        x = torch.randn(4, 10, 64)
        for _ in range(100):
            _ = self.transformer_block(x)
        
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§ÛŒØ¯ Ù…Ø¹Ù‚ÙˆÙ„ Ø¨Ø§Ø´Ø¯
        memory_increase = memory_after - memory_before
        assert memory_increase < 100  # Ú©Ù…ØªØ± Ø§Ø² 100MB
    
    def test_gradient_flow(self):
        """ØªØ³Øª Ø¬Ø±ÛŒØ§Ù† Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†"""
        x = torch.randn(2, 10, 64, requires_grad=True)
        output = self.transformer_block(x)
        
        # Ø§ÛŒØ¬Ø§Ø¯ loss Ù…ØµÙ†ÙˆØ¹ÛŒ
        loss = output.sum()
        loss.backward()
        
        # Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯
        assert x.grad is not None
        assert not torch.isnan(x.grad).any()

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
