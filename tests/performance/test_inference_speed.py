# ğŸ“ tests/performance/test_inference_speed.py

import pytest
import time
import numpy as np
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from src.core.spiking_transformer.transformer_block import SpikingTransformerBlock

class TestInferenceSpeed:
    """ØªØ³Øª Ø³Ø±Ø¹Øª inference"""
    
    def setup_method(self):
        self.transformer = SpikingTransformerBlock(
            d_model=64,
            n_heads=4,
            seq_len=10
        )
    
    def test_transformer_inference_speed(self):
        """ØªØ³Øª Ø³Ø±Ø¹Øª inference ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù…Ø±"""
        # Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        batch_size = 2
        seq_len = 10
        d_model = 64
        
        test_input = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)
        
        # Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø²Ù…Ø§Ù† inference
        start_time = time.time()
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ú†Ù†Ø¯ÛŒÙ† Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ
        for _ in range(100):
            output = self.transformer.forward(test_input)
        
        end_time = time.time()
        avg_inference_time = (end_time - start_time) / 100
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† inference Ø¨Ø§ÛŒØ¯ Ú©Ù…ØªØ± Ø§Ø² 10ms Ø¨Ø§Ø´Ø¯
        assert avg_inference_time < 0.01  # 10ms
    
    def test_batch_processing_speed(self):
        """ØªØ³Øª Ø³Ø±Ø¹Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ batch"""
        batch_sizes = [1, 2, 4, 8]
        seq_len = 10
        d_model = 64
        
        for batch_size in batch_sizes:
            test_input = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)
            
            start_time = time.time()
            output = self.transformer.forward(test_input)
            inference_time = time.time() - start_time
            
            # Ø²Ù…Ø§Ù† inference Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ batch size Ø¨Ù‡ ØµÙˆØ±Øª Ø®Ø·ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§Ø¨Ø¯
            if batch_size > 1:
                assert inference_time < batch_size * 0.005  # 5ms per batch
    
    def test_memory_efficiency_during_inference(self):
        """ØªØ³Øª Ú©Ø§Ø±Ø§ÛŒÛŒ Ø­Ø§ÙØ¸Ù‡ Ø¯Ø± Ø­ÛŒÙ† inference"""
        import psutil
        process = psutil.Process()
        
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # Ø§Ø¬Ø±Ø§ÛŒ inference
        test_input = np.random.randn(4, 10, 64).astype(np.float32)
        for _ in range(50):
            output = self.transformer.forward(test_input)
        
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        
        # Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø´Ø¯Øª Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§Ø¨Ø¯
        memory_increase = memory_after - memory_before
        assert memory_increase < 50  # Ú©Ù…ØªØ± Ø§Ø² 50MB Ø§ÙØ²Ø§ÛŒØ´
    
    def test_spike_processing_efficiency(self):
        """ØªØ³Øª Ú©Ø§Ø±Ø§ÛŒÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Ù¾Ø§ÛŒÚ©"""
        spike_stats = self.transformer.get_spike_statistics()
        
        assert 'total_spikes' in spike_stats
        assert 'spike_rates' in spike_stats
        
        # Ù†Ø±Ø® Ø§Ø³Ù¾Ø§ÛŒÚ© Ø¨Ø§ÛŒØ¯ Ù…Ø¹Ù‚ÙˆÙ„ Ø¨Ø§Ø´Ø¯
        for rate in spike_stats['spike_rates'].values():
            assert 0 <= rate <= 1

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
