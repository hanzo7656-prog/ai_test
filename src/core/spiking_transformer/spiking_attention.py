# ðŸ“ src/core/spiking_transformer/spiking_attention.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple
import math

class SpikingSelfAttention(nn.Module):
    """Spiking Self-Attention Ø¨Ø§ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ LIF"""
    
    def __init__(self, d_model: int = 64, n_heads: int = 4, seq_len: int = 10, 
                 spike_threshold: float = 1.0, decay: float = 0.9):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.seq_len = seq_len
        self.head_dim = d_model // n_heads
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ø³Ù¾Ø§ÛŒÚ©ÛŒÙ†Ú¯
        self.spike_threshold = spike_threshold
        self.decay = decay
        
        # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Q, K, V
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        
        # membrane potentials
        self.register_buffer('membrane_q', torch.zeros(1, seq_len, d_model))
        self.register_buffer('membrane_k', torch.zeros(1, seq_len, d_model))
        self.register_buffer('membrane_v', torch.zeros(1, seq_len, d_model))
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø§Ø³Ù¾Ø§ÛŒÚ©â€ŒÙ‡Ø§
        self.spike_counts = {
            'q_spikes': 0, 'k_spikes': 0, 'v_spikes': 0
        }
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, d_model = x.shape
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Q, K, V
        Q = self.w_q(x)
        K = self.w_k(x)
        V = self.w_v(x)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø§Ø³Ù¾Ø§ÛŒÚ©ÛŒÙ†Ú¯
        Q_spikes = self._lif_neurons(Q, 'q')
        K_spikes = self._lif_neurons(K, 'k') 
        V_spikes = self._lif_neurons(V, 'v')
        
        # ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¨Ø±Ø§ÛŒ multi-head attention
        Q_spikes = Q_spikes.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        K_spikes = K_spikes.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        V_spikes = V_spikes.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ attention scores
        attn_scores = torch.matmul(Q_spikes, K_spikes.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn_weights = F.softmax(attn_scores, dim=-1)
        
        # Ø§Ø¹Ù…Ø§Ù„ attention Ø¨Ù‡ values
        attn_output = torch.matmul(attn_weights, V_spikes)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        output = self.w_o(attn_output)
        
        return output
    
    def _lif_neurons(self, x: torch.Tensor, neuron_type: str) -> torch.Tensor:
        """Leaky Integrate-and-Fire neurons"""
        # Ø§Ù†ØªØ®Ø§Ø¨ membrane Ù…Ù†Ø§Ø³Ø¨
        if neuron_type == 'q':
            membrane = self.membrane_q
        elif neuron_type == 'k':
            membrane = self.membrane_k
        else:  # 'v'
            membrane = self.membrane_v
        
        # update membrane potential
        membrane = self.decay * membrane + x
        
        # generate spikes
        spikes = (membrane >= self.spike_threshold).float()
        
        # reset membrane
        membrane = membrane * (1 - spikes)
        
        # update buffer
        if neuron_type == 'q':
            self.membrane_q = membrane
            self.spike_counts['q_spikes'] += spikes.sum().item()
        elif neuron_type == 'k':
            self.membrane_k = membrane
            self.spike_counts['k_spikes'] += spikes.sum().item()
        else:
            self.membrane_v = membrane
            self.spike_counts['v_spikes'] += spikes.sum().item()
        
        return spikes
    
    def reset_states(self):
        """Ø±ÛŒØ³Øª Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§"""
        self.membrane_q.zero_()
        self.membrane_k.zero_()
        self.membrane_v.zero_()
        self.spike_counts = {'q_spikes': 0, 'k_spikes': 0, 'v_spikes': 0}
    
    def get_spike_statistics(self) -> dict:
        """Ø¢Ù…Ø§Ø± Ø§Ø³Ù¾Ø§ÛŒÚ©â€ŒÙ‡Ø§"""
        total_spikes = sum(self.spike_counts.values())
        return {
            'total_spikes': total_spikes,
            'spike_rates': {k: v / (self.seq_len * self.d_model) 
                           for k, v in self.spike_counts.items()},
            **self.spike_counts
        }
