# ðŸ“ src/core/optimization/memory_optimizer.py

import gc
import psutil
import logging
from typing import Dict, List
import torch

logger = logging.getLogger(__name__)

class MemoryOptimizer:
    """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ø§Ø±Ø³"""
    
    def __init__(self, max_memory_mb: int = 512):
        self.max_memory_mb = max_memory_mb
        self.memory_history: List[Dict] = []
    
    def optimize_model_memory(self, model):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ Ù…Ø¯Ù„"""
        try:
            # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
            if hasattr(model, 'parameters'):
                for param in model.parameters():
                    if param.data.is_sparse:
                        param.data = param.data.coalesce()
            
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø²Ø¨Ø§Ù„Ù‡
            gc.collect()
            
            logger.info("âœ… Model memory optimized")
            
        except Exception as e:
            logger.warning(f"Memory optimization failed: {e}")
    
    def get_memory_usage(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø­Ø§ÙØ¸Ù‡"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': process.memory_percent(),
            'available_mb': psutil.virtual_memory().available / 1024 / 1024
        }
    
    def should_cleanup(self) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡"""
        memory_usage = self.get_memory_usage()
        return memory_usage['rss_mb'] > self.max_memory_mb * 0.8  # 80% Ø§Ø² Ø­Ø¯Ø§Ú©Ø«Ø±
