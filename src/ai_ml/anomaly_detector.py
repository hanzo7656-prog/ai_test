# ðŸ“ src/ai_ml/anomaly_detector.py

import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Tuple
import logging

logger = logging.getLogger(__name__)

class AnomalyDetector:
    """ØªØ´Ø®ÛŒØµâ€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±"""
    
    def __init__(self, contamination: float = 0.1):
        self.model = IsolationForest(
            contamination=contamination,
            random_state=42
        )
        self.scaler = StandardScaler()
        self.is_trained = False
    
    def extract_features(self, data: pd.DataFrame) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ"""
        features = []
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ
        features.append(data['close'].pct_change().rolling(5).std())  # Ù†ÙˆØ³Ø§Ù† Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª
        features.append(data['close'].pct_change().rolling(20).std()) # Ù†ÙˆØ³Ø§Ù† Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø­Ø¬Ù…
        if 'volume' in data.columns:
            volume_anomaly = (data['volume'] - data['volume'].rolling(20).mean()) / data['volume'].rolling(20).std()
            features.append(volume_anomaly)
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù‚ÛŒÙ…Øª
        price_range = (data['high'] - data['low']) / data['close']
        features.append(price_range)
        
        # ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        feature_matrix = np.column_stack([f.dropna() for f in features])
        return feature_matrix
    
    def train(self, data: pd.DataFrame) -> Dict:
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ"""
        try:
            features = self.extract_features(data)
            
            if len(features) < 50:
                logger.warning("Insufficient data for anomaly detection training")
                return {}
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
            features_scaled = self.scaler.fit_transform(features)
            
            # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
            self.model.fit(features_scaled)
            self.is_trained = True
            
            # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´
            predictions = self.model.predict(features_scaled)
            anomaly_count = (predictions == -1).sum()
            
            logger.info(f"âœ… Anomaly detector trained. Found {anomaly_count} anomalies in training data")
            
            return {
                'training_samples': len(features),
                'anomalies_detected': anomaly_count,
                'anomaly_rate': anomaly_count / len(features)
            }
            
        except Exception as e:
            logger.error(f"Anomaly detection training failed: {e}")
            return {}
    
    def detect_anomalies(self, data: pd.DataFrame) -> Dict:
        """ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯"""
        if not self.is_trained:
            logger.warning("Anomaly detector not trained")
            return {}
        
        try:
            features = self.extract_features(data)
            
            if len(features) == 0:
                return {}
            
            features_scaled = self.scaler.transform(features)
            predictions = self.model.predict(features_scaled)
            anomaly_scores = self.model.decision_function(features_scaled)
            
            # ÛŒØ§ÙØªÙ† Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§
            anomaly_indices = np.where(predictions == -1)[0]
            anomalies = []
            
            for idx in anomaly_indices:
                anomalies.append({
                    'timestamp': data.index[idx] if hasattr(data, 'index') else idx,
                    'score': float(anomaly_scores[idx]),
                    'features': features[idx].tolist()
                })
            
            return {
                'total_anomalies': len(anomalies),
                'anomaly_rate': len(anomalies) / len(features),
                'anomalies': anomalies,
                'average_anomaly_score': np.mean(anomaly_scores[anomaly_indices]) if len(anomaly_indices) > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"Anomaly detection failed: {e}")
            return {}
