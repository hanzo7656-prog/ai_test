# ğŸ“ src/ai_ml/pattern_predictor.py

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.preprocessing import MinMaxScaler
import os

class LSTMPatternPredictor(nn.Module):
    """Ø´Ø¨Ú©Ù‡ LSTM Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ"""
    
    def __init__(self, input_size: int = 5, hidden_size: int = 50, 
                 num_layers: int = 2, output_size: int = 3):
        super(LSTMPatternPredictor, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 25),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(25, output_size),
            nn.Softmax(dim=1)
        )
    
    def forward(self, x):
        # LSTM forward
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ù„Ø§ÛŒÙ‡
        last_time_step = lstm_out[:, -1, :]
        
        # Ù„Ø§ÛŒÙ‡ fully connected
        output = self.fc(last_time_step)
        
        return output

class PatternPredictor:
    """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ Ø¨Ø§ LSTM"""
    
    def __init__(self, sequence_length: int = 20, model_path: str = "models/pattern_predictor.pth"):
        self.sequence_length = sequence_length
        self.model_path = model_path
        self.scaler = MinMaxScaler()
        self.model = None
        self.is_trained = False
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        self.patterns = {
            0: 'UPTREND_CONTINUATION',   # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ
            1: 'DOWNTREND_CONTINUATION', # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ
            2: 'REVERSAL'                # Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯
        }
    
    def prepare_sequences(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´"""
        features = self._extract_features(data)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        features_scaled = self.scaler.fit_transform(features)
        
        X, y = [], []
        
        for i in range(self.sequence_length, len(features_scaled)):
            # Ø¯Ù†Ø¨Ø§Ù„Ù‡ ÙˆØ±ÙˆØ¯ÛŒ
            X.append(features_scaled[i-self.sequence_length:i])
            
            # Ù„ÛŒØ¨Ù„ Ø®Ø±ÙˆØ¬ÛŒ (ØªØºÛŒÛŒØ± Ù‚ÛŒÙ…Øª Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡)
            future_return = (
                data['close'].iloc[i+5] / data['close'].iloc[i] - 1 
                if i + 5 < len(data) else 0
            )
            
            if future_return > 0.02:
                y.append(0)  # UPTREND_CONTINUATION
            elif future_return < -0.02:
                y.append(1)  # DOWNTREND_CONTINUATION
            else:
                y.append(2)  # REVERSAL
        
        return np.array(X), np.array(y)
    
    def _extract_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ù‚ÛŒÙ…Øª"""
        features = pd.DataFrame(index=data.index)
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ
        features['open'] = data['open']
        features['high'] = data['high']
        features['low'] = data['low']
        features['close'] = data['close']
        
        if 'volume' in data.columns:
            features['volume'] = data['volume']
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
        features['returns'] = data['close'].pct_change()
        features['volatility'] = data['close'].pct_change().rolling(5).std()
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªØ­Ø±Ú©
        features['ma_5'] = data['close'].rolling(5).mean()
        features['ma_10'] = data['close'].rolling(10).mean()
        features['ma_20'] = data['close'].rolling(20).mean()
        
        # RSI Ø³Ø§Ø¯Ù‡
        delta = data['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        features['rsi'] = 100 - (100 / (1 + rs))
        
        return features.dropna()
    
    def train(self, data: pd.DataFrame, epochs: int = 100) -> Dict:
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LSTM"""
        print("ğŸ”„ Training Pattern Predictor LSTM...")
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
        X, y = self.prepare_sequences(data)
        
        if len(X) < 100:
            print("âŒ Insufficient sequences for training")
            return {}
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡
        split_idx = int(0.8 * len(X))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªÙ†Ø³ÙˆØ±
        X_train_tensor = torch.FloatTensor(X_train)
        X_test_tensor = torch.FloatTensor(X_test)
        y_train_tensor = torch.LongTensor(y_train)
        y_test_tensor = torch.LongTensor(y_test)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
        input_size = X_train.shape[2]
        self.model = LSTMPatternPredictor(
            input_size=input_size,
            hidden_size=50,
            num_layers=2,
            output_size=3
        )
        
        # Ø¢Ù…ÙˆØ²Ø´
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        
        train_losses = []
        test_accuracies = []
        
        for epoch in range(epochs):
            # Ø­Ø§Ù„Øª Ø¢Ù…ÙˆØ²Ø´
            self.model.train()
            
            # Forward pass
            outputs = self.model(X_train_tensor)
            loss = criterion(outputs, y_train_tensor)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            self.model.eval()
            with torch.no_grad():
                test_outputs = self.model(X_test_tensor)
                _, predicted = torch.max(test_outputs, 1)
                test_accuracy = (predicted == y_test_tensor).float().mean()
            
            train_losses.append(loss.item())
            test_accuracies.append(test_accuracy.item())
            
            if epoch % 20 == 0:
                print(f"   Epoch {epoch}: Loss = {loss.item():.4f}, Test Acc = {test_accuracy.item():.4f}")
        
        self.is_trained = True
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
        self._save_model()
        
        final_accuracy = test_accuracies[-1]
        print(f"âœ… LSTM Training Completed - Final Accuracy: {final_accuracy:.4f}")
        
        return {
            'final_accuracy': final_accuracy,
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'input_features': input_size
        }
    
    def predict_pattern(self, data: pd.DataFrame) -> Dict:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù„Ú¯ÙˆÛŒ Ø¢ÛŒÙ†Ø¯Ù‡"""
        if not self.is_trained or self.model is None:
            if not self._load_model():
                return {'pattern': 'UNKNOWN', 'confidence': 0.0}
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø¯Ù†Ø¨Ø§Ù„Ù‡
        features = self._extract_features(data)
        if len(features) < self.sequence_length:
            return {'pattern': 'UNKNOWN', 'confidence': 0.0}
        
        latest_sequence = features.iloc[-self.sequence_length:]
        latest_sequence_scaled = self.scaler.transform(latest_sequence)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªÙ†Ø³ÙˆØ±
        sequence_tensor = torch.FloatTensor(latest_sequence_scaled).unsqueeze(0)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        self.model.eval()
        with torch.no_grad():
            output = self.model(sequence_tensor)
            probabilities = output[0].numpy()
            predicted_class = np.argmax(probabilities)
        
        confidence = probabilities[predicted_class]
        pattern = self.patterns.get(predicted_class, 'UNKNOWN')
        
        return {
            'pattern': pattern,
            'confidence': confidence,
            'all_probabilities': {
                self.patterns[i]: prob for i, prob in enumerate(probabilities)
            },
            'time_horizon': '5_periods'
        }
    
    def _save_model(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡"""
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'scaler': self.scaler,
            'sequence_length': self.sequence_length
        }, self.model_path)
    
    def _load_model(self) -> bool:
        """Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        try:
            if os.path.exists(self.model_path):
                checkpoint = torch.load(self.model_path)
                
                input_size = 5  # ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
                self.model = LSTMPatternPredictor(input_size=input_size)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.scaler = checkpoint['scaler']
                self.sequence_length = checkpoint['sequence_length']
                self.is_trained = True
                
                print("âœ… LSTM Model loaded successfully")
                return True
        except Exception as e:
            print(f"âŒ Error loading LSTM model: {e}")
        
        return False
