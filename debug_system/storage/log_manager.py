import logging
import json
import gzip
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import threading
from collections import deque
import asyncio

logger = logging.getLogger(__name__)

class LogManager:
    def __init__(self, log_dir: str = "./logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # Ø¨Ø§ÙØ± Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Real-Time
        self.log_buffer = deque(maxlen=10000)
        self._buffer_lock = threading.Lock()
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª rotation
        self.max_file_size = 10 * 1024 * 1024  # 10MB
        self.retention_days = 30
        
        # Ø´Ø±ÙˆØ¹ background task Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ´ØªÙ† Ù„Ø§Ú¯â€ŒÙ‡Ø§
        self._start_log_writer()
        
    def _start_log_writer(self):
        """Ø´Ø±ÙˆØ¹ background task Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ´ØªÙ† Ù„Ø§Ú¯â€ŒÙ‡Ø§"""
        def log_writer_loop():
            while True:
                try:
                    self._flush_buffer_to_disk()
                    threading.Event().wait(10)  # Ù‡Ø± Û±Û° Ø«Ø§Ù†ÛŒÙ‡
                except Exception as e:
                    logger.error(f"âŒ Log writer error: {e}")
                    threading.Event().wait(30)
        
        writer_thread = threading.Thread(target=log_writer_loop, daemon=True)
        writer_thread.start()
        logger.info("âœ… Log writer started")
    
    def log_endpoint_call(self, endpoint_data: Dict[str, Any]):
        """Ø«Ø¨Øª Ù„Ø§Ú¯ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª"""
        log_entry = {
            'type': 'endpoint_call',
            'timestamp': datetime.now().isoformat(),
            'data': endpoint_data
        }
        
        self._add_to_buffer(log_entry)
    
    def log_system_metrics(self, metrics_data: Dict[str, Any]):
        """Ø«Ø¨Øª Ù„Ø§Ú¯ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
        log_entry = {
            'type': 'system_metrics',
            'timestamp': datetime.now().isoformat(), 
            'data': metrics_data
        }
        
        self._add_to_buffer(log_entry)
    
    def log_security_event(self, security_data: Dict[str, Any]):
        """Ø«Ø¨Øª Ù„Ø§Ú¯ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø§Ù…Ù†ÛŒØªÛŒ"""
        log_entry = {
            'type': 'security_event',
            'timestamp': datetime.now().isoformat(),
            'data': security_data
        }
        
        self._add_to_buffer(log_entry)
    
    def log_performance_alert(self, alert_data: Dict[str, Any]):
        """Ø«Ø¨Øª Ù„Ø§Ú¯ Ù‡Ø´Ø¯Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        log_entry = {
            'type': 'performance_alert',
            'timestamp': datetime.now().isoformat(),
            'data': alert_data
        }
        
        self._add_to_buffer(log_entry)
    
    def _add_to_buffer(self, log_entry: Dict[str, Any]):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„Ø§Ú¯ Ø¨Ù‡ Ø¨Ø§ÙØ±"""
        with self._buffer_lock:
            self.log_buffer.append(log_entry)
    
    def _flush_buffer_to_disk(self):
        """Ù†ÙˆØ´ØªÙ† Ø¨Ø§ÙØ± Ø¨Ù‡ Ø¯ÛŒØ³Ú©"""
        if not self.log_buffer:
            return
            
        with self._buffer_lock:
            logs_to_write = list(self.log_buffer)
            self.log_buffer.clear()
        
        if not logs_to_write:
            return
        
        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ùˆ ØªØ§Ø±ÛŒØ®
        grouped_logs = {}
        for log in logs_to_write:
            log_date = datetime.fromisoformat(log['timestamp']).strftime('%Y-%m-%d')
            log_type = log['type']
            key = f"{log_date}_{log_type}"
            
            if key not in grouped_logs:
                grouped_logs[key] = []
            grouped_logs[key].append(log)
        
        # Ù†ÙˆØ´ØªÙ† Ù‡Ø± Ú¯Ø±ÙˆÙ‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ù…Ø±Ø¨ÙˆØ·Ù‡
        for key, logs in grouped_logs.items():
            filename = self.log_dir / f"{key}.log"
            self._write_logs_to_file(filename, logs)
    
    def _write_logs_to_file(self, filename: Path, logs: List[Dict]):
        """Ù†ÙˆØ´ØªÙ† Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ§ÛŒÙ„"""
        try:
            with open(filename, 'a', encoding='utf-8') as f:
                for log in logs:
                    f.write(json.dumps(log, ensure_ascii=False) + '\n')
        except Exception as e:
            logger.error(f"âŒ Error writing logs to {filename}: {e}")
    
    def get_logs(self, 
                 log_type: str = None,
                 start_date: datetime = None,
                 end_date: datetime = None,
                 limit: int = 1000) -> List[Dict[str, Any]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ø§ ÙÛŒÙ„ØªØ±"""
        if start_date is None:
            start_date = datetime.now() - timedelta(days=1)
        if end_date is None:
            end_date = datetime.now()
        
        logs = []
        current_date = start_date
        
        while current_date <= end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            
            if log_type:
                # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ¹ Ø®Ø§Øµ
                filename = self.log_dir / f"{date_str}_{log_type}.log"
                if filename.exists():
                    logs.extend(self._read_log_file(filename, limit))
            else:
                # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø§Ù†ÙˆØ§Ø¹
                for file in self.log_dir.glob(f"{date_str}_*.log"):
                    logs.extend(self._read_log_file(file, limit))
            
            current_date += timedelta(days=1)
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ timestamp
        logs.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        return logs[:limit]
    
    def _read_log_file(self, filename: Path, limit: int) -> List[Dict]:
        """Ø®ÙˆØ§Ù†Ø¯Ù† Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„"""
        logs = []
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            log_entry = json.loads(line.strip())
                            logs.append(log_entry)
                            if len(logs) >= limit:
                                break
                        except json.JSONDecodeError:
                            continue
        except FileNotFoundError:
            pass
        except Exception as e:
            logger.error(f"âŒ Error reading log file {filename}: {e}")
        
        return logs
    
    def compress_old_logs(self):
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for log_file in self.log_dir.glob("*.log"):
            file_date_str = log_file.stem.split('_')[0]
            try:
                file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
                if file_date < cutoff_date:
                    self._compress_file(log_file)
            except ValueError:
                continue
    
    def _compress_file(self, file_path: Path):
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÛŒÚ© ÙØ§ÛŒÙ„"""
        try:
            compressed_path = file_path.with_suffix('.log.gz')
            
            with open(file_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    f_out.writelines(f_in)
            
            # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ Ù¾Ø³ Ø§Ø² ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
            file_path.unlink()
            logger.info(f"âœ… Compressed log file: {file_path.name}")
            
        except Exception as e:
            logger.error(f"âŒ Error compressing {file_path}: {e}")
    
    def get_log_statistics(self, days: int = 7) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ù„Ø§Ú¯â€ŒÙ‡Ø§"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        stats = {
            'total_logs': 0,
            'by_type': defaultdict(int),
            'by_date': defaultdict(int),
            'largest_log_file': {'name': '', 'size_mb': 0}
        }
        
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            
            for log_file in self.log_dir.glob(f"{date_str}_*.log"):
                # Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„
                file_size_mb = log_file.stat().st_size / (1024 * 1024)
                
                if file_size_mb > stats['largest_log_file']['size_mb']:
                    stats['largest_log_file'] = {
                        'name': log_file.name,
                        'size_mb': round(file_size_mb, 2)
                    }
                
                # Ø´Ù…Ø§Ø±Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        log_count = sum(1 for _ in f)
                        stats['total_logs'] += log_count
                        
                        # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„
                        log_type = log_file.stem.split('_')[1]
                        stats['by_type'][log_type] += log_count
                        stats['by_date'][date_str] += log_count
                        
                except Exception as e:
                    logger.error(f"âŒ Error reading {log_file}: {e}")
            
            current_date += timedelta(days=1)
        
        return stats
    
    def cleanup_old_logs(self):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        cutoff_date = datetime.now() - timedelta(days=self.retention_days)
        
        for log_file in self.log_dir.glob("*.log*"):  # Ø´Ø§Ù…Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙØ´Ø±Ø¯Ù‡ Ù‡Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
            file_date_str = log_file.stem.split('_')[0]
            try:
                file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
                if file_date < cutoff_date:
                    log_file.unlink()
                    logger.info(f"ğŸ§¹ Deleted old log file: {log_file.name}")
            except ValueError:
                # Ø§Ú¯Ø± ÙØ±Ù…Øª ØªØ§Ø±ÛŒØ® Ø¯Ø±Ø³Øª Ù†Ø¨ÙˆØ¯ØŒ ÙØ§ÛŒÙ„ Ø±Ø§ Ø­Ø°Ù Ù†Ú©Ù†
                continue

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ù„ÙˆØ¨Ø§Ù„
log_manager = LogManager()
