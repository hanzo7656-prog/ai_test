import functools
import gzip
import json
import pickle
from datetime import datetime
from typing import Callable, Any, Dict, Optional
import asyncio
from .cache_debugger import cache_debugger

class SmartCache:
    """Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    def __init__(self):
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
        self.cache_strategies = {
            # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ - Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
            'processed': {
                'base_ttl': 600,      # 10 Ø¯Ù‚ÛŒÙ‚Ù‡
                'compress_threshold': 50000,
                'priority': 'high',
                'routes': ['coins', 'exchanges', 'news', 'insights']
            },
            # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… - Ø§ÙˆÙ„ÙˆÛŒØª Ù¾Ø§ÛŒÛŒÙ†
            'raw': {
                'base_ttl': 300,      # 5 Ø¯Ù‚ÛŒÙ‚Ù‡
                'compress_threshold': 50000,
                'priority': 'low', 
                'routes': ['raw_coins', 'raw_exchanges', 'raw_news', 'raw_insights']
            }
        }
        
        # Ø¢Ù…Ø§Ø± ÙØ´Ø±Ø¯Ù‡
        self.cache_stats = {
            'total_requests': 0,
            'hits': 0,
            'misses': 0,
            'compressions': 0,
            'errors': 0,
            'bytes_saved': 0,
            'performance': {
                'avg_response_time': 0,
                'last_cleanup': None,
                'health_score': 100
            }
        }
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡
        self.compression_enabled = True
        self.max_cache_size = 25 * 1024 * 1024  # 25MB
        self.cleanup_threshold = 0.7  # 70% Ù¾Ø± Ø´Ø¯Ù‡

    def compress_data(self, data: Any) -> tuple[bytes, bool]:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø¯Ù‡"""
        try:
            if not self.compression_enabled:
                return pickle.dumps(data), False
            
            serialized = pickle.dumps(data)
            if len(serialized) < 2000:  # Ø²ÛŒØ± 2KB ÙØ´Ø±Ø¯Ù‡ Ù†Ú©Ù†
                return serialized, False
            
            compressed = gzip.compress(serialized)
            if len(compressed) >= len(serialized) * 0.85:  # Ø§Ú¯Ø± Ú©Ù…ØªØ± Ø§Ø² 15% Ø¨Ù‡Ø¨ÙˆØ¯
                return serialized, False
            
            self.cache_stats['compressions'] += 1
            self.cache_stats['bytes_saved'] += (len(serialized) - len(compressed))
            return compressed, True
            
        except Exception as e:
            self._log_error(f"Ø®Ø·Ø§ÛŒ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
            return pickle.dumps(data), False

    def decompress_data(self, data: bytes, was_compressed: bool) -> Any:
        """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø§Ø¯Ù‡ ÙØ´Ø±Ø¯Ù‡"""
        try:
            if was_compressed:
                return pickle.loads(gzip.decompress(data))
            return pickle.loads(data)
        except Exception as e:
            self._log_error(f"Ø®Ø·Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ: {e}")
            return None

    def get_ttl(self, strategy: str, data_size: int = 0) -> int:
        """TTL Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØ²"""
        base_ttl = self.cache_strategies[strategy]['base_ttl']
        
        if data_size > 5000000:    # Ø¨Ø§Ù„Ø§ÛŒ 5MB
            return max(60, base_ttl // 3)
        elif data_size > 1000000:  # Ø¨Ø§Ù„Ø§ÛŒ 1MB  
            return max(120, base_ttl // 2)
        
        return base_ttl

    async def cleanup_if_needed(self):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
        try:
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØ² - Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ Ø§Ø² redis Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ
            current_size = 0  # await self.get_actual_cache_size()
            
            if current_size > self.max_cache_size * self.cleanup_threshold:
                self._log_info("ğŸ”¥ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ú©Ø´")
                # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ raw Ø§ÙˆÙ„
                self.cache_stats['performance']['last_cleanup'] = datetime.now().isoformat()
                
        except Exception as e:
            self._log_error(f"Ø®Ø·Ø§ÛŒ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ: {e}")

    def cache_strategy(self, strategy: str):
        """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ø§ØµÙ„ÛŒ"""
        
        def decorator(func: Callable) -> Callable:
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                cache_key = f"{strategy}:{func.__name__}"
                start_time = datetime.now()
                
                try:
                    # Ú†Ú© Ú©Ø´
                    cached_data = cache_debugger.get_data(cache_key)
                    
                    if cached_data is not None:
                        if isinstance(cached_data, tuple):
                            data, was_compressed = cached_data
                            result = self.decompress_data(data, was_compressed)
                        else:
                            result = cached_data
                            
                        if result is not None:
                            self._update_stats(True, start_time)
                            self._log_info(f"âœ… HIT: {strategy}.{func.__name__}")
                            return result
                    
                    self._update_stats(False, start_time)
                    self._log_info(f"ğŸ”„ MISS: {strategy}.{func.__name__}")
                    
                    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹
                    result = await func(*args, **kwargs)
                    
                    if result is not None:
                        compressed_data, was_compressed = self.compress_data(result)
                        data_size = len(compressed_data)
                        
                        if data_size < self.max_cache_size * 0.15:  # ÙÙ‚Ø· Ø§Ú¯Ø± Ú©Ù…ØªØ± Ø§Ø² 15% ÙØ¶Ø§Ø³Øª
                            expire = self.get_ttl(strategy, data_size)
                            cache_value = (compressed_data, was_compressed) if was_compressed else result
                            cache_debugger.set_data(cache_key, cache_value, expire)
                            
                            self._log_info(f"ğŸ’¾ SET: {strategy}.{func.__name__} ({expire}s)")
                            await self.cleanup_if_needed()
                    
                    return result
                    
                except Exception as e:
                    self._log_error(f"âŒ ERROR: {strategy}.{func.__name__} - {e}")
                    return await func(*args, **kwargs)
            
            return wrapper
        return decorator

    def _update_stats(self, hit: bool, start_time: datetime):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±"""
        self.cache_stats['total_requests'] += 1
        
        if hit:
            self.cache_stats['hits'] += 1
        else:
            self.cache_stats['misses'] += 1
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®
        response_time = (datetime.now() - start_time).total_seconds() * 1000
        current_avg = self.cache_stats['performance']['avg_response_time']
        requests = self.cache_stats['total_requests']
        
        # Moving average
        self.cache_stats['performance']['avg_response_time'] = (
            (current_avg * (requests - 1) + response_time) / requests
        )

    def _log_info(self, message: str):
        """Ù„Ø§Ú¯ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ"""
        print(f"â„¹ï¸ [Cache] {datetime.now().strftime('%H:%M:%S')} - {message}")

    def _log_error(self, message: str):
        """Ù„Ø§Ú¯ Ø®Ø·Ø§"""
        self.cache_stats['errors'] += 1
        print(f"âŒ [Cache] {datetime.now().strftime('%H:%M:%S')} - {message}")

    def get_health_status(self) -> Dict[str, Any]:
        """ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª ÙØ´Ø±Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØª Ù…Ø§Ø¯Ø±"""
        total = self.cache_stats['total_requests']
        hit_rate = (self.cache_stats['hits'] / total * 100) if total > 0 else 0
        error_rate = (self.cache_stats['errors'] / max(total, 1) * 100)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø³Ù„Ø§Ù…Øª
        health_score = max(0, 100 - (error_rate * 2) - ((100 - hit_rate) / 2))
        
        return {
            'status': 'healthy' if health_score > 80 else 'degraded' if health_score > 60 else 'unhealthy',
            'health_score': round(health_score, 1),
            'summary': {
                'hit_rate': round(hit_rate, 1),
                'total_requests': total,
                'avg_response_time': round(self.cache_stats['performance']['avg_response_time'], 2),
                'compression_savings': self.cache_stats['bytes_saved'],
                'strategies_active': len(self.cache_strategies)
            },
            'timestamp': datetime.now().isoformat(),
            'cache_size': '25MB',  # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© Ú©Ù†ÛŒ
            'compression': self.compression_enabled
        }

# Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ù„ÙˆØ¨Ø§Ù„
smart_cache = SmartCache()


coins_cache = smart_cache.cache_strategy("coins")
exchanges_cache = smart_cache.cache_strategy("exchanges")
news_cache = smart_cache.cache_strategy("news") 
insights_cache = smart_cache.cache_strategy("insights")

# Ø¨Ø±Ø§ÛŒ routes Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…
raw_coins_cache = smart_cache.cache_strategy("raw_coins")
raw_exchanges_cache = smart_cache.cache_strategy("raw_exchanges")
raw_news_cache = smart_cache.cache_strategy("raw_news")
raw_insights_cache = smart_cache.cache_strategy("raw_insights")

print("âœ… Smart Cache System initialized with all decorators")
