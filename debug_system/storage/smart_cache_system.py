"""
Cache Analytics & Optimization Engine
Ø¢Ù†Ø§Ù„ÛŒØ² Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ø´
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict, deque

logger = logging.getLogger(__name__)

class CacheOptimizationEngine:
    """Ù…ÙˆØªÙˆØ± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ø´"""
    
    def __init__(self):
        # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        from .cache_debugger import cache_debugger
        from .redis_manager import redis_manager
        
        self.debugger = cache_debugger
        self.redis_manager = redis_manager
        
        # Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØªÛŒÚ©Ø³ (MOTHER_B)
        self.analytics_db = "mother_b"
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ
        self.access_patterns = defaultdict(lambda: {
            'access_count': 0,
            'last_access': None,
            'access_times': deque(maxlen=100),
            'size_history': deque(maxlen=50),
            'hit_miss_ratio': 0
        })
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.optimization_suggestions = deque(maxlen=100)
        
        # Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡
        self.advanced_stats = {
            'peak_usage_times': defaultdict(int),
            'key_lifespan_analysis': defaultdict(list),
            'database_load_distribution': defaultdict(int),
            'compression_efficiency': 0,
            'cost_savings_estimate': 0
        }

    def analyze_access_patterns(self, hours: int = 24) -> Dict[str, Any]:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ú©Ø´"""
        try:
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø§Ø² cache_debugger
            recent_operations = [
                op for op in self.debugger.cache_operations
                if datetime.fromisoformat(op['timestamp']) >= datetime.now() - timedelta(hours=hours)
            ]
            
            analysis = {
                'period_hours': hours,
                'total_operations': len(recent_operations),
                'operations_by_hour': defaultdict(int),
                'hot_keys': [],
                'cold_keys': [],
                'access_trends': {},
                'recommendations': []
            }
            
            # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¹ØªÛŒ
            for op in recent_operations:
                hour = datetime.fromisoformat(op['timestamp']).hour
                analysis['operations_by_hour'][hour] += 1
            
            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø¯Ø§Øº Ùˆ Ø³Ø±Ø¯
            key_access_count = defaultdict(int)
            for op in recent_operations:
                key_access_count[op['key']] += 1
            
            sorted_keys = sorted(key_access_count.items(), key=lambda x: x[1], reverse=True)
            if sorted_keys:
                analysis['hot_keys'] = [{'key': k, 'access_count': v} for k, v in sorted_keys[:10]]
                analysis['cold_keys'] = [{'key': k, 'access_count': v} for k, v in sorted_keys[-10:]]
            
            # ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª
            self._generate_access_recommendations(analysis, recent_operations)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù†Ø§Ù„ÛŒØ²
            self._store_analytics('access_patterns', analysis)
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Error analyzing access patterns: {e}")
            return {'error': str(e)}

    def predict_optimal_ttl(self, key_pattern: str, database: str = "utb") -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ TTL Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ"""
        try:
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
            keys = self.redis_manager.get_keys(database, key_pattern)[0]
            
            ttl_analysis = {
                'pattern': key_pattern,
                'database': database,
                'sample_size': len(keys),
                'current_avg_ttl': 0,
                'recommended_ttl': 300,
                'confidence_score': 0,
                'key_analysis': []
            }
            
            total_ttl = 0
            analyzed_keys = 0
            
            for key in keys[:50]:  # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² 50 Ú©Ù„ÛŒØ¯ Ø§ÙˆÙ„
                try:
                    # Ø¨Ø±Ø±Ø³ÛŒ TTL ÙØ¹Ù„ÛŒ
                    ttl = self.redis_manager.get_client(database).ttl(key)
                    if ttl > 0:
                        total_ttl += ttl
                        analyzed_keys += 1
                        
                        # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø§ÛŒÙ† Ú©Ù„ÛŒØ¯
                        access_stats = self._get_key_access_stats(key)
                        ttl_analysis['key_analysis'].append({
                            'key': key,
                            'current_ttl': ttl,
                            'access_count': access_stats.get('access_count', 0),
                            'last_access': access_stats.get('last_access')
                        })
                except:
                    continue
            
            if analyzed_keys > 0:
                current_avg = total_ttl / analyzed_keys
                ttl_analysis['current_avg_ttl'] = current_avg
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ TTL Ø¨Ù‡ÛŒÙ†Ù‡
                recommended_ttl = self._calculate_optimal_ttl(ttl_analysis['key_analysis'])
                ttl_analysis['recommended_ttl'] = recommended_ttl
                ttl_analysis['confidence_score'] = min(100, analyzed_keys * 2)
            
            return ttl_analysis
            
        except Exception as e:
            logger.error(f"âŒ Error predicting optimal TTL: {e}")
            return {'error': str(e)}

    def database_health_check(self) -> Dict[str, Any]:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ùˆ ØªØ¹Ø§Ø¯Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§"""
        health_report = {
            'timestamp': datetime.now().isoformat(),
            'database_health': {},
            'load_balancing': {},
            'recommendations': [],
            'alerts': []
        }
        
        databases = ['uta', 'utb', 'utc', 'mother_a', 'mother_b']
        
        for db in databases:
            try:
                # Ø³Ù„Ø§Ù…Øª Ø§ØªØµØ§Ù„
                health = self.redis_manager.health_check(db)
                
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡
                usage = self.redis_manager.get_database_usage().get(db, {})
                
                health_report['database_health'][db] = {
                    'status': health.get('status', 'unknown'),
                    'memory_usage_percentage': usage.get('used_memory_percentage', 0),
                    'memory_used': usage.get('used_memory_human', 'N/A'),
                    'keys_count': usage.get('keys_count', 0),
                    'connected_clients': health.get('connected_clients', 0),
                    'ping_time_ms': health.get('ping_time_ms', 0)
                }
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
                if usage.get('used_memory_percentage', 0) > 80:
                    health_report['alerts'].append(f"ğŸ”´ {db}: Ø­Ø§ÙØ¸Ù‡ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ø¸Ø±ÙÛŒØª")
                
                if health.get('status') != 'connected':
                    health_report['alerts'].append(f"ğŸ”´ {db}: Ù…Ø´Ú©Ù„ Ø§ØªØµØ§Ù„")
                    
            except Exception as e:
                health_report['database_health'][db] = {'error': str(e)}
                health_report['alerts'].append(f"ğŸ”´ {db}: Ø®Ø·Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª")
        
        # ØªØ­Ù„ÛŒÙ„ ØªØ¹Ø§Ø¯Ù„ Ø¨Ø§Ø±
        self._analyze_load_balancing(health_report)
        
        return health_report

    def cost_optimization_report(self) -> Dict[str, Any]:
        """Ú¯Ø²Ø§Ø±Ø´ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§"""
        try:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ®Ù…ÛŒÙ†ÛŒ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Upstash)
            report = {
                'timestamp': datetime.now().isoformat(),
                'cost_estimation': {},
                'optimization_opportunities': [],
                'monthly_savings_estimate': 0
            }
            
            usage_data = self.redis_manager.get_database_usage()
            
            for db_name, usage in usage_data.items():
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡ ØªØ®Ù…ÛŒÙ†ÛŒ (ÙØ±Ù…ÙˆÙ„ Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡)
                memory_usage_mb = usage.get('used_memory_bytes', 0) / (1024 * 1024)
                estimated_cost = max(0.50, memory_usage_mb * 0.01)  # Ù…Ø¯Ù„ Ù‡Ø²ÛŒÙ†Ù‡ Ø³Ø§Ø¯Ù‡
                
                report['cost_estimation'][db_name] = {
                    'memory_usage_mb': round(memory_usage_mb, 2),
                    'estimated_monthly_cost': round(estimated_cost, 2),
                    'keys_count': usage.get('keys_count', 0),
                    'efficiency_score': self._calculate_efficiency_score(usage)
                }
            
            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
            self._identify_cost_savings(report)
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ Error generating cost report: {e}")
            return {'error': str(e)}

    def intelligent_cache_warming(self, key_patterns: List[str], databases: List[str] = None):
        """Ú¯Ø±Ù… Ú©Ø±Ø¯Ù† Ù‡ÙˆØ´Ù…Ù†Ø¯ Ú©Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡"""
        if databases is None:
            databases = ['utb', 'utc']  # Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        
        warming_report = {
            'timestamp': datetime.now().isoformat(),
            'warmed_keys': 0,
            'success_rate': 0,
            'performance_impact': 'low',
            'details': []
        }
        
        successful_warms = 0
        total_attempts = 0
        
        for db in databases:
            for pattern in key_patterns:
                try:
                    keys = self.redis_manager.get_keys(db, pattern)[0]
                    total_attempts += len(keys)
                    
                    for key in keys[:20]:  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overload
                        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ú©Ù„ÛŒØ¯ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ)
                        exists, _ = self.redis_manager.exists(db, key)
                        if exists:
                            successful_warms += 1
                            warming_report['details'].append({
                                'database': db,
                                'key': key,
                                'status': 'warmed'
                            })
                    
                except Exception as e:
                    warming_report['details'].append({
                        'database': db,
                        'pattern': pattern,
                        'status': 'error',
                        'error': str(e)
                    })
        
        if total_attempts > 0:
            warming_report['warmed_keys'] = successful_warms
            warming_report['success_rate'] = round((successful_warms / total_attempts) * 100, 2)
        
        return warming_report

    def _generate_access_recommendations(self, analysis: Dict, operations: List):
        """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ"""
        recommendations = []
        
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¹Ø§Øª Ù¾ÛŒÚ©
        peak_hours = sorted(analysis['operations_by_hour'].items(), key=lambda x: x[1], reverse=True)[:3]
        if peak_hours:
            recommendations.append(f"ğŸ•’ Ø³Ø§Ø¹Ø§Øª Ù¾ÛŒÚ© Ø¯Ø³ØªØ±Ø³ÛŒ: {[h[0] for h in peak_hours]}")
        
        # ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø¯Ø§Øº
        if analysis['hot_keys']:
            hot_key = analysis['hot_keys'][0]
            recommendations.append(f"ğŸ”¥ Ú©Ù„ÛŒØ¯ Ø¯Ø§Øº: {hot_key['key']} ({hot_key['access_count']} Ø¯Ø³ØªØ±Ø³ÛŒ)")
        
        # ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø³Ø±Ø¯
        if analysis['cold_keys']:
            cold_key_count = len([k for k in analysis['cold_keys'] if k['access_count'] == 1])
            if cold_key_count > 10:
                recommendations.append(f"ğŸ§Š {cold_key_count} Ú©Ù„ÛŒØ¯ Ø¨Ø§ Ø¯Ø³ØªØ±Ø³ÛŒ ØªÚ©â€ŒØ¨Ø§Ø±Ù‡ - Ø§Ù…Ú©Ø§Ù† Ø­Ø°Ù")
        
        analysis['recommendations'] = recommendations

    def _calculate_optimal_ttl(self, key_analysis: List[Dict]) -> int:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ TTL Ø¨Ù‡ÛŒÙ†Ù‡"""
        if not key_analysis:
            return 300  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† TTL ÙØ¹Ù„ÛŒ
        current_ttls = [k['current_ttl'] for k in key_analysis if k['current_ttl'] > 0]
        if not current_ttls:
            return 300
        
        avg_ttl = sum(current_ttls) / len(current_ttls)
        
        # ØªÙ†Ø¸ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ
        access_counts = [k['access_count'] for k in key_analysis]
        avg_access = sum(access_counts) / len(access_counts) if access_counts else 1
        
        if avg_access > 50:  # Ø¯Ø³ØªØ±Ø³ÛŒ Ø²ÛŒØ§Ø¯
            return min(3600, int(avg_ttl * 1.5))
        elif avg_access < 5:  # Ø¯Ø³ØªØ±Ø³ÛŒ Ú©Ù…
            return max(60, int(avg_ttl * 0.7))
        else:
            return int(avg_ttl)

    def _analyze_load_balancing(self, health_report: Dict):
        """ØªØ­Ù„ÛŒÙ„ ØªØ¹Ø§Ø¯Ù„ Ø¨Ø§Ø± Ø¨ÛŒÙ† Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§"""
        memory_usage = []
        for db, health in health_report['database_health'].items():
            if 'memory_usage_percentage' in health:
                memory_usage.append(health['memory_usage_percentage'])
        
        if memory_usage:
            avg_usage = sum(memory_usage) / len(memory_usage)
            max_usage = max(memory_usage)
            min_usage = min(memory_usage)
            
            imbalance = max_usage - min_usage
            if imbalance > 30:  # Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡
                health_report['recommendations'].append(
                    f"âš–ï¸ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ø­Ø§ÙØ¸Ù‡: {imbalance:.1f}% - Ø¨Ø§Ø²ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯"
                )

    def _identify_cost_savings(self, report: Dict):
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø¯Ø± Ù‡Ø²ÛŒÙ†Ù‡"""
        total_cost = sum([db['estimated_monthly_cost'] for db in report['cost_estimation'].values()])
        
        # ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ø±Ø§ÛŒÛŒ
        for db_name, data in report['cost_estimation'].items():
            efficiency = data['efficiency_score']
            if efficiency < 60:
                report['optimization_opportunities'].append(
                    f"ğŸ”§ {db_name}: Ú©Ø§Ø±Ø§ÛŒÛŒ Ù¾Ø§ÛŒÛŒÙ† ({efficiency}%) - Ø§Ù…Ú©Ø§Ù† Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"
                )
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ consolidating Ø§Ú¯Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø§Ù„Ø§ Ø¨Ø§Ø´Ø¯
        if total_cost > 10:  # Ø§Ú¯Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ú©Ù„ Ø¨ÛŒØ´ Ø§Ø² 10 Ø¯Ù„Ø§Ø± Ø¨Ø§Ø´Ø¯
            report['optimization_opportunities'].append(
                "ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡ Ù…Ø§Ù‡Ø§Ù†Ù‡ Ø¨Ø§Ù„Ø§ - Ø§Ù…Ú©Ø§Ù† Ø§Ø¯ØºØ§Ù… Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§"
            )

    def _calculate_efficiency_score(self, usage: Dict) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ø§Ø±Ø§ÛŒÛŒ"""
        memory_usage = usage.get('used_memory_percentage', 0)
        keys_count = usage.get('keys_count', 0)
        
        # Ù‡Ø±Ú†Ù‡ Ø­Ø§ÙØ¸Ù‡ Ú©Ù…ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ùˆ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ø§Ù„Ø§ØªØ±
        if keys_count == 0:
            return 0
        
        efficiency = (100 - memory_usage) * (min(keys_count, 1000) / 1000)
        return round(min(efficiency, 100), 1)

    def _get_key_access_stats(self, key: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¯Ø³ØªØ±Ø³ÛŒ ÛŒÚ© Ú©Ù„ÛŒØ¯"""
        # Ø§ÛŒÙ† Ù…ØªØ¯ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¯Ø§Ø±Ø¯
        # Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡
        return {
            'access_count': 0,
            'last_access': None
        }

    def _store_analytics(self, analytics_type: str, data: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¢Ù†Ø§Ù„ÛŒØªÛŒÚ©Ø³"""
        try:
            key = f"analytics:{analytics_type}:{datetime.now().strftime('%Y%m%d_%H')}"
            self.redis_manager.set(
                self.analytics_db, 
                key, 
                data, 
                expire=7*24*3600  # Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ 7 Ø±ÙˆØ²
            )
        except Exception as e:
            logger.error(f"âŒ Error storing analytics: {e}")

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§ØµÙ„ÛŒ
cache_optimizer = CacheOptimizationEngine()

logger.info("ğŸš€ Cache Optimization Engine Initialized - Advanced Analytics & Optimization")

__all__ = ["CacheOptimizationEngine", "cache_optimizer"]
