"""
Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ - ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø§ cache_debugger
Smart Cache System Integrated with Cache Debugger
"""

import functools
import gzip
import pickle
from datetime import datetime
from typing import Callable, Any, Dict, Optional
import asyncio
import logging

logger = logging.getLogger(__name__)

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø³ÛŒØ³ØªÙ… Ú©Ø´ ÙˆØ§Ù‚Ø¹ÛŒ
try:
    from .cache_debugger import cache_debugger
    CACHE_DEBUGGER_AVAILABLE = True
    logger.info("âœ… Cache Debugger integrated with Smart Cache")
except ImportError as e:
    CACHE_DEBUGGER_AVAILABLE = False
    logger.error(f"âŒ Cache Debugger not available: {e}")

class SmartCache:
    """Ø³ÛŒØ³ØªÙ… Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„"""
    
    def __init__(self):
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯
        self.cache_strategies = {
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ - TTL Ø¨ÛŒØ´ØªØ±
            'coins': {
                'base_ttl': 300,
                'description': 'Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ú©ÙˆÛŒÙ†â€ŒÙ‡Ø§',
                'compress_threshold': 100000,
                'priority': 'high'
            },
            'exchanges': {
                'base_ttl': 600,
                'description': 'Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ ØµØ±Ø§ÙÛŒâ€ŒÙ‡Ø§',
                'compress_threshold': 100000,
                'priority': 'high'
            },
            'news': {
                'base_ttl': 600,
                'description': 'Ø§Ø®Ø¨Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡',
                'compress_threshold': 50000,
                'priority': 'medium'
            },
            'insights': {
                'base_ttl': 1800,
                'description': 'ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡',
                'compress_threshold': 50000,
                'priority': 'high'
            },
            
            # Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… - TTL Ú©Ù…ØªØ±
            'raw_coins': {
                'base_ttl': 180,
                'description': 'Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ú©ÙˆÛŒÙ†â€ŒÙ‡Ø§',
                'compress_threshold': 50000,
                'priority': 'low'
            },
            'raw_exchanges': {
                'base_ttl': 300,
                'description': 'Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… ØµØ±Ø§ÙÛŒâ€ŒÙ‡Ø§',
                'compress_threshold': 50000,
                'priority': 'low'
            },
            'raw_news': {
                'base_ttl': 300,
                'description': 'Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ø§Ø®Ø¨Ø§Ø±',
                'compress_threshold': 50000,
                'priority': 'low'
            },
            'raw_insights': {
                'base_ttl': 900,
                'description': 'Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§',
                'compress_threshold': 50000,
                'priority': 'medium'
            }
        }
        
        # Ø¢Ù…Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ Ø³ÛŒØ³ØªÙ…
        self.cache_stats = {
            'total_requests': 0,
            'hits': 0,
            'misses': 0,
            'compressions': 0,
            'errors': 0,
            'bytes_saved': 0,
            'performance': {
                'avg_response_time': 0,
                'last_cleanup': None,
                'health_score': 100
            },
            'strategy_stats': {}
        }
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
        self.compression_enabled = True
        self.max_cache_size = 25 * 1024 * 1024  # 25MB
        
        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¢Ù…Ø§Ø± Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§
        for strategy in self.cache_strategies.keys():
            self.cache_stats['strategy_stats'][strategy] = {
                'hits': 0, 'misses': 0, 'size': 0, 'items': 0
            }

    def compress_data(self, data: Any) -> tuple[bytes, bool]:
        """ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø¯Ù‡"""
        try:
            if not self.compression_enabled:
                serialized = pickle.dumps(data)
                return serialized, False
            
            serialized = pickle.dumps(data)
            original_size = len(serialized)
            
            # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†
            if original_size < 2000:  # Ú©Ù…ØªØ± Ø§Ø² 2KB
                return serialized, False
            
            compressed = gzip.compress(serialized)
            compressed_size = len(compressed)
            
            # Ø§Ú¯Ø± ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ«Ø± Ù†Ø¨ÙˆØ¯
            if compressed_size >= original_size * 0.9:
                return serialized, False
            
            self.cache_stats['compressions'] += 1
            self.cache_stats['bytes_saved'] += (original_size - compressed_size)
            return compressed, True
            
        except Exception as e:
            self.cache_stats['errors'] += 1
            logger.error(f"Ø®Ø·Ø§ÛŒ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
            return pickle.dumps(data), False

    def decompress_data(self, data: bytes, was_compressed: bool) -> Any:
        """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø§Ø¯Ù‡ ÙØ´Ø±Ø¯Ù‡"""
        try:
            if was_compressed:
                decompressed = gzip.decompress(data)
                return pickle.loads(decompressed)
            return pickle.loads(data)
        except Exception as e:
            self.cache_stats['errors'] += 1
            logger.error(f"Ø®Ø·Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ: {e}")
            return None

    def get_ttl(self, strategy: str, data_size: int = 0) -> int:
        """TTL Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ùˆ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡"""
        strategy_config = self.cache_strategies.get(strategy, {'base_ttl': 300})
        base_ttl = strategy_config['base_ttl']
        
        # Ú©Ø§Ù‡Ø´ TTL Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø¬ÛŒÙ…
        if data_size > 5000000:    # Ø¨ÛŒØ´ Ø§Ø² 5MB
            return max(60, base_ttl // 3)
        elif data_size > 1000000:  # Ø¨ÛŒØ´ Ø§Ø² 1MB
            return max(120, base_ttl // 2)
        
        return base_ttl

    def _update_stats(self, strategy: str, hit: bool, data_size: int = 0, response_time: float = 0):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ"""
        self.cache_stats['total_requests'] += 1
        
        if hit:
            self.cache_stats['hits'] += 1
            self.cache_stats['strategy_stats'][strategy]['hits'] += 1
        else:
            self.cache_stats['misses'] += 1
            self.cache_stats['strategy_stats'][strategy]['misses'] += 1
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®
        current_avg = self.cache_stats['performance']['avg_response_time']
        total_requests = self.cache_stats['total_requests']
        
        if total_requests == 1:
            self.cache_stats['performance']['avg_response_time'] = response_time
        else:
            self.cache_stats['performance']['avg_response_time'] = (
                (current_avg * (total_requests - 1) + response_time) / total_requests
            )
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø³Ø§ÛŒØ² Ø¯Ø§Ø¯Ù‡
        if data_size > 0 and not hit:
            self.cache_stats['strategy_stats'][strategy]['size'] += data_size
            self.cache_stats['strategy_stats'][strategy]['items'] += 1

    def cache_strategy(self, strategy: str):
        """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ø§ØµÙ„ÛŒ Ø¨Ø§ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ"""
        
        def decorator(func: Callable) -> Callable:
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                if not CACHE_DEBUGGER_AVAILABLE:
                    # Fallback: Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø¯ÙˆÙ† Ú©Ø´
                    return await func(*args, **kwargs)
                
                start_time = datetime.now()
                cache_key = f"{strategy}:{func.__module__}:{func.__name__}"
                
                try:
                    # Ú†Ú© Ú©Ø´ Ø¯Ø± cache_debugger ÙˆØ§Ù‚Ø¹ÛŒ
                    cached_data = cache_debugger.get_data(cache_key)
                    
                    if cached_data is not None:
                        # Ø¯Ø§Ø¯Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª ÙØ´Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                        if isinstance(cached_data, tuple) and len(cached_data) == 2:
                            data, was_compressed = cached_data
                            result = self.decompress_data(data, was_compressed)
                        else:
                            result = cached_data
                            
                        if result is not None:
                            response_time = (datetime.now() - start_time).total_seconds() * 1000
                            self._update_stats(strategy, True, 0, response_time)
                            logger.info(f"âœ… Cache HIT: {strategy}.{func.__name__}")
                            return result
                    
                    # Cache MISS
                    response_time = (datetime.now() - start_time).total_seconds() * 1000
                    self._update_stats(strategy, False, 0, response_time)
                    logger.info(f"ğŸ”„ Cache MISS: {strategy}.{func.__name__}")
                    
                    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
                    result = await func(*args, **kwargs)
                    
                    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
                    if result is not None:
                        # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯
                        compressed_data, was_compressed = self.compress_data(result)
                        data_size = len(compressed_data)
                        
                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ TTL Ù‡ÙˆØ´Ù…Ù†Ø¯
                        expire = self.get_ttl(strategy, data_size)
                        
                        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± cache_debugger ÙˆØ§Ù‚Ø¹ÛŒ
                        cache_value = (compressed_data, was_compressed) if was_compressed else result
                        cache_debugger.set_data(cache_key, cache_value, expire)
                        
                        logger.info(f"ğŸ’¾ Cache SET: {strategy}.{func.__name__} ({expire}s, {data_size} bytes, compressed: {was_compressed})")
                    
                    return result
                    
                except Exception as e:
                    self.cache_stats['errors'] += 1
                    logger.error(f"âŒ Cache ERROR in {strategy}.{func.__name__}: {e}")
                    # Fallback: Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø¨Ø¯ÙˆÙ† Ú©Ø´
                    return await func(*args, **kwargs)
            
            return wrapper
        return decorator

    def get_health_status(self) -> Dict[str, Any]:
        """Ú¯Ø²Ø§Ø±Ø´ Ø³Ù„Ø§Ù…Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ù…Ø§Ø±"""
        total_requests = self.cache_stats['total_requests']
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ hit rate ÙˆØ§Ù‚Ø¹ÛŒ
        if total_requests > 0:
            hit_rate = (self.cache_stats['hits'] / total_requests) * 100
        else:
            hit_rate = 0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø³Ù„Ø§Ù…Øª
        health_score = 100
        
        # Ú©Ø³Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®Ø·Ø§Ù‡Ø§
        error_rate = (self.cache_stats['errors'] / max(total_requests, 1)) * 100
        health_score -= min(30, error_rate * 3)
        
        # Ú©Ø³Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ hit rate Ù¾Ø§ÛŒÛŒÙ†
        if hit_rate < 50:
            health_score -= (50 - hit_rate) / 2
        
        health_score = max(0, min(100, health_score))
        
        # ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ
        if health_score >= 80:
            status = "healthy"
        elif health_score >= 60:
            status = "degraded"
        else:
            status = "unhealthy"
        
        return {
            "status": status,
            "health_score": round(health_score, 1),
            "summary": {
                "hit_rate": round(hit_rate, 1),
                "total_requests": total_requests,
                "avg_response_time": round(self.cache_stats['performance']['avg_response_time'], 2),
                "compression_savings": self.cache_stats['bytes_saved'],
                "strategies_active": len(self.cache_strategies)
            },
            "timestamp": datetime.now().isoformat(),
            "cache_size": "25MB",
            "compression": self.compression_enabled,
            "detailed_stats": {
                "hits": self.cache_stats['hits'],
                "misses": self.cache_stats['misses'],
                "compressions": self.cache_stats['compressions'],
                "errors": self.cache_stats['errors'],
                "strategy_breakdown": self.cache_stats['strategy_stats']
            }
        }

    def get_cache_stats(self) -> Dict[str, Any]:
        """Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ú©Ø´"""
        return {
            "timestamp": datetime.now().isoformat(),
            "smart_cache_stats": self.cache_stats,
            "strategies": self.cache_strategies,
            "settings": {
                "compression_enabled": self.compression_enabled,
                "max_cache_size": f"{self.max_cache_size / 1024 / 1024}MB",
                "cache_debugger_available": CACHE_DEBUGGER_AVAILABLE
            }
        }

    def clear_cache(self):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ø´ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ)"""
        # Ø¯Ø± Ø¹Ù…Ù„ØŒ Ø§ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ø§ cache_debugger Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ø´ÙˆØ¯
        self.cache_stats = {
            'total_requests': 0,
            'hits': 0,
            'misses': 0,
            'compressions': 0,
            'errors': 0,
            'bytes_saved': 0,
            'performance': {'avg_response_time': 0, 'last_cleanup': datetime.now().isoformat(), 'health_score': 100},
            'strategy_stats': {s: {'hits': 0, 'misses': 0, 'size': 0, 'items': 0} for s in self.cache_strategies}
        }
        logger.info("ğŸ§¹ Smart Cache statistics cleared")

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§ØµÙ„ÛŒ
smart_cache = SmartCache()

# ğŸ”½ Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Û¸ ÙØ§ÛŒÙ„ route

# Ø¨Ø±Ø§ÛŒ routes Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
coins_cache = smart_cache.cache_strategy("coins")
exchanges_cache = smart_cache.cache_strategy("exchanges")
news_cache = smart_cache.cache_strategy("news")
insights_cache = smart_cache.cache_strategy("insights")

# Ø¨Ø±Ø§ÛŒ routes Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…
raw_coins_cache = smart_cache.cache_strategy("raw_coins")
raw_exchanges_cache = smart_cache.cache_strategy("raw_exchanges")
raw_news_cache = smart_cache.cache_strategy("raw_news")
raw_insights_cache = smart_cache.cache_strategy("raw_insights")

logger.info("ğŸš€ Smart Cache System Initialized - Full Integration with Cache Debugger")

# ğŸ”½ export Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØª
__all__ = [
    "SmartCache", "smart_cache",
    "coins_cache", "exchanges_cache", "news_cache", "insights_cache",
    "raw_coins_cache", "raw_exchanges_cache", "raw_news_cache", "raw_insights_cache"
]
