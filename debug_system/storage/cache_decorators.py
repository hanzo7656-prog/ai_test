import functools
import hashlib
import json
from datetime import datetime, timedelta
from typing import Any, Callable, Dict, List, Optional
from collections import defaultdict

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª ØµØ­ÛŒØ­ Ø§Ø² Ù…Ø§Ú˜ÙˆÙ„ Ù‡Ù…Ø³Ø·Ø­
try:
    # Ø±ÙˆØ´ Û±: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ù†Ø³Ø¨ÛŒ
    from .cache_debugger import cache_debugger
except ImportError:
    try:
        # Ø±ÙˆØ´ Û²: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ù…Ø·Ù„Ù‚  
        from debug_system.storage.cache_debugger import cache_debugger
    except ImportError:
        # Ø±ÙˆØ´ Û³: Fallback Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from storage.cache_debugger import cache_debugger

# Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±...
# Ù†Ù‚Ø´Ù‡â€ŒÙ†Ú¯Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù Ø¯Ø§Ø¯Ù‡
DATABASE_MAPPING = {
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ AI - UTB
    'coins': 'utb',
    'news': 'utb', 
    'insights': 'utb',
    'exchanges': 'utb',
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… - UTC
    'raw_coins': 'utc',
    'raw_news': 'utc',
    'raw_insights': 'utc',
    'raw_exchanges': 'utc',
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ AI - UTA
    'model_predictions': 'uta',
    'ai_analysis': 'uta',
    'technical_signals': 'uta',
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ… - MOTHER_A
    'user_data': 'mother_a',
    'system_config': 'mother_a',
    'transactions': 'mother_a',
    
    # Ú©Ø´ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ - MOTHER_B
    'page_cache': 'mother_b',
    'session_data': 'mother_b',
    'temp_cache': 'mother_b',
    
    # Ø¢Ø±Ø´ÛŒÙˆ ØªØ§Ø±ÛŒØ®ÛŒ - UTC
    'archive': 'utc',
    'historical': 'utc'
}

def cache_response(expire: int = 300, key_prefix: str = "", database: str = None):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø´ Ú©Ø±Ø¯Ù† Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ûµ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # ØªØ¹ÛŒÛŒÙ† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø± Ø§Ø³Ø§Ø³ prefix ÛŒØ§ Ù…Ù‚Ø¯Ø§Ø± explicit
            target_db = database or DATABASE_MAPPING.get(key_prefix, 'utb')
            
            cache_key = generate_cache_key(func, key_prefix, *args, **kwargs)
            
            # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Ú©Ø´
            cached_result = cache_debugger.get_data(target_db, cache_key)
            if cached_result is not None:
                print(f"âœ… Cache HIT: {func.__name__} [DB: {target_db.upper()}]")
                return cached_result
            
            # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
            result = await func(*args, **kwargs)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ Ø¯Ø± Ú©Ø´
            if result is not None:
                cache_debugger.set_data(target_db, cache_key, result, expire)
                print(f"ğŸ’¾ Cache SET: {func.__name__} [DB: {target_db.upper()}, TTL: {expire}s]")
            
            return result
        return wrapper
    return decorator

def cache_with_archive(realtime_ttl: int = 300, archive_ttl: int = 365*24*3600, 
                      archive_strategy: str = "hourly", key_prefix: str = ""):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ø¨Ø±Ø§ÛŒ Ú©Ø´ Ù…ÙˆÙ‚Øª + Ø¢Ø±Ø´ÛŒÙˆ ØªØ§Ø±ÛŒØ®ÛŒ"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # ØªØ¹ÛŒÛŒÙ† Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            target_db = DATABASE_MAPPING.get(key_prefix, 'utc')
            
            # Ú©Ù„ÛŒØ¯ Ú©Ø´ Ù…ÙˆÙ‚Øª
            realtime_key = generate_cache_key(func, f"realtime_{key_prefix}", *args, **kwargs)
            
            # Û±. Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´ Ù…ÙˆÙ‚Øª
            cached_realtime = cache_debugger.get_data(target_db, realtime_key)
            if cached_realtime is not None:
                print(f"âœ… Realtime Cache HIT: {func.__name__} [DB: {target_db.upper()}]")
                return cached_realtime
            
            # Û². Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
            result = await func(*args, **kwargs)
            if result is None:
                return None
            
            # Û³. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ Ù…ÙˆÙ‚Øª
            cache_debugger.set_data(target_db, realtime_key, result, realtime_ttl)
            print(f"ğŸ’¾ Realtime Cache SET: {func.__name__} [DB: {target_db.upper()}, TTL: {realtime_ttl}s]")
            
            # Û´. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¢Ø±Ø´ÛŒÙˆ ØªØ§Ø±ÛŒØ®ÛŒ (Ù‡Ù…ÛŒØ´Ù‡ Ø¯Ø± UTC)
            archive_key = generate_archive_key(func, archive_strategy, key_prefix, *args, **kwargs)
            archive_data = {
                'timestamp': datetime.now().isoformat(),
                'data': result,
                'metadata': {
                    'function': func.__name__,
                    'prefix': key_prefix,
                    'strategy': archive_strategy,
                    'realtime_ttl': realtime_ttl,
                    'archive_ttl': archive_ttl
                }
            }
            
            cache_debugger.set_data("utc", archive_key, archive_data, archive_ttl)
            print(f"ğŸ“¦ Historical Archive SET: {func.__name__} [Strategy: {archive_strategy}, TTL: {archive_ttl}s]")
            
            return result
        return wrapper
    return decorator

def generate_cache_key(func: Callable, prefix: str, *args, **kwargs) -> str:
    """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ú©Ø´"""
    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ø³Ø±ÛŒØ§Ù„â€ŒØ³Ø§Ø²ÛŒ
    filtered_kwargs = {}
    for k, v in kwargs.items():
        try:
            json.dumps(v)
            filtered_kwargs[k] = v
        except:
            filtered_kwargs[k] = str(v)
    
    key_data = {
        'func': func.__name__,
        'module': func.__module__,
        'args': str(args),
        'kwargs': str(sorted(filtered_kwargs.items()))
    }
    key_string = f"{prefix}:{json.dumps(key_data, sort_keys=True)}"
    return hashlib.md5(key_string.encode()).hexdigest()

def generate_archive_key(func: Callable, strategy: str, prefix: str, *args, **kwargs) -> str:
    """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ø±Ø´ÛŒÙˆ ØªØ§Ø±ÛŒØ®ÛŒ"""
    timestamp = datetime.now()
    
    if strategy == "minutely":
        time_part = timestamp.strftime("%Y%m%d_%H%M")
    elif strategy == "hourly":
        time_part = timestamp.strftime("%Y%m%d_%H")
    elif strategy == "daily":
        time_part = timestamp.strftime("%Y%m%d")
    elif strategy == "weekly":
        time_part = timestamp.strftime("%Y%W")
    else:  # monthly
        time_part = timestamp.strftime("%Y%m")
    
    base_key = generate_cache_key(func, prefix, *args, **kwargs)
    return f"archive:{strategy}:{prefix}:{time_part}:{base_key}"

# ==================== Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ Û¸ ÙØ§ÛŒÙ„ route Ø¨Ø§ Ø¢Ø±Ø´ÛŒÙˆ ====================

# ğŸ”½ Ø¨Ø±Ø§ÛŒ routes Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… + Ø¢Ø±Ø´ÛŒÙˆ ØªØ§Ø±ÛŒØ®ÛŒ (UTC)
def cache_raw_coins_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_coins.py - Ú©Ø´ Û³ Ø¯Ù‚ÛŒÙ‚Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ Ø³Ø§Ø¹ØªÛŒ"""
    return cache_with_archive(
        realtime_ttl=180,           # Û³ Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ
        archive_ttl=30*24*3600,     # Ø¢Ø±Ø´ÛŒÙˆ Û³Û° Ø±ÙˆØ²Ù‡
        archive_strategy="hourly",  # Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø¹ØªÛŒ
        key_prefix="raw_coins"
    )

def cache_raw_news_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_news.py - Ú©Ø´ Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    return cache_with_archive(
        realtime_ttl=300,           # Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡
        archive_ttl=90*24*3600,     # Ø¢Ø±Ø´ÛŒÙˆ Û³ Ù…Ø§Ù‡Ù‡
        archive_strategy="daily",   # Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡
        key_prefix="raw_news"
    )

def cache_raw_insights_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_insights.py - Ú©Ø´ Û±Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    return cache_with_archive(
        realtime_ttl=900,           # Û±Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡
        archive_ttl=180*24*3600,    # Ø¢Ø±Ø´ÛŒÙˆ Û¶ Ù…Ø§Ù‡Ù‡
        archive_strategy="daily",   # Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡
        key_prefix="raw_insights"
    )

def cache_raw_exchanges_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_exchanges.py - Ú©Ø´ Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ Ø³Ø§Ø¹ØªÛŒ"""
    return cache_with_archive(
        realtime_ttl=300,           # Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡
        archive_ttl=30*24*3600,     # Ø¢Ø±Ø´ÛŒÙˆ Û³Û° Ø±ÙˆØ²Ù‡
        archive_strategy="hourly",  # Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø¹ØªÛŒ
        key_prefix="raw_exchanges"
    )

# ğŸ”½ Ø¨Ø±Ø§ÛŒ routes Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ (UTB)
def cache_coins_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ coins.py - Ú©Ø´ Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    return cache_with_archive(
        realtime_ttl=600,           # Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡
        archive_ttl=365*24*3600,    # Ø¢Ø±Ø´ÛŒÙˆ Û± Ø³Ø§Ù„Ù‡
        archive_strategy="daily",   # Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡
        key_prefix="coins"
    )

def cache_news_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ news.py - Ú©Ø´ Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ Ù‡ÙØªÚ¯ÛŒ"""
    return cache_with_archive(
        realtime_ttl=600,           # Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡
        archive_ttl=180*24*3600,    # Ø¢Ø±Ø´ÛŒÙˆ Û¶ Ù…Ø§Ù‡Ù‡
        archive_strategy="weekly",  # Ø°Ø®ÛŒØ±Ù‡ Ù‡ÙØªÚ¯ÛŒ
        key_prefix="news"
    )

def cache_insights_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ insights.py - Ú©Ø´ Û± Ø³Ø§Ø¹Øª + Ø¢Ø±Ø´ÛŒÙˆ Ù‡ÙØªÚ¯ÛŒ"""
    return cache_with_archive(
        realtime_ttl=3600,          # Û± Ø³Ø§Ø¹Øª
        archive_ttl=365*24*3600,    # Ø¢Ø±Ø´ÛŒÙˆ Û± Ø³Ø§Ù„Ù‡
        archive_strategy="weekly",  # Ø°Ø®ÛŒØ±Ù‡ Ù‡ÙØªÚ¯ÛŒ
        key_prefix="insights"
    )

def cache_exchanges_with_archive():
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ exchanges.py - Ú©Ø´ Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡ + Ø¢Ø±Ø´ÛŒÙˆ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    return cache_with_archive(
        realtime_ttl=600,           # Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡
        archive_ttl=180*24*3600,    # Ø¢Ø±Ø´ÛŒÙˆ Û¶ Ù…Ø§Ù‡Ù‡
        archive_strategy="daily",   # Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡
        key_prefix="exchanges"
    )

# ==================== Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ (Ø¨Ø¯ÙˆÙ† Ø¢Ø±Ø´ÛŒÙˆ) ====================

def cache_coins(expire: int = 600):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ coins.py (Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡) - UTB"""
    return cache_response(expire=expire, key_prefix="coins", database="utb")

def cache_news(expire: int = 600):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ news.py (Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡) - UTB"""
    return cache_response(expire=expire, key_prefix="news", database="utb")

def cache_insights(expire: int = 3600):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ insights.py (Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡) - UTB"""
    return cache_response(expire=expire, key_prefix="insights", database="utb")

def cache_exchanges(expire: int = 600):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ exchanges.py (Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡) - UTB"""
    return cache_response(expire=expire, key_prefix="exchanges", database="utb")

def cache_raw_coins(expire: int = 180):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_coins.py (Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…) - UTC"""
    return cache_response(expire=expire, key_prefix="raw_coins", database="utc")

def cache_raw_news(expire: int = 300):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_news.py (Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…) - UTC"""
    return cache_response(expire=expire, key_prefix="raw_news", database="utc")

def cache_raw_insights(expire: int = 900):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_insights.py (Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…) - UTC"""
    return cache_response(expire=expire, key_prefix="raw_insights", database="utc")

def cache_raw_exchanges(expire: int = 300):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ù…Ø®ØµÙˆØµ raw_exchanges.py (Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…) - UTC"""
    return cache_response(expire=expire, key_prefix="raw_exchanges", database="utc")

# ==================== Ù…ØªØ¯Ù‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¢Ø±Ø´ÛŒÙˆ ØªØ§Ø±ÛŒØ®ÛŒ ====================

def get_historical_data(function_name: str, prefix: str, start_date: str, end_date: str, 
                       strategy: str = "daily") -> List[Dict[str, Any]]:
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø§Ø² Ø¢Ø±Ø´ÛŒÙˆ"""
    historical_results = []
    
    try:
        start = datetime.strptime(start_date, "%Y%m%d")
        end = datetime.strptime(end_date, "%Y%m%d")
        
        current = start
        while current <= end:
            if strategy == "hourly":
                # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¹ØªÛŒØŒ ØªÙ…Ø§Ù… Ø³Ø§Ø¹Ø§Øª Ø±ÙˆØ² Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯
                for hour in range(24):
                    time_part = current.strftime(f"%Y%m%d_{hour:02d}")
                    archive_pattern = f"archive:{strategy}:{prefix}:{time_part}:*"
                    keys = cache_debugger.get_keys("utc", archive_pattern)[0]
                    
                    for key in keys:
                        data = cache_debugger.get_data("utc", key)
                        if data and data.get('metadata', {}).get('function') == function_name:
                            historical_results.append(data)
            else:
                time_part = current.strftime("%Y%m%d")
                archive_pattern = f"archive:{strategy}:{prefix}:{time_part}:*"
                keys = cache_debugger.get_keys("utc", archive_pattern)[0]
                
                for key in keys:
                    data = cache_debugger.get_data("utc", key)
                    if data and data.get('metadata', {}).get('function') == function_name:
                        historical_results.append(data)
            
            current += timedelta(days=1)
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ timestamp
        historical_results.sort(key=lambda x: x.get('timestamp', ''))
        
    except Exception as e:
        print(f"âŒ Error retrieving historical data: {e}")
    
    return historical_results

def get_archive_stats(prefix: str = None) -> Dict[str, Any]:
    """Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø±Ø´ÛŒÙˆ Ø´Ø¯Ù‡"""
    archive_pattern = "archive:*" if not prefix else f"archive:*:{prefix}:*"
    archive_keys = cache_debugger.get_keys("utc", archive_pattern)[0]
    
    stats = {
        'total_archives': len(archive_keys),
        'by_strategy': defaultdict(int),
        'by_prefix': defaultdict(int),
        'by_function': defaultdict(int),
        'oldest_archive': None,
        'newest_archive': None,
        'total_size_mb': 0
    }
    
    for key in archive_keys:
        try:
            parts = key.split(':')
            if len(parts) >= 4:
                strategy = parts[1]
                archive_prefix = parts[2] if len(parts) > 2 else "unknown"
                time_part = parts[3] if len(parts) > 3 else "unknown"
                
                stats['by_strategy'][strategy] += 1
                stats['by_prefix'][archive_prefix] += 1
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ ØªÙ‚Ø±ÛŒØ¨ÛŒ
                data = cache_debugger.get_data("utc", key)
                if data:
                    stats['total_size_mb'] += len(json.dumps(data)) / (1024 * 1024)
                    
                    function_name = data.get('metadata', {}).get('function', 'unknown')
                    stats['by_function'][function_name] += 1
                
                # Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ùˆ Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†
                if time_part != "unknown":
                    if not stats['oldest_archive'] or time_part < stats['oldest_archive']:
                        stats['oldest_archive'] = time_part
                    if not stats['newest_archive'] or time_part > stats['newest_archive']:
                        stats['newest_archive'] = time_part
        
        except Exception as e:
            print(f"âŒ Error processing archive key {key}: {e}")
    
    stats['total_size_mb'] = round(stats['total_size_mb'], 2)
    return stats

def cleanup_old_archives(days_old: int = 365):
    """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø¢Ø±Ø´ÛŒÙˆÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
    try:
        cutoff_date = datetime.now() - timedelta(days=days_old)
        archive_keys = cache_debugger.get_keys("utc", "archive:*")[0]
        
        deleted_count = 0
        for key in archive_keys:
            try:
                parts = key.split(':')
                if len(parts) >= 4:
                    time_part = parts[3]
                    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ datetime (ÙØ±Ù…Øª: YYYYMMDD ÛŒØ§ YYYYMMDD_HH)
                    if '_' in time_part:
                        archive_date = datetime.strptime(time_part.split('_')[0], "%Y%m%d")
                    else:
                        archive_date = datetime.strptime(time_part, "%Y%m%d")
                    
                    if archive_date < cutoff_date:
                        cache_debugger.delete_data("utc", key)
                        deleted_count += 1
            except:
                continue
        
        print(f"ğŸ§¹ Cleaned up {deleted_count} archives older than {days_old} days")
        return deleted_count
        
    except Exception as e:
        print(f"âŒ Error cleaning up old archives: {e}")
        return 0

# ==================== Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================

def cache_with_fallback(fallback_func: Callable = None, expire: int = 300, 
                       database: str = "utb", use_archive: bool = False):
    """Ø¯Ú©ÙˆØ±Ø§ØªÙˆØ± Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª fallback Ùˆ Ø¢Ø±Ø´ÛŒÙˆ Ø§Ø®ØªÛŒØ§Ø±ÛŒ"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            cache_key = generate_cache_key(func, "fallback", *args, **kwargs)
            
            try:
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Ú©Ø´
                cached_result = cache_debugger.get_data(database, cache_key)
                if cached_result is not None:
                    print(f"âœ… Cache HIT (Fallback): {func.__name__}")
                    return cached_result
                
                # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
                result = await func(*args, **kwargs)
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
                if result is not None:
                    cache_debugger.set_data(database, cache_key, result, expire)
                    
                    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¢Ø±Ø´ÛŒÙˆ Ø§Ú¯Ø± ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ø¯
                    if use_archive:
                        archive_key = generate_archive_key(func, "daily", "fallback", *args, **kwargs)
                        archive_data = {
                            'timestamp': datetime.now().isoformat(),
                            'data': result,
                            'metadata': {'function': func.__name__, 'type': 'fallback'}
                        }
                        cache_debugger.set_data("utc", archive_key, archive_data, 30*24*3600)
                
                return result
                
            except Exception as e:
                print(f"âŒ Error in {func.__name__}: {e}")
                
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² fallback
                if fallback_func:
                    print(f"ğŸ”„ Using fallback for {func.__name__}")
                    fallback_result = fallback_func(*args, **kwargs)
                    
                    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ fallback Ø¯Ø± Ú©Ø´
                    if fallback_result is not None:
                        cache_debugger.set_data(database, cache_key, fallback_result, expire // 2)
                    
                    return fallback_result
                else:
                    # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø± Ø§Ø² Ú©Ø´
                    cached_result = cache_debugger.get_data(database, cache_key)
                    if cached_result is not None:
                        print(f"ğŸ”„ Using cached data as fallback for {func.__name__}")
                        return cached_result
                    raise e
        return wrapper
    return decorator

# utility function Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø³ØªÛŒ Ú©Ø´
def clear_cache_pattern(pattern: str, database: str = None):
    """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯Ùˆ"""
    from redis_manager import redis_manager
    
    if database:
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…Ø´Ø®Øµ
        keys, _ = redis_manager.get_keys(database, pattern)
        for key in keys:
            redis_manager.delete(database, key)
        print(f"ğŸ§¹ Cleared {len(keys)} keys from {database} matching pattern: {pattern}")
    else:
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø§Ø² ØªÙ…Ø§Ù… Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§
        total_cleared = 0
        for db_name in ['uta', 'utb', 'utc', 'mother_a', 'mother_b']:
            keys, _ = redis_manager.get_keys(db_name, pattern)
            for key in keys:
                redis_manager.delete(db_name, key)
            total_cleared += len(keys)
            if keys:
                print(f"ğŸ§¹ Cleared {len(keys)} keys from {db_name}")
        print(f"âœ… Total cleared: {total_cleared} keys")
