"""
ğŸ¤– Data Normalizer - Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ API
ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
- ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
- ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡  
- Ø­ÙØ¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
- Ø§Ø±Ø§Ø¦Ù‡ Ù…ØªØ±ÛŒÚ© Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³Ù„Ø§Ù…Øª
- Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ù…ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø³ÛŒØ³ØªÙ…
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
import logging
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class StructureType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡"""
    DIRECT_LIST = "direct_list"
    DICT_WITH_DATA = "dict_with_data"
    DICT_WITH_RESULT = "dict_with_result" 
    DICT_WITH_ITEMS = "dict_with_items"
    DICT_WITH_COINS = "dict_with_coins"
    CUSTOM_STRUCTURE = "custom_structure"
    UNKNOWN = "unknown"

class NormalizationStrategy(Enum):
    """Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    SMART = "smart"
    STRICT = "strict"
    LENIENT = "lenient"

@dataclass
class NormalizationResult:
    """Ù†ØªÛŒØ¬Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    status: str  # success | error
    data: List[Any]
    metadata: Dict[str, Any]
    raw_data: Any
    normalization_info: Dict[str, Any]
    quality_score: float

@dataclass  
class HealthMetrics:
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…"""
    success_rate: float
    total_processed: int
    total_success: int
    total_errors: int
    common_structures: Dict[str, int]
    performance_metrics: Dict[str, Any]
    alerts: List[str]
    data_quality: Dict[str, float]

class DataNormalizer:
    """
    Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ API
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self._setup_logging()
        self._initialize_cache()
        self._reset_metrics()
        
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        self.default_strategy = NormalizationStrategy.SMART
        
        # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡
        self.supported_structures = {
            StructureType.DIRECT_LIST: self._normalize_direct_list,
            StructureType.DICT_WITH_DATA: self._normalize_dict_with_data,
            StructureType.DICT_WITH_RESULT: self._normalize_dict_with_result,
            StructureType.DICT_WITH_ITEMS: self._normalize_dict_with_items,
            StructureType.DICT_WITH_COINS: self._normalize_dict_with_coins,
        }
        
        logger.info("âœ… Data Normalizer Initialized - Smart Mode Active")

    def _setup_logging(self):
        """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ"""
        self.logger = logging.getLogger(__name__)

    def _initialize_cache(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø´"""
        self.structure_cache = {}  # Ú©Ø´ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ú©Ø´Ù Ø´Ø¯Ù‡
        self.health_cache = {}     # Ú©Ø´ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³Ù„Ø§Ù…Øª
        self.analysis_cache = {}   # Ú©Ø´ Ø¢Ù†Ø§Ù„ÛŒØ²Ù‡Ø§
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…Ø± Ú©Ø´
        self.cache_ttl = {
            'structure': timedelta(days=7),      # 7 Ø±ÙˆØ² Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§
            'health': timedelta(hours=1),        # 1 Ø³Ø§Ø¹Øª Ø¨Ø±Ø§ÛŒ Ø³Ù„Ø§Ù…Øª
            'analysis': timedelta(minutes=30),   # 30 Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ²
        }

    def _reset_metrics(self):
        """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
        self.metrics = {
            'total_processed': 0,
            'total_success': 0, 
            'total_errors': 0,
            'structure_counts': {stype.value: 0 for stype in StructureType},
            'processing_times': [],
            'endpoint_patterns': {},
            'quality_scores': [],
            'alerts': []
        }

    def normalize(self, raw_data: Any, endpoint: str = "unknown", 
                 strategy: NormalizationStrategy = None) -> NormalizationResult:
        """
        Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
        
        Args:
            raw_data: Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ø§Ø² API
            endpoint: Ù†Ø§Ù… endpoint Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÚ¯ÛŒØ±ÛŒ
            strategy: Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            
        Returns:
            NormalizationResult: Ù†ØªÛŒØ¬Ù‡ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø´Ø¯Ù‡
        """
        start_time = time.time()
        self.metrics['total_processed'] += 1
        
        try:
            # ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡
            structure_type, confidence = self._detect_structure(raw_data)
            self.metrics['structure_counts'][structure_type.value] += 1
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡
            normalization_func = self.supported_structures.get(
                structure_type, 
                self._normalize_fallback
            )
            
            normalized_data = normalization_func(raw_data)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡
            quality_score = self._calculate_quality_score(normalized_data, structure_type)
            self.metrics['quality_scores'].append(quality_score)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø§Ù„Ú¯ÙˆÛŒ endpoint
            self._update_endpoint_pattern(endpoint, structure_type, confidence)
            
            # Ø«Ø¨Øª Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´
            processing_time = time.time() - start_time
            self.metrics['processing_times'].append(processing_time)
            
            self.metrics['total_success'] += 1
            
            result = NormalizationResult(
                status="success",
                data=normalized_data,
                metadata=self._extract_metadata(raw_data, structure_type),
                raw_data=raw_data,
                normalization_info={
                    "detected_structure": structure_type.value,
                    "confidence": confidence,
                    "processing_time_ms": round(processing_time * 1000, 2),
                    "endpoint": endpoint,
                    "strategy": (strategy or self.default_strategy).value,
                    "timestamp": datetime.now().isoformat()
                },
                quality_score=quality_score
            )
            
            logger.info(f"âœ… Normalized {endpoint} - Structure: {structure_type.value} - Quality: {quality_score}%")
            return result
            
        except Exception as e:
            self.metrics['total_errors'] += 1
            error_msg = f"Normalization failed for {endpoint}: {str(e)}"
            logger.error(error_msg)
            self.metrics['alerts'].append(error_msg)
            
            return NormalizationResult(
                status="error",
                data=[],
                metadata={},
                raw_data=raw_data,
                normalization_info={
                    "error": str(e),
                    "endpoint": endpoint,
                    "timestamp": datetime.now().isoformat()
                },
                quality_score=0.0
            )

    def _detect_structure(self, raw_data: Any) -> tuple[StructureType, float]:
        """
        ØªØ´Ø®ÛŒØµ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡
        
        Returns:
            tuple: (Ù†ÙˆØ¹ Ø³Ø§Ø®ØªØ§Ø±, Ù…ÛŒØ²Ø§Ù† Ø§Ø·Ù…ÛŒÙ†Ø§Ù†)
        """
        if isinstance(raw_data, list):
            return StructureType.DIRECT_LIST, 0.95
            
        elif isinstance(raw_data, dict):
            # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
            if 'data' in raw_data and isinstance(raw_data['data'], list):
                return StructureType.DICT_WITH_DATA, 0.90
            elif 'result' in raw_data and isinstance(raw_data['result'], list):
                return StructureType.DICT_WITH_RESULT, 0.85
            elif 'items' in raw_data and isinstance(raw_data['items'], list):
                return StructureType.DICT_WITH_ITEMS, 0.80
            elif 'coins' in raw_data and isinstance(raw_data['coins'], list):
                return StructureType.DICT_WITH_COINS, 0.75
            else:
                # Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø³ÙØ§Ø±Ø³ÛŒ
                return self._analyze_custom_structure(raw_data)
        else:
            return StructureType.UNKNOWN, 0.1

    def _analyze_custom_structure(self, raw_data: Dict) -> tuple[StructureType, float]:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø³ÙØ§Ø±Ø³ÛŒ"""
        # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ³Øª Ø¯Ø± Ø³Ø·ÙˆØ­ Ù…Ø®ØªÙ„Ù
        for key, value in raw_data.items():
            if isinstance(value, list) and len(value) > 0:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ú¯Ø± Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù„ÛŒØ³Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù‡Ø³ØªÙ†Ø¯ (Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡)
                if all(isinstance(item, dict) for item in value):
                    return StructureType.CUSTOM_STRUCTURE, 0.7
                    
        return StructureType.UNKNOWN, 0.3

    def _normalize_direct_list(self, raw_data: List) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù„ÛŒØ³Øª Ù…Ø³ØªÙ‚ÛŒÙ…"""
        return raw_data

    def _normalize_dict_with_data(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ data"""
        return raw_data.get('data', [])

    def _normalize_dict_with_result(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ result"""
        return raw_data.get('result', [])

    def _normalize_dict_with_items(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ items"""
        return raw_data.get('items', [])

    def _normalize_dict_with_coins(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ coins"""
        return raw_data.get('coins', [])

    def _normalize_fallback(self, raw_data: Any) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ fallback Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡"""
        if isinstance(raw_data, (list, dict)):
            # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
            return self._extract_data_from_complex_structure(raw_data)
        else:
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù„ÛŒØ³Øª
            return [raw_data] if raw_data is not None else []

    def _extract_data_from_complex_structure(self, raw_data: Any) -> List:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡"""
        if isinstance(raw_data, dict):
            # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ù„ÛŒØ³Øª Ø¨Ø§Ø´Ø¯
            lists_in_dict = [v for v in raw_data.values() if isinstance(v, list)]
            if lists_in_dict:
                # Ø¨Ø§Ø²Ú¯ÙˆØ±Ø¯Ù† Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ù„ÛŒØ³Øª
                return max(lists_in_dict, key=len)
        return []

    def _extract_metadata(self, raw_data: Any, structure_type: StructureType) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…"""
        metadata = {
            "structure_type": structure_type.value,
            "extracted_at": datetime.now().isoformat(),
            "data_source": "coinstats_api"
        }
        
        if isinstance(raw_data, dict):
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§Ù‡Ø§ÛŒ Ø±Ø§ÛŒØ¬
            common_meta_keys = ['meta', 'metadata', 'pagination', 'info', 'total', 'count']
            for key in common_meta_keys:
                if key in raw_data:
                    metadata[key] = raw_data[key]
                    
        return metadata

    def _calculate_quality_score(self, normalized_data: List, structure_type: StructureType) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡"""
        if not normalized_data:
            return 0.0
            
        score = 0.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Ù‡
        data_count = len(normalized_data)
        if data_count > 0:
            score += min(data_count / 100, 0.3)  # Ø­Ø¯Ø§Ú©Ø«Ø± 30% Ø¨Ø±Ø§ÛŒ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡
            
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø±
        structure_scores = {
            StructureType.DIRECT_LIST: 0.2,
            StructureType.DICT_WITH_DATA: 0.25,
            StructureType.DICT_WITH_RESULT: 0.25,
            StructureType.DICT_WITH_ITEMS: 0.2,
            StructureType.DICT_WITH_COINS: 0.2,
            StructureType.CUSTOM_STRUCTURE: 0.15,
            StructureType.UNKNOWN: 0.1
        }
        score += structure_scores.get(structure_type, 0.1)
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ Ø¯Ø§Ø¯Ù‡
        if data_count > 1:
            uniformity_score = self._calculate_uniformity_score(normalized_data)
            score += uniformity_score * 0.5
            
        return min(score * 100, 100.0)  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¯Ø±ØµØ¯

    def _calculate_uniformity_score(self, data: List) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        if not data or len(data) < 2:
            return 0.5
            
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ Ú©Ù„ÛŒØ¯Ù‡Ø§ Ø¯Ø± Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ (Ø§Ú¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù‡Ø³ØªÙ†Ø¯)
            if all(isinstance(item, dict) for item in data):
                first_keys = set(data[0].keys())
                common_keys = first_keys.intersection(*(set(item.keys()) for item in data[1:]))
                return len(common_keys) / len(first_keys) if first_keys else 0.5
        except:
            pass
            
        return 0.5

    def _update_endpoint_pattern(self, endpoint: str, structure_type: StructureType, confidence: float):
        """Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ endpoint"""
        if endpoint not in self.metrics['endpoint_patterns']:
            self.metrics['endpoint_patterns'][endpoint] = {
                'total_requests': 0,
                'structure_counts': {},
                'last_detected': None,
                'confidence_avg': 0.0
            }
            
        pattern = self.metrics['endpoint_patterns'][endpoint]
        pattern['total_requests'] += 1
        pattern['structure_counts'][structure_type.value] = pattern['structure_counts'].get(structure_type.value, 0) + 1
        pattern['last_detected'] = datetime.now().isoformat()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        current_avg = pattern['confidence_avg']
        total_reqs = pattern['total_requests']
        pattern['confidence_avg'] = (current_avg * (total_reqs - 1) + confidence) / total_reqs

    # ========================== PUBLIC METHODS FOR EXTERNAL USE ==========================

    def get_health_metrics(self) -> HealthMetrics:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³Ù„Ø§Ù…Øª Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯"""
        total_processed = self.metrics['total_processed']
        success_rate = (self.metrics['total_success'] / total_processed * 100) if total_processed > 0 else 0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
        processing_times = self.metrics['processing_times']
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡
        quality_scores = self.metrics['quality_scores']
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        return HealthMetrics(
            success_rate=round(success_rate, 2),
            total_processed=total_processed,
            total_success=self.metrics['total_success'],
            total_errors=self.metrics['total_errors'],
            common_structures=self.metrics['structure_counts'],
            performance_metrics={
                'avg_processing_time_ms': round(avg_processing_time * 1000, 2),
                'total_processing_time_ms': round(sum(processing_times) * 1000, 2),
                'requests_per_second': round(total_processed / (sum(processing_times) or 1), 2)
            },
            alerts=self.metrics['alerts'][-10:],  # 10 Ù‡Ø´Ø¯Ø§Ø± Ø¢Ø®Ø±
            data_quality={
                'avg_quality_score': round(avg_quality, 2),
                'completeness_score': round(success_rate, 2),
                'consistency_score': round(self._calculate_consistency_score(), 2)
            }
        )

    def get_deep_analysis(self, raw_data: Any = None, endpoint: str = None) -> Dict[str, Any]:
        """
        Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ù…ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØ¨Ø§Ú¯
        
        Args:
            raw_data: Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ² (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            endpoint: endpoint Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ² (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            
        Returns:
            Dict: Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„
        """
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "system_overview": {
                "total_requests": self.metrics['total_processed'],
                "success_rate": self.get_health_metrics().success_rate,
                "most_common_structure": max(
                    self.metrics['structure_counts'].items(), 
                    key=lambda x: x[1],
                    default=('unknown', 0)
                )
            },
            "endpoint_patterns": self.metrics['endpoint_patterns'],
            "structure_analysis": self.metrics['structure_counts'],
            "performance_analysis": {
                "avg_processing_time": f"{sum(self.metrics['processing_times']) / len(self.metrics['processing_times']) * 1000:.2f}ms" 
                if self.metrics['processing_times'] else "0ms",
                "total_processing_time": f"{sum(self.metrics['processing_times']) * 1000:.2f}ms",
                "fastest_processing": f"{min(self.metrics['processing_times']) * 1000:.2f}ms" 
                if self.metrics['processing_times'] else "0ms",
                "slowest_processing": f"{max(self.metrics['processing_times']) * 1000:.2f}ms" 
                if self.metrics['processing_times'] else "0ms"
            },
            "quality_analysis": {
                "avg_quality_score": f"{sum(self.metrics['quality_scores']) / len(self.metrics['quality_scores']):.2f}%"
                if self.metrics['quality_scores'] else "0%",
                "quality_trend": "stable" if len(self.metrics['quality_scores']) < 2 else
                "improving" if self.metrics['quality_scores'][-1] > self.metrics['quality_scores'][0] else "declining"
            },
            "alerts_and_warnings": self.metrics['alerts'][-20:],  # 20 Ù‡Ø´Ø¯Ø§Ø± Ø¢Ø®Ø±
            "recommendations": self._generate_recommendations()
        }
        
        # Ø¢Ù†Ø§Ù„ÛŒØ² Ø¯Ø§Ø¯Ù‡ Ø®Ø§Øµ Ø§Ú¯Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡
        if raw_data is not None:
            analysis["specific_data_analysis"] = self._analyze_specific_data(raw_data, endpoint)
            
        return analysis

    def _calculate_consistency_score(self) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø«Ø¨Ø§Øª"""
        endpoint_patterns = self.metrics['endpoint_patterns']
        if not endpoint_patterns:
            return 0.0
            
        consistency_scores = []
        for endpoint, pattern in endpoint_patterns.items():
            if pattern['total_requests'] > 1:
                # Ù‡Ø±Ú†Ù‡ Ø§Ù„Ú¯ÙˆÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ø«Ø§Ø¨Øªâ€ŒØªØ± Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…ØªÛŒØ§Ø²æ›´é«˜
                main_structure = max(pattern['structure_counts'].items(), key=lambda x: x[1])
                consistency = main_structure[1] / pattern['total_requests']
                consistency_scores.append(consistency)
                
        return sum(consistency_scores) / len(consistency_scores) * 100 if consistency_scores else 0.0

    def _analyze_specific_data(self, raw_data: Any, endpoint: str = None) -> Dict[str, Any]:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ø¯Ø§Ø¯Ù‡ Ø®Ø§Øµ"""
        structure_type, confidence = self._detect_structure(raw_data)
        
        return {
            "detected_structure": structure_type.value,
            "confidence": confidence,
            "data_type": type(raw_data).__name__,
            "data_size": len(raw_data) if hasattr(raw_data, '__len__') else 'unknown',
            "sample_preview": str(raw_data)[:200] + "..." if len(str(raw_data)) > 200 else str(raw_data),
            "endpoint_context": endpoint,
            "normalization_preview": self.normalize(raw_data, endpoint or "analysis").normalization_info
        }

    def _generate_recommendations(self) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        recommendations = []
        metrics = self.get_health_metrics()
        
        if metrics.success_rate < 95:
            recommendations.append("ğŸ”„ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
            
        if metrics.total_errors > 10:
            recommendations.append("ğŸ› Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡. Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
            
        if metrics.data_quality['avg_quality_score'] < 80:
            recommendations.append("ğŸ“Š Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ù†ÛŒØ¯.")
            
        if not recommendations:
            recommendations.append("âœ… Ø³ÛŒØ³ØªÙ… Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù…Ø·Ù„ÙˆØ¨ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯.")
            
        return recommendations

    def clear_cache(self, cache_type: str = None):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ø´"""
        if cache_type == 'structure' or cache_type is None:
            self.structure_cache.clear()
        if cache_type == 'health' or cache_type is None:
            self.health_cache.clear() 
        if cache_type == 'analysis' or cache_type is None:
            self.analysis_cache.clear()
            
        logger.info("ğŸ§¹ Data Normalizer cache cleared")

    def reset_metrics(self):
        """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ùˆ ØªÙˆØ³Ø¹Ù‡)"""
        self._reset_metrics()
        logger.info("ğŸ”„ Data Normalizer metrics reset")

# Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ù„ÙˆØ¨Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¢Ø³Ø§Ù†
data_normalizer = DataNormalizer()
