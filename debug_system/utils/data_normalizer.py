"""
ðŸ¤– Data Normalizer v2 - Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ API
ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯:
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² 15+ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø®ØªÙ„Ù
- ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø®Ø§Øµ CoinStats API
- Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¯Ø±ØªÙˆ
- Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± endpointÙ‡Ø§
- fallbackÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ØªØ±
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple
import logging
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class StructureType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
    DIRECT_LIST = "direct_list"
    SINGLE_ITEM_LIST = "single_item_list"  # Ø¬Ø¯ÛŒØ¯: Ù„ÛŒØ³Øª ØªÚ©â€ŒØ¢ÛŒØªÙ… [{}]
    
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ù„ÛŒØ³Øª
    DICT_WITH_DATA = "dict_with_data"
    DICT_WITH_RESULT = "dict_with_result" 
    DICT_WITH_ITEMS = "dict_with_items"
    DICT_WITH_COINS = "dict_with_coins"
    DICT_WITH_NEWS = "dict_with_news"  # Ø¬Ø¯ÛŒØ¯
    DICT_WITH_RESULTS = "dict_with_results"  # Ø¬Ø¯ÛŒØ¯
    
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ CoinStats API Ø®Ø§Øµ
    COIN_STATS_PAGINATED = "coin_stats_paginated"  # {"result": [], "meta": {}}
    COIN_STATS_SINGLE_COIN = "coin_stats_single_coin"  # [{}] Ø¨Ø±Ø§ÛŒ coins/bitcoin
    COIN_STATS_NEWS = "coin_stats_news"  # Ø³Ø§Ø®ØªØ§Ø± Ø®Ø§Øµ Ø§Ø®Ø¨Ø§Ø±
    
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
    NESTED_STRUCTURE = "nested_structure"  # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¯Ø±ØªÙˆ
    PAGINATED_RESPONSE = "paginated_response"  # Ù¾Ø§Ø³Ø® ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡
    
    # fallback
    CUSTOM_STRUCTURE = "custom_structure"
    UNKNOWN = "unknown"

class NormalizationStrategy(Enum):
    """Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    SMART = "smart"
    STRICT = "strict"
    LENIENT = "lenient"
    COIN_STATS_OPTIMIZED = "coin_stats_optimized"  # Ø¬Ø¯ÛŒØ¯

@dataclass
class NormalizationResult:
    """Ù†ØªÛŒØ¬Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    status: str  # success | error
    data: List[Any]
    metadata: Dict[str, Any]
    raw_data: Any
    normalization_info: Dict[str, Any]
    quality_score: float

@dataclass  
class HealthMetrics:
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…"""
    success_rate: float
    total_processed: int
    total_success: int
    total_errors: int
    common_structures: Dict[str, int]
    performance_metrics: Dict[str, Any]
    alerts: List[str]
    data_quality: Dict[str, float]
    endpoint_intelligence: Dict[str, Any]  # Ø¬Ø¯ÛŒØ¯

class DataNormalizer:
    """
    Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ API - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self._setup_logging()
        self._initialize_cache()
        self._reset_metrics()
        
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        self.default_strategy = NormalizationStrategy.COIN_STATS_OPTIMIZED
        
        # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡
        self.supported_structures = {
            # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
            StructureType.DIRECT_LIST: self._normalize_direct_list,
            StructureType.SINGLE_ITEM_LIST: self._normalize_single_item_list,
            
            # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
            StructureType.DICT_WITH_DATA: self._normalize_dict_with_data,
            StructureType.DICT_WITH_RESULT: self._normalize_dict_with_result,
            StructureType.DICT_WITH_ITEMS: self._normalize_dict_with_items,
            StructureType.DICT_WITH_COINS: self._normalize_dict_with_coins,
            StructureType.DICT_WITH_NEWS: self._normalize_dict_with_news,
            StructureType.DICT_WITH_RESULTS: self._normalize_dict_with_results,
            
            # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ CoinStats API
            StructureType.COIN_STATS_PAGINATED: self._normalize_coin_stats_paginated,
            StructureType.COIN_STATS_SINGLE_COIN: self._normalize_coin_stats_single_coin,
            StructureType.COIN_STATS_NEWS: self._normalize_coin_stats_news,
            
            # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
            StructureType.PAGINATED_RESPONSE: self._normalize_paginated_response,
            StructureType.NESTED_STRUCTURE: self._normalize_nested_structure,
        }
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ endpointÙ‡Ø§
        self.known_patterns = {
            "coins/list": StructureType.COIN_STATS_PAGINATED,
            "coins/bitcoin": StructureType.SINGLE_ITEM_LIST,
            "coins/ethereum": StructureType.SINGLE_ITEM_LIST,
            "news/type/handpicked": StructureType.COIN_STATS_NEWS,
            "news/type/trending": StructureType.COIN_STATS_NEWS,
            "exchanges/list": StructureType.DICT_WITH_RESULT,
        }
        
        logger.info("ðŸš€ Data Normalizer v2 Initialized - CoinStats Optimized")

    def _setup_logging(self):
        """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ"""
        self.logger = logging.getLogger(__name__)

    def _initialize_cache(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø´"""
        self.structure_cache = {}
        self.health_cache = {}
        self.analysis_cache = {}
        self.pattern_cache = {}  # Ú©Ø´ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        
        self.cache_ttl = {
            'structure': timedelta(days=7),
            'health': timedelta(hours=1),
            'analysis': timedelta(minutes=30),
            'patterns': timedelta(days=1),  # Ú©Ø´ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        }

    def _reset_metrics(self):
        """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
        self.metrics = {
            'total_processed': 0,
            'total_success': 0, 
            'total_errors': 0,
            'structure_counts': {stype.value: 0 for stype in StructureType},
            'processing_times': [],
            'endpoint_patterns': {},
            'quality_scores': [],
            'alerts': [],
            'confidence_scores': [],  # Ø¬Ø¯ÛŒØ¯
            'pattern_matches': 0,  # Ø¬Ø¯ÛŒØ¯
        }

    def normalize(self, raw_data: Any, endpoint: str = "unknown", 
                 strategy: NormalizationStrategy = None) -> NormalizationResult:
        """
        Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        """
        start_time = time.time()
        self.metrics['total_processed'] += 1
        
        try:
            # ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ endpoint
            structure_type, confidence, pattern_used = self._detect_structure_advanced(raw_data, endpoint)
            self.metrics['structure_counts'][structure_type.value] += 1
            self.metrics['confidence_scores'].append(confidence)
            
            if pattern_used:
                self.metrics['pattern_matches'] += 1
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            normalization_func = self.supported_structures.get(
                structure_type, 
                self._normalize_fallback_advanced
            )
            
            normalized_data = normalization_func(raw_data)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡
            quality_score = self._calculate_quality_score_advanced(normalized_data, structure_type, confidence)
            self.metrics['quality_scores'].append(quality_score)
            
            # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ù„Ú¯Ùˆ
            self._update_endpoint_intelligence(endpoint, structure_type, confidence, raw_data)
            
            processing_time = time.time() - start_time
            self.metrics['processing_times'].append(processing_time)
            self.metrics['total_success'] += 1
            
            result = NormalizationResult(
                status="success",
                data=normalized_data,
                metadata=self._extract_metadata_advanced(raw_data, structure_type),
                raw_data=raw_data,
                normalization_info={
                    "detected_structure": structure_type.value,
                    "confidence": confidence,
                    "pattern_used": pattern_used,
                    "processing_time_ms": round(processing_time * 1000, 2),
                    "endpoint": endpoint,
                    "strategy": (strategy or self.default_strategy).value,
                    "data_quality": quality_score,
                    "timestamp": datetime.now().isoformat()
                },
                quality_score=quality_score
            )
            
            logger.info(f"âœ… Normalized {endpoint} - {structure_type.value} (Conf: {confidence}%) - Quality: {quality_score}%")
            return result
            
        except Exception as e:
            self.metrics['total_errors'] += 1
            error_msg = f"Normalization failed for {endpoint}: {str(e)}"
            logger.error(error_msg)
            self.metrics['alerts'].append(error_msg)
            
            return NormalizationResult(
                status="error",
                data=[],
                metadata={},
                raw_data=raw_data,
                normalization_info={
                    "error": str(e),
                    "endpoint": endpoint,
                    "timestamp": datetime.now().isoformat(),
                    "fallback_used": True
                },
                quality_score=0.0
            )

    def _detect_structure_advanced(self, raw_data: Any, endpoint: str = "unknown") -> Tuple[StructureType, float, bool]:
        """
        ØªØ´Ø®ÛŒØµ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ endpoint
        """
        # Ø§ÙˆÙ„ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡
        if endpoint in self.known_patterns:
            known_structure = self.known_patterns[endpoint]
            logger.debug(f"ðŸŽ¯ Using known pattern for {endpoint}: {known_structure.value}")
            return known_structure, 0.95, True
        
        # ØªØ´Ø®ÛŒØµ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡
        if isinstance(raw_data, list):
            if len(raw_data) == 1 and isinstance(raw_data[0], dict):
                # Ù„ÛŒØ³Øª ØªÚ©â€ŒØ¢ÛŒØªÙ… - Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ coins/bitcoin
                return StructureType.SINGLE_ITEM_LIST, 0.92, False
            elif len(raw_data) > 0:
                return StructureType.DIRECT_LIST, 0.90, False
            else:
                return StructureType.DIRECT_LIST, 0.70, False
        
        elif isinstance(raw_data, dict):
            # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ CoinStats API
            if 'result' in raw_data and isinstance(raw_data['result'], list):
                if 'meta' in raw_data:
                    return StructureType.COIN_STATS_PAGINATED, 0.94, False
                else:
                    return StructureType.DICT_WITH_RESULT, 0.88, False
            
            # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
            key_structures = {
                'data': StructureType.DICT_WITH_DATA,
                'items': StructureType.DICT_WITH_ITEMS,
                'coins': StructureType.DICT_WITH_COINS,
                'news': StructureType.DICT_WITH_NEWS,
                'results': StructureType.DICT_WITH_RESULTS,
            }
            
            for key, structure in key_structures.items():
                if key in raw_data and isinstance(raw_data[key], list):
                    return structure, 0.85, False
            
            # ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ ØªÙˆØ¯Ø±ØªÙˆ
            nested_list = self._find_nested_list(raw_data)
            if nested_list:
                return StructureType.NESTED_STRUCTURE, 0.80, False
        
        # fallback Ø¨Ù‡ Ø¢Ù†Ø§Ù„ÛŒØ² Ù¾ÛŒØ´Ø±ÙØªÙ‡
        return self._advanced_structure_analysis(raw_data), 0.5, False

    def _advanced_structure_analysis(self, raw_data: Any) -> StructureType:
        """Ø¢Ù†Ø§Ù„ÛŒØ² Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ø§Ø®ØªØ§Ø±"""
        if isinstance(raw_data, dict):
            # Ø´Ù…Ø§Ø±Ø´ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ Ø¯Ø± Ø³Ø·ÙˆØ­ Ù…Ø®ØªÙ„Ù
            list_count = self._count_lists_in_dict(raw_data)
            if list_count == 1:
                return StructureType.NESTED_STRUCTURE
            elif list_count > 1:
                return StructureType.CUSTOM_STRUCTURE
        
        return StructureType.UNKNOWN

    def _count_lists_in_dict(self, data: Dict, max_depth: int = 3) -> int:
        """Ø´Ù…Ø§Ø±Ø´ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ"""
        def count_recursive(obj, depth=0):
            if depth >= max_depth:
                return 0
            
            count = 0
            if isinstance(obj, list):
                return 1
            elif isinstance(obj, dict):
                for value in obj.values():
                    count += count_recursive(value, depth + 1)
            return count
        
        return count_recursive(data)

    def _find_nested_list(self, data: Dict, max_depth: int = 3) -> Optional[List]:
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒØ³Øª Ø¯Ø± Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ ØªÙˆØ¯Ø±ØªÙˆ"""
        def find_recursive(obj, depth=0):
            if depth >= max_depth:
                return None
            
            if isinstance(obj, list) and len(obj) > 0:
                return obj
            elif isinstance(obj, dict):
                for value in obj.values():
                    result = find_recursive(value, depth + 1)
                    if result:
                        return result
            return None
        
        return find_recursive(data)

    # ========================== Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ==========================

    def _normalize_single_item_list(self, raw_data: List) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù„ÛŒØ³Øª ØªÚ©â€ŒØ¢ÛŒØªÙ…"""
        return raw_data  # Ù„ÛŒØ³Øª Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…

    def _normalize_dict_with_news(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ news"""
        return raw_data.get('news', [])

    def _normalize_dict_with_results(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ results"""
        return raw_data.get('results', [])

    def _normalize_coin_stats_paginated(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ CoinStats"""
        return raw_data.get('result', [])

    def _normalize_coin_stats_single_coin(self, raw_data: List) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø®ØªØ§Ø± ØªÚ© Ú©ÙˆÛŒÙ† CoinStats"""
        return raw_data  # [{}] Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…

    def _normalize_coin_stats_news(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø§Ø®Ø¨Ø§Ø± CoinStats"""
        if 'result' in raw_data:
            return raw_data['result']
        elif 'news' in raw_data:
            return raw_data['news']
        else:
            return self._extract_data_from_complex_structure(raw_data)

    def _normalize_paginated_response(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡"""
        return raw_data.get('data', raw_data.get('result', []))

    def _normalize_nested_structure(self, raw_data: Dict) -> List:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ ØªÙˆØ¯Ø±ØªÙˆ"""
        nested_list = self._find_nested_list(raw_data)
        return nested_list or []

    def _normalize_fallback_advanced(self, raw_data: Any) -> List:
        """Fallback Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù fallback
        if isinstance(raw_data, dict):
            # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ 1: Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ù„ÛŒØ³Øª Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†
            lists = [v for v in raw_data.values() if isinstance(v, list)]
            if lists:
                return max(lists, key=len)
            
            # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ 2: Ø§ÙˆÙ„ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ù„ÛŒØ³Øª Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†
            for value in raw_data.values():
                if isinstance(value, list):
                    return value
            
            # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ 3: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø±Ø§ Ø¨Ù‡ Ù„ÛŒØ³Øª ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†
            return [raw_data]
        
        elif isinstance(raw_data, list):
            return raw_data
        
        else:
            return [raw_data] if raw_data is not None else []

    # ========================== Ù…ØªØ¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ (Ø¨Ø§ Ø¨Ù‡Ø¨ÙˆØ¯) ==========================

    def _normalize_direct_list(self, raw_data: List) -> List:
        return raw_data

    def _normalize_dict_with_data(self, raw_data: Dict) -> List:
        return raw_data.get('data', [])

    def _normalize_dict_with_result(self, raw_data: Dict) -> List:
        return raw_data.get('result', [])

    def _normalize_dict_with_items(self, raw_data: Dict) -> List:
        return raw_data.get('items', [])

    def _normalize_dict_with_coins(self, raw_data: Dict) -> List:
        return raw_data.get('coins', [])

    def _extract_data_from_complex_structure(self, raw_data: Any) -> List:
        if isinstance(raw_data, dict):
            lists_in_dict = [v for v in raw_data.values() if isinstance(v, list)]
            if lists_in_dict:
                return max(lists_in_dict, key=len)
        return []

    def _extract_metadata_advanced(self, raw_data: Any, structure_type: StructureType) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§ - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        metadata = {
            "structure_type": structure_type.value,
            "extracted_at": datetime.now().isoformat(),
            "data_source": "coinstats_api",
            "structure_complexity": self._calculate_structure_complexity(raw_data)
        }
        
        if isinstance(raw_data, dict):
            common_meta_keys = ['meta', 'metadata', 'pagination', 'info', 'total', 'count', 'page', 'limit']
            for key in common_meta_keys:
                if key in raw_data:
                    metadata[key] = raw_data[key]
                    
        return metadata

    def _calculate_structure_complexity(self, data: Any) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø³Ø§Ø®ØªØ§Ø±"""
        if isinstance(data, list):
            return "low" if len(data) < 10 else "medium"
        elif isinstance(data, dict):
            key_count = len(data)
            if key_count < 5:
                return "low"
            elif key_count < 15:
                return "medium"
            else:
                return "high"
        else:
            return "unknown"

    def _calculate_quality_score_advanced(self, normalized_data: List, structure_type: StructureType, confidence: float) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        if not normalized_data:
            return 0.0
            
        score = 0.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡
        data_count = len(normalized_data)
        if data_count > 0:
            score += min(data_count / 50, 0.3)  # Ø­Ø¯Ø§Ú©Ø«Ø± 30%
            
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± (Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ØªØ±)
        structure_scores = {
            StructureType.COIN_STATS_PAGINATED: 0.3,
            StructureType.COIN_STATS_SINGLE_COIN: 0.25,
            StructureType.COIN_STATS_NEWS: 0.25,
            StructureType.DICT_WITH_RESULT: 0.25,
            StructureType.DICT_WITH_DATA: 0.25,
            StructureType.SINGLE_ITEM_LIST: 0.2,
            StructureType.DIRECT_LIST: 0.2,
            StructureType.DICT_WITH_ITEMS: 0.2,
            StructureType.DICT_WITH_COINS: 0.2,
            StructureType.PAGINATED_RESPONSE: 0.25,
            StructureType.NESTED_STRUCTURE: 0.15,
            StructureType.CUSTOM_STRUCTURE: 0.1,
            StructureType.UNKNOWN: 0.05
        }
        score += structure_scores.get(structure_type, 0.1)
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ confidence
        score += confidence * 0.3
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ
        if data_count > 1:
            uniformity_score = self._calculate_uniformity_score(normalized_data)
            score += uniformity_score * 0.2
            
        return min(score * 100, 100.0)

    def _calculate_uniformity_score(self, data: List) -> float:
        if not data or len(data) < 2:
            return 0.5
            
        try:
            if all(isinstance(item, dict) for item in data):
                first_keys = set(data[0].keys())
                common_keys = first_keys.intersection(*(set(item.keys()) for item in data[1:]))
                return len(common_keys) / len(first_keys) if first_keys else 0.5
        except:
            pass
            
        return 0.5

    def _update_endpoint_intelligence(self, endpoint: str, structure_type: StructureType, confidence: float, raw_data: Any):
        """Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ endpointÙ‡Ø§"""
        if endpoint not in self.metrics['endpoint_patterns']:
            self.metrics['endpoint_patterns'][endpoint] = {
                'total_requests': 0,
                'structure_counts': {},
                'confidence_history': [],
                'raw_data_samples': [],
                'last_detected': None,
                'pattern_stability': 0.0
            }
            
        pattern = self.metrics['endpoint_patterns'][endpoint]
        pattern['total_requests'] += 1
        pattern['structure_counts'][structure_type.value] = pattern['structure_counts'].get(structure_type.value, 0) + 1
        pattern['confidence_history'].append(confidence)
        pattern['last_detected'] = datetime.now().isoformat()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ²
        if pattern['total_requests'] <= 5:  # ÙÙ‚Ø· 5 Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„
            pattern['raw_data_samples'].append({
                'timestamp': datetime.now().isoformat(),
                'structure': structure_type.value,
                'data_preview': str(raw_data)[:200] + "..." if len(str(raw_data)) > 200 else str(raw_data)
            })
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø§Ù„Ú¯Ùˆ
        if pattern['total_requests'] > 1:
            main_structure_count = max(pattern['structure_counts'].values())
            pattern['pattern_stability'] = main_structure_count / pattern['total_requests']

    # ========================== Ù…ØªØ¯Ù‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø¬Ø¯ÛŒØ¯ ==========================

    def get_endpoint_intelligence(self, endpoint: str = None) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù‡ÙˆØ´ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ endpointÙ‡Ø§"""
        if endpoint:
            return self.metrics['endpoint_patterns'].get(endpoint, {})
        else:
            return {
                'total_endpoints': len(self.metrics['endpoint_patterns']),
                'endpoints': self.metrics['endpoint_patterns'],
                'pattern_efficiency': f"{(self.metrics['pattern_matches'] / self.metrics['total_processed'] * 100) if self.metrics['total_processed'] > 0 else 0:.1f}%",
                'timestamp': datetime.now().isoformat()
            }

    def add_known_pattern(self, endpoint: str, structure_type: StructureType):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡"""
        self.known_patterns[endpoint] = structure_type
        logger.info(f"ðŸŽ¯ Added known pattern: {endpoint} -> {structure_type.value}")

    def get_health_metrics(self) -> HealthMetrics:
        total_processed = self.metrics['total_processed']
        success_rate = (self.metrics['total_success'] / total_processed * 100) if total_processed > 0 else 0
        
        processing_times = self.metrics['processing_times']
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        quality_scores = self.metrics['quality_scores']
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        confidence_scores = self.metrics['confidence_scores']
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        return HealthMetrics(
            success_rate=round(success_rate, 2),
            total_processed=total_processed,
            total_success=self.metrics['total_success'],
            total_errors=self.metrics['total_errors'],
            common_structures=self.metrics['structure_counts'],
            performance_metrics={
                'avg_processing_time_ms': round(avg_processing_time * 1000, 2),
                'total_processing_time_ms': round(sum(processing_times) * 1000, 2),
                'requests_per_second': round(total_processed / (sum(processing_times) or 1), 2),
                'avg_confidence': round(avg_confidence, 2),
                'pattern_efficiency': f"{(self.metrics['pattern_matches'] / total_processed * 100) if total_processed > 0 else 0:.1f}%"
            },
            alerts=self.metrics['alerts'][-10:],
            data_quality={
                'avg_quality_score': round(avg_quality, 2),
                'completeness_score': round(success_rate, 2),
                'consistency_score': round(self._calculate_consistency_score(), 2)
            },
            endpoint_intelligence=self.get_endpoint_intelligence()
        )

    # Ø¨Ù‚ÛŒÙ‡ Ù…ØªØ¯Ù‡Ø§ Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„...
    def get_deep_analysis(self, raw_data: Any = None, endpoint: str = None) -> Dict[str, Any]:
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "system_overview": {
                "total_requests": self.metrics['total_processed'],
                "success_rate": self.get_health_metrics().success_rate,
                "most_common_structure": max(
                    self.metrics['structure_counts'].items(), 
                    key=lambda x: x[1],
                    default=('unknown', 0)
                ),
                "avg_confidence": f"{sum(self.metrics['confidence_scores']) / len(self.metrics['confidence_scores']):.1f}%" if self.metrics['confidence_scores'] else "0%"
            },
            "endpoint_intelligence": self.get_endpoint_intelligence(),
            "structure_analysis": self.metrics['structure_counts'],
            "performance_analysis": {
                "avg_processing_time": f"{sum(self.metrics['processing_times']) / len(self.metrics['processing_times']) * 1000:.2f}ms" if self.metrics['processing_times'] else "0ms",
                "pattern_efficiency": f"{(self.metrics['pattern_matches'] / self.metrics['total_processed'] * 100) if self.metrics['total_processed'] > 0 else 0:.1f}%"
            },
            "known_patterns": {k: v.value for k, v in self.known_patterns.items()},
            "alerts_and_warnings": self.metrics['alerts'][-20:],
            "recommendations": self._generate_recommendations_advanced()
        }
        
        if raw_data is not None:
            analysis["specific_data_analysis"] = self._analyze_specific_data_advanced(raw_data, endpoint)
            
        return analysis

    def _generate_recommendations_advanced(self) -> List[str]:
        recommendations = []
        metrics = self.get_health_metrics()
        
        if metrics.success_rate < 95:
            recommendations.append("ðŸ”„ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
            
        if metrics.performance_metrics['pattern_efficiency'] < '80%':
            recommendations.append("ðŸŽ¯ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. endpointÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ù‡ known patterns Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.")
            
        if metrics.data_quality['avg_quality_score'] < 80:
            recommendations.append("ðŸ“Š Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø±Ø¯.")
            
        if not recommendations:
            recommendations.append("âœ… Ø³ÛŒØ³ØªÙ… Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù…Ø·Ù„ÙˆØ¨ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯.")
            
        return recommendations

    def _analyze_specific_data_advanced(self, raw_data: Any, endpoint: str = None) -> Dict[str, Any]:
        structure_type, confidence, pattern_used = self._detect_structure_advanced(raw_data, endpoint or "analysis")
        
        return {
            "detected_structure": structure_type.value,
            "confidence": confidence,
            "pattern_used": pattern_used,
            "data_type": type(raw_data).__name__,
            "data_size": len(raw_data) if hasattr(raw_data, '__len__') else 'unknown',
            "structure_complexity": self._calculate_structure_complexity(raw_data),
            "sample_preview": str(raw_data)[:200] + "..." if len(str(raw_data)) > 200 else str(raw_data),
            "endpoint_context": endpoint,
        }

    def _calculate_consistency_score(self) -> float:
        endpoint_patterns = self.metrics['endpoint_patterns']
        if not endpoint_patterns:
            return 0.0
            
        consistency_scores = []
        for endpoint, pattern in endpoint_patterns.items():
            if pattern['total_requests'] > 1:
                main_structure = max(pattern['structure_counts'].items(), key=lambda x: x[1])
                consistency = main_structure[1] / pattern['total_requests']
                consistency_scores.append(consistency)
                
        return sum(consistency_scores) / len(consistency_scores) * 100 if consistency_scores else 0.0

    def clear_cache(self, cache_type: str = None):
        if cache_type == 'structure' or cache_type is None:
            self.structure_cache.clear()
        if cache_type == 'health' or cache_type is None:
            self.health_cache.clear() 
        if cache_type == 'analysis' or cache_type is None:
            self.analysis_cache.clear()
        if cache_type == 'patterns' or cache_type is None:
            self.pattern_cache.clear()
            
        logger.info("ðŸ§¹ Data Normalizer cache cleared")

    def reset_metrics(self):
        self._reset_metrics()
        logger.info("ðŸ”„ Data Normalizer metrics reset")

# Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ù„ÙˆØ¨Ø§Ù„
data_normalizer = DataNormalizer()
