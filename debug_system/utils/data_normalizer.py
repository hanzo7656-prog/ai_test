"""
ğŸ¤– Data Normalizer v2.1 - Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø¹Ù…Ø± Ø¯Ø§Ø¯Ù‡
ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯:
- Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ú©Ø´
- Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¹Ù…Ø± Ø¯Ø§Ø¯Ù‡ (10 Ø±ÙˆØ²)
- Ù‚Ø§Ø¨Ù„ÛŒØª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
- Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ
"""

import json
import time
import os
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple
import logging
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path

logger = logging.getLogger(__name__)

class StructureType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
    DIRECT_LIST = "direct_list"
    SINGLE_ITEM_LIST = "single_item_list"
    
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§ Ù„ÛŒØ³Øª
    DICT_WITH_DATA = "dict_with_data"
    DICT_WITH_RESULT = "dict_with_result" 
    DICT_WITH_ITEMS = "dict_with_items"
    DICT_WITH_COINS = "dict_with_coins"
    DICT_WITH_NEWS = "dict_with_news"
    DICT_WITH_RESULTS = "dict_with_results"
    
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ CoinStats API Ø®Ø§Øµ
    COIN_STATS_PAGINATED = "coin_stats_paginated"
    COIN_STATS_SINGLE_COIN = "coin_stats_single_coin"
    COIN_STATS_NEWS = "coin_stats_news"
    COIN_STATS_DETAILED = "coin_stats_detailed"
    COIN_STATS_CHART = "coin_stats_chart"
    COIN_STATS_MARKETS = "coin_stats_markets"
    COIN_STATS_INSIGHTS = "coin_stats_insights"
    COIN_STATS_ERROR = "coin_stats_error"
    COIN_STATS_SENTIMENT = "coin_stats_sentiment"
    
    # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
    NESTED_STRUCTURE = "nested_structure"
    PAGINATED_RESPONSE = "paginated_response"
    
    # fallback
    CUSTOM_STRUCTURE = "custom_structure"
    UNKNOWN = "unknown"

class NormalizationStrategy(Enum):
    """Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    SMART = "smart"
    STRICT = "strict"
    LENIENT = "lenient"
    COIN_STATS_OPTIMIZED = "coin_stats_optimized"

@dataclass
class NormalizationResult:
    """Ù†ØªÛŒØ¬Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ"""
    status: str  # success | error
    data: List[Any]
    metadata: Dict[str, Any]
    raw_data: Any
    normalization_info: Dict[str, Any]
    quality_score: float

@dataclass  
class HealthMetrics:
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…"""
    success_rate: float
    total_processed: int
    total_success: int
    total_errors: int
    common_structures: Dict[str, int]
    performance_metrics: Dict[str, Any]
    alerts: List[str]
    data_quality: Dict[str, float]
    endpoint_intelligence: Dict[str, Any]

class DataNormalizer:
    """
    Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ API - Ù†Ø³Ø®Ù‡ Ø¨Ø§ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self._setup_logging()
        self._initialize_cache_system()
        self._reset_metrics()
        
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        self.default_strategy = NormalizationStrategy.COIN_STATS_OPTIMIZED
        
        # Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡
        self.supported_structures = {
            StructureType.DIRECT_LIST: self._normalize_direct_list,
            StructureType.SINGLE_ITEM_LIST: self._normalize_single_item_list,
            StructureType.DICT_WITH_DATA: self._normalize_dict_with_data,
            StructureType.DICT_WITH_RESULT: self._normalize_dict_with_result,
            StructureType.DICT_WITH_ITEMS: self._normalize_dict_with_items,
            StructureType.DICT_WITH_COINS: self._normalize_dict_with_coins,
            StructureType.DICT_WITH_NEWS: self._normalize_dict_with_news,
            StructureType.DICT_WITH_RESULTS: self._normalize_dict_with_results,
            StructureType.COIN_STATS_PAGINATED: self._normalize_coin_stats_paginated,
            StructureType.COIN_STATS_SINGLE_COIN: self._normalize_coin_stats_single_coin,
            StructureType.COIN_STATS_NEWS: self._normalize_coin_stats_news,
            StructureType.COIN_STATS_DETAILED: self._normalize_coin_stats_detailed,
            StructureType.COIN_STATS_CHART: self._normalize_coin_stats_chart,
            StructureType.COIN_STATS_MARKETS: self._normalize_coin_stats_markets,
            StructureType.COIN_STATS_INSIGHTS: self._normalize_coin_stats_insights,
            StructureType.COIN_STATS_ERROR: self._normalize_coin_stats_error,
            StructureType.COIN_STATS_SENTIMENT: self._normalize_coin_stats_sentiment,
            StructureType.PAGINATED_RESPONSE: self._normalize_paginated_response,
            StructureType.NESTED_STRUCTURE: self._normalize_nested_structure,
        }
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡
        self.known_patterns = {
            "coins/list": StructureType.COIN_STATS_PAGINATED,
            "coins/bitcoin": StructureType.COIN_STATS_SINGLE_COIN,
            "coins/ethereum": StructureType.COIN_STATS_SINGLE_COIN,
            "news/type/handpicked": StructureType.COIN_STATS_NEWS,
            "news/type/trending": StructureType.COIN_STATS_NEWS,
            "news/type/latest": StructureType.COIN_STATS_NEWS,
            "news/type/bullish": StructureType.COIN_STATS_NEWS,
            "news/type/bearish": StructureType.COIN_STATS_NEWS,
            "exchanges/list": StructureType.COIN_STATS_MARKETS,
            "markets": StructureType.COIN_STATS_MARKETS,
            "insights/fear-and-greed": StructureType.COIN_STATS_INSIGHTS,
            "insights/btc-dominance": StructureType.COIN_STATS_INSIGHTS,
            "coins/charts": StructureType.COIN_STATS_CHART,
        }
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        self._load_persisted_data()
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾Ø§Ú©â€ŒØ³Ø§Ø² Ø®ÙˆØ¯Ú©Ø§Ø±
        self._start_auto_cleanup()
        
        logger.info("ğŸš€ Data Normalizer v2.1 Initialized - File Storage Enabled")

    def _setup_logging(self):
        """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ"""
        self.logger = logging.getLogger(__name__)

    def _reset_metrics(self):
        """Ø¨Ø§Ø²Ù†Ø´Ø§Ù†ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
        self.metrics = {
            'total_processed': 0,
            'total_success': 0, 
            'total_errors': 0,
            'structure_counts': {stype.value: 0 for stype in StructureType},
            'processing_times': [],
            'endpoint_patterns': {},
            'quality_scores': [],
            'alerts': [],
            'confidence_scores': [],
            'pattern_matches': 0,
        }

    def _initialize_cache_system(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„"""
        # Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ú©Ø´
        self.cache_base_path = Path(__file__).parent / "data_normalizer_cache"
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
        self.cache_dirs = {
            'structure': self.cache_base_path / "structure",
            'patterns': self.cache_base_path / "patterns", 
            'metrics': self.cache_base_path / "metrics",
            'samples': self.cache_base_path / "samples",
            'logs': self.cache_base_path / "logs",
        }
        
        for dir_path in self.cache_dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)
            
        logger.info(f"ğŸ“ Cache system initialized at: {self.cache_base_path}")

    def _load_persisted_data(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§"""
        try:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            patterns_file = self.cache_dirs['patterns'] / "known_patterns.json"
            if patterns_file.exists():
                with open(patterns_file, 'r', encoding='utf-8') as f:
                    saved_patterns = json.load(f)
                    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ StructureType
                    for endpoint, struct_type in saved_patterns.items():
                        if struct_type in [st.value for st in StructureType]:
                            self.known_patterns[endpoint] = StructureType(struct_type)
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            metrics_file = self.cache_dirs['metrics'] / "health_metrics.json"
            if metrics_file.exists():
                with open(metrics_file, 'r', encoding='utf-8') as f:
                    saved_metrics = json.load(f)
                    self.metrics.update(saved_metrics)
                    
            logger.info("âœ… Persisted data loaded successfully")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load persisted data: {e}")

    def _save_to_cache(self, data_type: str, key: str, data: Any, timestamp: str = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø´"""
        try:
            if data_type not in self.cache_dirs:
                logger.error(f"âŒ Unknown cache type: {data_type}")
                return
                
            timestamp = timestamp or datetime.now().isoformat()
            filename = f"{key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            file_path = self.cache_dirs[data_type] / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'data': data,
                    'timestamp': timestamp,
                    'expires_at': (datetime.now() + timedelta(days=10)).isoformat()
                }, f, indent=2, ensure_ascii=False, default=str)
                
        except Exception as e:
            logger.error(f"âŒ Failed to save cache {data_type}/{key}: {e}")

    def _load_from_cache(self, data_type: str, key: str) -> Optional[Any]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ Ú©Ø´"""
        try:
            if data_type not in self.cache_dirs:
                return None
                
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† key
            pattern = f"{key}_*.json"
            files = list(self.cache_dirs[data_type].glob(pattern))
            if not files:
                return None
                
            # Ú¯Ø±ÙØªÙ† Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ÙØ§ÛŒÙ„
            latest_file = max(files, key=lambda x: x.stat().st_mtime)
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ù‚Ø¶Ø§
            expires_at = datetime.fromisoformat(cached_data.get('expires_at', '2000-01-01'))
            if datetime.now() > expires_at:
                os.remove(latest_file)
                return None
                
            return cached_data.get('data')
            
        except Exception as e:
            logger.error(f"âŒ Failed to load cache {data_type}/{key}: {e}")
            return None

    def _save_sample_data(self, endpoint: str, raw_data: Any, structure_type: StructureType):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ²"""
        try:
            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ (Ø­Ø¯Ø§Ú©Ø«Ø± 10 Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± Ø±ÙˆØ²)
            endpoint_dir = self.cache_dirs['samples'] / endpoint.replace('/', '_')
            endpoint_dir.mkdir(parents=True, exist_ok=True)
            
            # Ø´Ù…Ø§Ø±Ø´ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ø±ÙˆØ²
            today_pattern = f"sample_{datetime.now().strftime('%Y%m%d')}_*.json"
            today_samples = list(endpoint_dir.glob(today_pattern))
            
            if len(today_samples) < 10:
                filename = f"sample_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                file_path = endpoint_dir / filename
                
                sample_data = {
                    'endpoint': endpoint,
                    'structure_type': structure_type.value,
                    'timestamp': datetime.now().isoformat(),
                    'raw_data_preview': str(raw_data)[:500] + "..." if len(str(raw_data)) > 500 else str(raw_data),
                    'data_size': len(str(raw_data)) if hasattr(raw_data, '__len__') else 'unknown'
                }
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(sample_data, f, indent=2, ensure_ascii=False)
                    
        except Exception as e:
            logger.error(f"âŒ Failed to save sample data for {endpoint}: {e}")

    def _start_auto_cleanup(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾Ø§Ú©â€ŒØ³Ø§Ø² Ø®ÙˆØ¯Ú©Ø§Ø±"""
        try:
            self._cleanup_old_files()
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø± 24 Ø³Ø§Ø¹Øª
            import threading
            def cleanup_scheduler():
                while True:
                    time.sleep(24 * 60 * 60)  # 24 Ø³Ø§Ø¹Øª
                    self._cleanup_old_files()
                    
            thread = threading.Thread(target=cleanup_scheduler, daemon=True)
            thread.start()
            logger.info("ğŸ§¹ Auto-cleanup scheduler started")
            
        except Exception as e:
            logger.error(f"âŒ Failed to start auto-cleanup: {e}")

    def _cleanup_old_files(self):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ø§Ø² 10 Ø±ÙˆØ²"""
        try:
            cutoff_time = datetime.now() - timedelta(days=10)
            deleted_count = 0
            
            for dir_type, dir_path in self.cache_dirs.items():
                if dir_path.exists():
                    for file_path in dir_path.rglob("*.json"):
                        try:
                            file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                            if file_time < cutoff_time:
                                file_path.unlink()
                                deleted_count += 1
                        except Exception as e:
                            logger.warning(f"âš ï¸ Could not delete {file_path}: {e}")
            
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ
            for dir_path in self.cache_dirs.values():
                if dir_path.exists() and dir_path.is_dir():
                    try:
                        # Ø­Ø°Ù Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø¯Ø± samples
                        if dir_path.name == "samples":
                            for subdir in dir_path.iterdir():
                                if subdir.is_dir() and not any(subdir.iterdir()):
                                    subdir.rmdir()
                    except Exception as e:
                        logger.warning(f"âš ï¸ Could not clean empty directories: {e}")
            
            logger.info(f"ğŸ§¹ Cleanup completed: {deleted_count} old files deleted")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ
            cleanup_log = {
                'timestamp': datetime.now().isoformat(),
                'deleted_files': deleted_count,
                'cutoff_time': cutoff_time.isoformat()
            }
            
            log_file = self.cache_dirs['logs'] / f"cleanup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(cleanup_log, f, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ Cleanup failed: {e}")

    def _persist_metrics(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„"""
        try:
            metrics_file = self.cache_dirs['metrics'] / "health_metrics.json"
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.metrics, f, indent=2, default=str)
                
            # Ø°Ø®ÛŒØ±Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            patterns_file = self.cache_dirs['patterns'] / "known_patterns.json"
            patterns_to_save = {k: v.value for k, v in self.known_patterns.items()}
            with open(patterns_file, 'w', encoding='utf-8') as f:
                json.dump(patterns_to_save, f, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ Failed to persist metrics: {e}")

    # ========================== Ù…ØªØ¯Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ==========================

    def normalize(self, raw_data: Any, endpoint: str = "unknown", 
                 strategy: NormalizationStrategy = None) -> NormalizationResult:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„"""
        start_time = time.time()
        self.metrics['total_processed'] += 1
        
        try:
            # ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø±
            structure_type, confidence, pattern_used = self._detect_structure_advanced(raw_data, endpoint)
            self.metrics['structure_counts'][structure_type.value] += 1
            self.metrics['confidence_scores'].append(confidence)
            
            if pattern_used:
                self.metrics['pattern_matches'] += 1
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            normalization_func = self.supported_structures.get(
                structure_type, 
                self._normalize_fallback_advanced
            )
            
            normalized_data = normalization_func(raw_data)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª
            quality_score = self._calculate_quality_score_advanced(normalized_data, structure_type, confidence)
            self.metrics['quality_scores'].append(quality_score)
            
            # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            self._save_sample_data(endpoint, raw_data, structure_type)
            self._update_endpoint_intelligence(endpoint, structure_type, confidence, raw_data)
            self._persist_metrics()
            
            processing_time = time.time() - start_time
            self.metrics['processing_times'].append(processing_time)
            self.metrics['total_success'] += 1
            
            result = NormalizationResult(
                status="success",
                data=normalized_data,
                metadata=self._extract_metadata_advanced(raw_data, structure_type),
                raw_data=raw_data,
                normalization_info={
                    "detected_structure": structure_type.value,
                    "confidence": confidence,
                    "pattern_used": pattern_used,
                    "processing_time_ms": round(processing_time * 1000, 2),
                    "endpoint": endpoint,
                    "strategy": (strategy or self.default_strategy).value,
                    "data_quality": quality_score,
                    "timestamp": datetime.now().isoformat(),
                    "cache_used": False
                },
                quality_score=quality_score
            )
            
            logger.info(f"âœ… Normalized {endpoint} - {structure_type.value} (Conf: {confidence}%) - Quality: {quality_score}%")
            return result
            
        except Exception as e:
            self.metrics['total_errors'] += 1
            error_msg = f"Normalization failed for {endpoint}: {str(e)}"
            logger.error(error_msg)
            self.metrics['alerts'].append(error_msg)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø®Ø·Ø§
            self._save_to_cache('logs', f"error_{endpoint}", {
                'error': str(e),
                'endpoint': endpoint,
                'timestamp': datetime.now().isoformat(),
                'raw_data_preview': str(raw_data)[:200] if raw_data else 'None'
            })
            
            return NormalizationResult(
                status="error",
                data=[],
                metadata={},
                raw_data=raw_data,
                normalization_info={
                    "error": str(e),
                    "endpoint": endpoint,
                    "timestamp": datetime.now().isoformat(),
                    "fallback_used": True
                },
                quality_score=0.0
            )

    def _detect_structure_advanced(self, raw_data: Any, endpoint: str = "unknown") -> Tuple[StructureType, float, bool]:
        """ØªØ´Ø®ÛŒØµ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ endpoint"""
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡
        if endpoint in self.known_patterns:
            known_structure = self.known_patterns[endpoint]
            logger.debug(f"ğŸ¯ Using known pattern for {endpoint}: {known_structure.value}")
            return known_structure, 0.95, True
        
        # ØªØ´Ø®ÛŒØµ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ CoinStats
        if isinstance(raw_data, dict):
            # ØªØ´Ø®ÛŒØµ Ø®Ø·Ø§Ù‡Ø§ÛŒ CoinStats
            if 'error' in raw_data or 'statusCode' in raw_data:
                return StructureType.COIN_STATS_ERROR, 0.90, False
            
            # ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø®Ø§Øµ CoinStats
            if 'now' in raw_data and 'value' in raw_data.get('now', {}):
                return StructureType.COIN_STATS_INSIGHTS, 0.88, False
                
            if 'meta' in raw_data and 'result' in raw_data:
                return StructureType.COIN_STATS_PAGINATED, 0.94, False
                
        # ØªØ´Ø®ÛŒØµ Ù¾Ø§ÛŒÙ‡
        return self._detect_structure_basic(raw_data)

    def _detect_structure_basic(self, raw_data: Any) -> Tuple[StructureType, float, bool]:
        """ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø§ÛŒÙ‡"""
        if isinstance(raw_data, list):
            if len(raw_data) == 1 and isinstance(raw_data[0], dict):
                return StructureType.SINGLE_ITEM_LIST, 0.92, False
            elif len(raw_data) > 0:
                return StructureType.DIRECT_LIST, 0.90, False
            else:
                return StructureType.DIRECT_LIST, 0.70, False
        
        elif isinstance(raw_data, dict):
            if 'result' in raw_data and isinstance(raw_data['result'], list):
                if 'meta' in raw_data:
                    return StructureType.COIN_STATS_PAGINATED, 0.94, False
                else:
                    return StructureType.DICT_WITH_RESULT, 0.88, False
            
            key_structures = {
                'data': StructureType.DICT_WITH_DATA,
                'items': StructureType.DICT_WITH_ITEMS,
                'coins': StructureType.DICT_WITH_COINS,
                'news': StructureType.DICT_WITH_NEWS,
                'results': StructureType.DICT_WITH_RESULTS,
            }
            
            for key, structure in key_structures.items():
                if key in raw_data and isinstance(raw_data[key], list):
                    return structure, 0.85, False
            
            nested_list = self._find_nested_list(raw_data)
            if nested_list:
                return StructureType.NESTED_STRUCTURE, 0.80, False
        
        return StructureType.UNKNOWN, 0.5, False

    def _find_nested_list(self, data: Dict, max_depth: int = 3) -> Optional[List]:
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒØ³Øª Ø¯Ø± Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ ØªÙˆØ¯Ø±ØªÙˆ"""
        def find_recursive(obj, depth=0):
            if depth >= max_depth:
                return None
            
            if isinstance(obj, list) and len(obj) > 0:
                return obj
            elif isinstance(obj, dict):
                for value in obj.values():
                    result = find_recursive(value, depth + 1)
                    if result:
                        return result
            return None
        
        return find_recursive(data)

    # ========================== Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ==========================

    def _normalize_direct_list(self, raw_data: List) -> List:
        return raw_data

    def _normalize_single_item_list(self, raw_data: List) -> List:
        return raw_data

    def _normalize_dict_with_data(self, raw_data: Dict) -> List:
        return raw_data.get('data', [])

    def _normalize_dict_with_result(self, raw_data: Dict) -> List:
        return raw_data.get('result', [])

    def _normalize_dict_with_items(self, raw_data: Dict) -> List:
        return raw_data.get('items', [])

    def _normalize_dict_with_coins(self, raw_data: Dict) -> List:
        return raw_data.get('coins', [])

    def _normalize_dict_with_news(self, raw_data: Dict) -> List:
        return raw_data.get('news', [])

    def _normalize_dict_with_results(self, raw_data: Dict) -> List:
        return raw_data.get('results', [])

    def _normalize_coin_stats_paginated(self, raw_data: Dict) -> List:
        return raw_data.get('result', [])

    def _normalize_coin_stats_single_coin(self, raw_data: List) -> List:
        return raw_data

    def _normalize_coin_stats_news(self, raw_data: Dict) -> List:
        if 'result' in raw_data:
            return raw_data['result']
        elif 'news' in raw_data:
            return raw_data['news']
        else:
            return self._extract_data_from_complex_structure(raw_data)

    def _normalize_coin_stats_detailed(self, raw_data: Dict) -> List:
        if 'data' in raw_data and isinstance(raw_data['data'], dict):
            return [raw_data['data']]
        return [raw_data]

    def _normalize_coin_stats_chart(self, raw_data: Any) -> List:
        if isinstance(raw_data, list):
            return raw_data
        elif isinstance(raw_data, dict) and 'result' in raw_data:
            return raw_data['result']
        return []

    def _normalize_coin_stats_markets(self, raw_data: Dict) -> List:
        return raw_data.get('data', raw_data.get('result', []))

    def _normalize_coin_stats_insights(self, raw_data: Dict) -> List:
        if 'now' in raw_data:
            return [raw_data]
        return raw_data.get('data', raw_data.get('result', []))

    def _normalize_coin_stats_error(self, raw_data: Dict) -> List:
        return [raw_data]

    def _normalize_coin_stats_sentiment(self, raw_data: Dict) -> List:
        return raw_data.get('data', raw_data.get('result', []))

    def _normalize_paginated_response(self, raw_data: Dict) -> List:
        return raw_data.get('data', raw_data.get('result', []))

    def _normalize_nested_structure(self, raw_data: Dict) -> List:
        nested_list = self._find_nested_list(raw_data)
        return nested_list or []

    def _normalize_fallback_advanced(self, raw_data: Any) -> List:
        """Fallback Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        if isinstance(raw_data, dict):
            lists = [v for v in raw_data.values() if isinstance(v, list)]
            if lists:
                return max(lists, key=len)
            
            for value in raw_data.values():
                if isinstance(value, list):
                    return value
            
            return [raw_data]
        
        elif isinstance(raw_data, list):
            return raw_data
        
        else:
            return [raw_data] if raw_data is not None else []

    def _extract_data_from_complex_structure(self, raw_data: Any) -> List:
        if isinstance(raw_data, dict):
            lists_in_dict = [v for v in raw_data.values() if isinstance(v, list)]
            if lists_in_dict:
                return max(lists_in_dict, key=len)
        return []

    def _extract_metadata_advanced(self, raw_data: Any, structure_type: StructureType) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§"""
        metadata = {
            "structure_type": structure_type.value,
            "extracted_at": datetime.now().isoformat(),
            "data_source": "coinstats_api",
            "structure_complexity": self._calculate_structure_complexity(raw_data)
        }
        
        if isinstance(raw_data, dict):
            common_meta_keys = ['meta', 'metadata', 'pagination', 'info', 'total', 'count', 'page', 'limit']
            for key in common_meta_keys:
                if key in raw_data:
                    metadata[key] = raw_data[key]
                    
        return metadata

    def _calculate_structure_complexity(self, data: Any) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø³Ø§Ø®ØªØ§Ø±"""
        if isinstance(data, list):
            return "low" if len(data) < 10 else "medium"
        elif isinstance(data, dict):
            key_count = len(data)
            if key_count < 5:
                return "low"
            elif key_count < 15:
                return "medium"
            else:
                return "high"
        else:
            return "unknown"

    def _calculate_quality_score_advanced(self, normalized_data: List, structure_type: StructureType, confidence: float) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª"""
        if not normalized_data:
            return 0.0
            
        score = 0.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡
        data_count = len(normalized_data)
        if data_count > 0:
            score += min(data_count / 50, 0.3)
            
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø±
        structure_scores = {
            StructureType.COIN_STATS_PAGINATED: 0.3,
            StructureType.COIN_STATS_SINGLE_COIN: 0.25,
            StructureType.COIN_STATS_NEWS: 0.25,
            StructureType.DICT_WITH_RESULT: 0.25,
            StructureType.DICT_WITH_DATA: 0.25,
            StructureType.SINGLE_ITEM_LIST: 0.2,
            StructureType.DIRECT_LIST: 0.2,
            StructureType.DICT_WITH_ITEMS: 0.2,
            StructureType.DICT_WITH_COINS: 0.2,
            StructureType.PAGINATED_RESPONSE: 0.25,
            StructureType.NESTED_STRUCTURE: 0.15,
            StructureType.CUSTOM_STRUCTURE: 0.1,
            StructureType.UNKNOWN: 0.05
        }
        score += structure_scores.get(structure_type, 0.1)
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ confidence
        score += confidence * 0.3
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ
        if data_count > 1:
            uniformity_score = self._calculate_uniformity_score(normalized_data)
            score += uniformity_score * 0.2
            
        return min(score * 100, 100.0)

    def _calculate_uniformity_score(self, data: List) -> float:
        if not data or len(data) < 2:
            return 0.5
            
        try:
            if all(isinstance(item, dict) for item in data):
                first_keys = set(data[0].keys())
                common_keys = first_keys.intersection(*(set(item.keys()) for item in data[1:]))
                return len(common_keys) / len(first_keys) if first_keys else 0.5
        except:
            pass
            
        return 0.5

    def _update_endpoint_intelligence(self, endpoint: str, structure_type: StructureType, confidence: float, raw_data: Any):
        """Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ endpointÙ‡Ø§"""
        if endpoint not in self.metrics['endpoint_patterns']:
            self.metrics['endpoint_patterns'][endpoint] = {
                'total_requests': 0,
                'structure_counts': {},
                'confidence_history': [],
                'last_detected': None,
                'pattern_stability': 0.0
            }
            
        pattern = self.metrics['endpoint_patterns'][endpoint]
        pattern['total_requests'] += 1
        pattern['structure_counts'][structure_type.value] = pattern['structure_counts'].get(structure_type.value, 0) + 1
        pattern['confidence_history'].append(confidence)
        pattern['last_detected'] = datetime.now().isoformat()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø§Ù„Ú¯Ùˆ
        if pattern['total_requests'] > 1:
            main_structure_count = max(pattern['structure_counts'].values())
            pattern['pattern_stability'] = main_structure_count / pattern['total_requests']
            
        # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ù„Ú¯ÙˆÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ú¯Ø± Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨Ø§Ù„Ø§ Ø¨Ø§Ø´Ø¯
        if (pattern['pattern_stability'] > 0.8 and 
            pattern['total_requests'] > 5 and 
            endpoint not in self.known_patterns):
            self.known_patterns[endpoint] = structure_type
            logger.info(f"ğŸ“ Learned new pattern: {endpoint} -> {structure_type.value}")

    # ========================== Ù…ØªØ¯Ù‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ ==========================

    def get_endpoint_intelligence(self, endpoint: str = None) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù‡ÙˆØ´ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ endpointÙ‡Ø§"""
        if endpoint:
            return self.metrics['endpoint_patterns'].get(endpoint, {})
        else:
            return {
                'total_endpoints': len(self.metrics['endpoint_patterns']),
                'endpoints': self.metrics['endpoint_patterns'],
                'pattern_efficiency': f"{(self.metrics['pattern_matches'] / self.metrics['total_processed'] * 100) if self.metrics['total_processed'] > 0 else 0:.1f}%",
                'timestamp': datetime.now().isoformat()
            }

    def add_known_pattern(self, endpoint: str, structure_type: StructureType):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡"""
        self.known_patterns[endpoint] = structure_type
        logger.info(f"ğŸ¯ Added known pattern: {endpoint} -> {structure_type.value}")

    def get_health_metrics(self) -> HealthMetrics:
        total_processed = self.metrics['total_processed']
        success_rate = (self.metrics['total_success'] / total_processed * 100) if total_processed > 0 else 0
        
        processing_times = self.metrics['processing_times']
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        quality_scores = self.metrics['quality_scores']
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        confidence_scores = self.metrics['confidence_scores']
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        return HealthMetrics(
            success_rate=round(success_rate, 2),
            total_processed=total_processed,
            total_success=self.metrics['total_success'],
            total_errors=self.metrics['total_errors'],
            common_structures=self.metrics['structure_counts'],
            performance_metrics={
                'avg_processing_time_ms': round(avg_processing_time * 1000, 2),
                'total_processing_time_ms': round(sum(processing_times) * 1000, 2),
                'requests_per_second': round(total_processed / (sum(processing_times) or 1), 2),
                'avg_confidence': round(avg_confidence, 2),
                'pattern_efficiency': f"{(self.metrics['pattern_matches'] / total_processed * 100) if total_processed > 0 else 0:.1f}%"
            },
            alerts=self.metrics['alerts'][-10:],
            data_quality={
                'avg_quality_score': round(avg_quality, 2),
                'completeness_score': round(success_rate, 2),
                'consistency_score': round(self._calculate_consistency_score(), 2)
            },
            endpoint_intelligence=self.get_endpoint_intelligence()
        )

    def _calculate_consistency_score(self) -> float:
        endpoint_patterns = self.metrics['endpoint_patterns']
        if not endpoint_patterns:
            return 0.0
            
        consistency_scores = []
        for endpoint, pattern in endpoint_patterns.items():
            if pattern['total_requests'] > 1:
                main_structure = max(pattern['structure_counts'].items(), key=lambda x: x[1])
                consistency = main_structure[1] / pattern['total_requests']
                consistency_scores.append(consistency)
                
        return sum(consistency_scores) / len(consistency_scores) * 100 if consistency_scores else 0.0

    def get_cache_info(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø´"""
        cache_info = {
            'cache_path': str(self.cache_base_path),
            'total_size_mb': 0,
            'file_counts': {},
            'oldest_file': None,
            'newest_file': None
        }
        
        try:
            total_size = 0
            for dir_type, dir_path in self.cache_dirs.items():
                if dir_path.exists():
                    file_count = len(list(dir_path.rglob("*.json")))
                    cache_info['file_counts'][dir_type] = file_count
                    
                    for file_path in dir_path.rglob("*.json"):
                        total_size += file_path.stat().st_size
                        
                        file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                        if (cache_info['oldest_file'] is None or 
                            file_time < datetime.fromisoformat(cache_info['oldest_file'])):
                            cache_info['oldest_file'] = file_time.isoformat()
                            
                        if (cache_info['newest_file'] is None or 
                            file_time > datetime.fromisoformat(cache_info['newest_file'])):
                            cache_info['newest_file'] = file_time.isoformat()
            
            cache_info['total_size_mb'] = round(total_size / (1024 * 1024), 2)
            
        except Exception as e:
            logger.error(f"âŒ Failed to get cache info: {e}")
            
        return cache_info

    def clear_cache(self, cache_type: str = None):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ø´"""
        try:
            if cache_type and cache_type in self.cache_dirs:
                shutil.rmtree(self.cache_dirs[cache_type])
                self.cache_dirs[cache_type].mkdir()
                logger.info(f"ğŸ§¹ Cleared {cache_type} cache")
            else:
                shutil.rmtree(self.cache_base_path)
                self._initialize_cache_system()
                logger.info("ğŸ§¹ Cleared all cache")
                
        except Exception as e:
            logger.error(f"âŒ Cache clear failed: {e}")

    def reset_metrics(self):
        self._reset_metrics()
        logger.info("ğŸ”„ Data Normalizer metrics reset")

# Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ù„ÙˆØ¨Ø§Ù„
data_normalizer = DataNormalizer()
